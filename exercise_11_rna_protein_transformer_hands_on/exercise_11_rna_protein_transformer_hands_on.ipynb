{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11- RNA-Protein Interaction with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this exercise, we will download the pre-trained Bidirectional Encoder Representations from Transformers model for DNA (DNABERT) and fine-tune it to predict the RNA-binding sites of the RNA-binding protein, PUM2. We will then also implement an LSTM model to train and evaluate it on the same dataset as a comparison.\n",
    "\n",
    "This notebook involves the following steps:\n",
    "1. [Preprocess Data](#preprocess-data)\n",
    "2. [Fine-tune DNABERT](#fine-tune-dnabert)\n",
    "3. [Implement and Train RNN](#implement-and-train-rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the link, you will find 3101 positive and 3101 negative RNA-binding sites for the RNA-binding protein, PUM2. Each file consists of a header followed by an RNA sequence ie. an RNA known to bind with PUM2 (positive) or that does not bind to PUM2 (negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by preprocessing this dataset to be used by DNABERT and our RNN. Since this is a binary classification problem, we will define X as the sequence and corresponding label y as whether this sequence binds to PUM2 (1) or not (0), depending on which file it comes from, such that all sequences in `positives.fa` is assigned a label of `1` and all sequences in `negatives.fa` is assigned a lable of `0`. We must also convert all RNA sequences to DNA sequences to be processed by DNABERT by changing all instances of `U` to `T`. Finally, we must create overlapping k-mers of these sequences to be in the format expected by DNABERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download DNABERT from its repository: https://github.com/jerryji1993/DNABERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNABERT requires python3.7 and must create a conda env to install it then set as kernel for this notebook before starting\n",
    "\n",
    "# %conda create -n dnabert python=3.7\n",
    "# %conda activate dnabert\n",
    "# %conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n",
    "\n",
    "# Clone repo and set up BERT locally \n",
    "# %git clone https://github.com/jerryji1993/DNABERT\n",
    "# %cd DNABERT\n",
    "# %python3 -m pip install --editable .\n",
    "# %cd examples\n",
    "# %python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./DNABERT/src (2.5.0)\n",
      "Requirement already satisfied: numpy in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tokenizers==0.5.0 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: boto3 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (1.33.13)\n",
      "Requirement already satisfied: filelock in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: requests in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: sentencepiece in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.13 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from boto3->transformers) (1.33.13)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from boto3->transformers) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from boto3->transformers) (0.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: six in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from sacremoses->transformers) (1.17.0)\n",
      "Requirement already satisfied: click in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from sacremoses->transformers) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from sacremoses->transformers) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from botocore<1.34.0,>=1.33.13->boto3->transformers) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-metadata in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from click->sacremoses->transformers) (6.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages \n",
    "%pip install transformers pandas numpy pytoch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why k-mers?\n",
    "\n",
    "DNABERT is based on BERT, originally designed for Natural Language Processing (NLP), adapted for DNA language. Traditional language models like BERT requires the input as discrete symbols and thus requires tokenization to convert input into meaningful groups to learn embeddings on and then help attention to learn. \n",
    "\n",
    "DNABERT uses the tokenization startegy of breaking input up into overlapping k-mers. This is in line with biological context and relevance where several biological motifs are made up of these groups within nucleotide sequences, e.g. `TATA` boxes in promoters, `ATG` start codons in Open Reading Frames (ORFs), `AGGT` for splicing signals, etc. This tokenization strategy therefore, allows the model to differentiate between and learn embeddings on these biological motifs, which can then be passed to the attention head to learn meaningful relationships between these motifs. If we consider the case of tokenizing individual sequences then each sequence, sometimes extremely long for DNA, will be considered unique and thus no relationships can be found between more meaningful motifs. In the other extreme, if we tokenize individual nucleotides then these motifs and the biological context also retained from them being overlapping, then this biologically significant contextual information would be lost.\n",
    "\n",
    "For example if we consider the sequence, `ATGGAT`, with an overlapping 3-mer split, we get the tokens `ATG`, `TGG`, `GGA`, and `GAT`. If these were non-overlapping, would get the tokens `ATG` and `GAT` which could miss potential motifs and thus the attention head will not be able to learn any potential relationshios between those with other tokens. If we tokenized this using single-nucleotide tokenization, we get the tokens `A`, `T`, `G`, `G`, `A`, and `T`. Also note that with kmers of length `L`, we get `L - k + 1` tokens and thus nearly the same number of tokens. However, the key difference is that we now obtain much more meaningful and biologically significant groups that the attention head can learn relationships between. With this strategy e.g. the attention head would not be able to differentiate between the `ATG` and `GAT` but could only find relationships between the individual letters, e.g. `A` from `ATG` and `G` from `GAT`, which would be no different to it than `A` and `G` in `ATG`, etc. \n",
    "\n",
    "This would therefore be analogous to a language model trying to find relationships between individual letters in a text instead of words when trying to determine the meaning of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta_into_dic(fasta_file,\n",
    "                        convert_to_rna=False,\n",
    "                        convert_to_dna=True,\n",
    "                        all_uc=True,\n",
    "                        skip_n_seqs=False):\n",
    "    \"\"\"\n",
    "    Read in FASTA sequences from file, return dictionary with mapping:\n",
    "    sequence_id -> sequence\n",
    "\n",
    "    convert_to_rna:\n",
    "        Convert input sequences to RNA.\n",
    "    all_uc:\n",
    "        Convert all sequence characters to uppercase.\n",
    "    skip_n_seqs:\n",
    "        Skip sequences that contain N letters.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert os.path.exists(fasta_file), \"Given FASTA file \\\"%s\\\" not found\" %(fasta_file)\n",
    "\n",
    "    seqs_dic = {}\n",
    "    seq_id = \"\"\n",
    "\n",
    "    with open(fasta_file) as f:\n",
    "        for line in f:\n",
    "            if re.search(\">.+\", line):\n",
    "                m = re.search(\">(.+)\", line)\n",
    "                seq_id = m.group(1)\n",
    "                assert seq_id not in seqs_dic, \"non-unique FASTA header \\\"%s\\\" in \\\"%s\\\"\" % (seq_id, fasta_file)\n",
    "                seqs_dic[seq_id] = \"\"\n",
    "            elif re.search(\"[ACGTUN]+\", line, re.I):\n",
    "                m = re.search(\"([ACGTUN]+)\", line, re.I)\n",
    "                if seq_id in seqs_dic:\n",
    "                    '''if convert_to_rna:\n",
    "                        seqs_dic[seq_id] += m.group(1).replace(\"T\",\"U\").replace(\"t\",\"u\")'''\n",
    "                    if convert_to_dna:\n",
    "                        seqs_dic[seq_id] += m.group(1).replace(\"U\",\"T\").replace(\"u\",\"t\")\n",
    "                    else:\n",
    "                        seqs_dic[seq_id] += m.group(1)\n",
    "                if all_uc:\n",
    "                    seqs_dic[seq_id] = seqs_dic[seq_id].upper()\n",
    "    f.closed\n",
    "\n",
    "    assert seqs_dic, \"no sequences read in (input FASTA file \\\"%s\\\" empty or mal-formatted?)\" %(fasta_file)\n",
    "\n",
    "    # If sequences with N nucleotides should be skipped.\n",
    "    c_skipped_n_ids = 0\n",
    "    if skip_n_seqs:\n",
    "        del_ids = []\n",
    "        for seq_id in seqs_dic:\n",
    "            seq = seqs_dic[seq_id]\n",
    "            if re.search(\"N\", seq, re.I):\n",
    "                print (\"WARNING: sequence with seq_id \\\"%s\\\" in file \\\"%s\\\" contains N nucleotides. Discarding sequence ... \" % (seq_id, fasta_file))\n",
    "                c_skipped_n_ids += 1\n",
    "                del_ids.append(seq_id)\n",
    "        for seq_id in del_ids:\n",
    "            del seqs_dic[seq_id]\n",
    "        assert seqs_dic, \"no sequences remaining after deleting N containing sequences (input FASTA file \\\"%s\\\")\" %(fasta_file)\n",
    "        if c_skipped_n_ids:\n",
    "            print(\"# of N-containing sequences discarded:  %i\" %(c_skipped_n_ids))\n",
    "\n",
    "    return seqs_dic\n",
    "\n",
    "\n",
    "def seq2kmer(seq, k):\n",
    "    \"\"\"\n",
    "    Convert original sequence to kmers\n",
    "    \n",
    "    Arguments:\n",
    "    seq -- str, original sequence.\n",
    "    k -- int, kmer of length k specified.\n",
    "    \n",
    "    Returns:\n",
    "    kmers -- str, kmers separated by space\n",
    "\n",
    "    \"\"\"\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    kmers = \" \".join(kmer)\n",
    "    return kmers\n",
    "\n",
    "\n",
    "def read_seqs(seq_dict, label, k=3, columns=[\"sequence\", \"label\"]):\n",
    "    seqs_kmer = list()\n",
    "    class_label = list()\n",
    "    for item in seq_dict:\n",
    "        seq = seq_dict[item]\n",
    "        k_seq = seq2kmer(seq, k)\n",
    "        seqs_kmer.append(k_seq)\n",
    "        class_label.append(label)\n",
    "    dataframe = pd.DataFrame(zip(seqs_kmer, class_label), columns=columns, index=None)\n",
    "    return dataframe\n",
    "\n",
    "def merge_pos_neg_kfold(pos_seqs, neg_seqs, n_splits=5, k=3, test_train=0.8):\n",
    "    \"\"\"\n",
    "    Merge positive and negative sequences into one DataFrame\n",
    "    Return a list of (train_df, test_df) folds for cross-validation\n",
    "    \"\"\"\n",
    "    # Read sequences -> k-mers\n",
    "    pos_df = read_seqs(pos_seqs, 1, k=k)\n",
    "    neg_df = read_seqs(neg_seqs, 0, k=k)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    merged_df = pd.concat([pos_df, neg_df], axis=0)\n",
    "    merged_df = merged_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # If 0 folds provided then do not create fold splits but use full train and test data\n",
    "    if n_splits == 0:\n",
    "        split_pos = int(test_train * len(merged_df.index))\n",
    "        train = merged_df[:split_pos]\n",
    "        test = merged_df.drop(train.index)\n",
    "        train.to_csv(\"./data/train.tsv\", sep=\"\\t\", index=None)\n",
    "        test.to_csv(\"./data/test.tsv\", sep=\"\\t\", index=None)\n",
    "        print(merged_df)\n",
    "        print()\n",
    "        print(train)\n",
    "        print()\n",
    "        print(test)\n",
    "    else:\n",
    "        # Extract features/labels for StratifiedKFold\n",
    "        X = merged_df[\"sequence\"]\n",
    "        y = merged_df[\"label\"]\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        folds = []\n",
    "        for fold_idx, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "            train_df = merged_df.iloc[train_index].reset_index(drop=True)\n",
    "            test_df = merged_df.iloc[test_index].reset_index(drop=True)\n",
    "            folds.append( (train_df, test_df) )\n",
    "\n",
    "            # Optionally save to disk for each fold\n",
    "            train_df.to_csv(f\"./data/train_fold{fold_idx+1}.tsv\", sep=\"\\t\", index=False)\n",
    "            test_df.to_csv(f\"./data/test_fold{fold_idx+1}.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "        return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# positive sequences:  3101\n",
      "# negative sequences:  3101\n",
      "Fold 1\n",
      "Train shape: (4961, 2)\n",
      "Test shape: (1241, 2)\n",
      "                                               sequence  label\n",
      "0     TGT GTC TCT CTT TTT TTT TTA TAT ATC TCA CAT AT...      0\n",
      "1     CTG TGT GTC TCA CAT ATA TAG AGC GCC CCT CTC TC...      1\n",
      "2     GAA AAA AAC ACC CCT CTG TGC GCC CCT CTG TGT GT...      1\n",
      "3     ATC TCC CCT CTG TGG GGG GGC GCT CTT TTC TCC CC...      0\n",
      "4     AAA AAT ATC TCA CAT ATC TCT CTG TGT GTG TGT GT...      1\n",
      "...                                                 ...    ...\n",
      "4956  TCT CTC TCG CGG GGC GCT CTG TGC GCC CCG CGG GG...      0\n",
      "4957  TGT GTT TTG TGT GTA TAA AAT ATT TTT TTT TTT TT...      0\n",
      "4958  CTA TAG AGT GTG TGC GCC CCT CTG TGG GGG GGA GA...      0\n",
      "4959  AGG GGT GTA TAA AAA AAA AAG AGT GTT TTT TTT TT...      0\n",
      "4960  AAA AAT ATT TTA TAA AAT ATT TTT TTG TGC GCT CT...      1\n",
      "\n",
      "[4961 rows x 2 columns]\n",
      "Fold 2\n",
      "Train shape: (4961, 2)\n",
      "Test shape: (1241, 2)\n",
      "                                               sequence  label\n",
      "0     CTG TGT GTC TCA CAT ATA TAG AGC GCC CCT CTC TC...      1\n",
      "1     GAA AAA AAC ACC CCT CTG TGC GCC CCT CTG TGT GT...      1\n",
      "2     ATC TCC CCT CTG TGG GGG GGC GCT CTT TTC TCC CC...      0\n",
      "3     AAA AAT ATC TCA CAT ATC TCT CTG TGT GTG TGT GT...      1\n",
      "4     TAA AAT ATC TCC CCC CCA CAC ACA CAG AGA GAA AA...      1\n",
      "...                                                 ...    ...\n",
      "4956  GAA AAC ACA CAA AAG AGC GCG CGT GTC TCT CTT TT...      0\n",
      "4957  TTT TTT TTT TTT TTC TCC CCT CTT TTT TTT TTT TT...      0\n",
      "4958  TGC GCA CAT ATG TGC GCA CAG AGT GTG TGG GGT GT...      0\n",
      "4959  CTA TAA AAA AAT ATC TCA CAT ATG TGG GGC GCA CA...      1\n",
      "4960  CTA TAG AGT GTG TGC GCC CCT CTG TGG GGG GGA GA...      0\n",
      "\n",
      "[4961 rows x 2 columns]\n",
      "Fold 3\n",
      "Train shape: (4962, 2)\n",
      "Test shape: (1240, 2)\n",
      "                                               sequence  label\n",
      "0     TGT GTC TCT CTT TTT TTT TTA TAT ATC TCA CAT AT...      0\n",
      "1     CTG TGT GTC TCA CAT ATA TAG AGC GCC CCT CTC TC...      1\n",
      "2     GAA AAA AAC ACC CCT CTG TGC GCC CCT CTG TGT GT...      1\n",
      "3     ATC TCC CCT CTG TGG GGG GGC GCT CTT TTC TCC CC...      0\n",
      "4     TAT ATA TAT ATA TAT ATG TGT GTG TGA GAA AAT AT...      1\n",
      "...                                                 ...    ...\n",
      "4957  CTA TAA AAA AAT ATC TCA CAT ATG TGG GGC GCA CA...      1\n",
      "4958  TCT CTC TCG CGG GGC GCT CTG TGC GCC CCG CGG GG...      0\n",
      "4959  TGT GTT TTG TGT GTA TAA AAT ATT TTT TTT TTT TT...      0\n",
      "4960  AGG GGT GTA TAA AAA AAA AAG AGT GTT TTT TTT TT...      0\n",
      "4961  AAA AAT ATT TTA TAA AAT ATT TTT TTG TGC GCT CT...      1\n",
      "\n",
      "[4962 rows x 2 columns]\n",
      "Fold 4\n",
      "Train shape: (4962, 2)\n",
      "Test shape: (1240, 2)\n",
      "                                               sequence  label\n",
      "0     TGT GTC TCT CTT TTT TTT TTA TAT ATC TCA CAT AT...      0\n",
      "1     CTG TGT GTC TCA CAT ATA TAG AGC GCC CCT CTC TC...      1\n",
      "2     ATC TCC CCT CTG TGG GGG GGC GCT CTT TTC TCC CC...      0\n",
      "3     AAA AAT ATC TCA CAT ATC TCT CTG TGT GTG TGT GT...      1\n",
      "4     TAA AAT ATC TCC CCC CCA CAC ACA CAG AGA GAA AA...      1\n",
      "...                                                 ...    ...\n",
      "4957  TCT CTC TCG CGG GGC GCT CTG TGC GCC CCG CGG GG...      0\n",
      "4958  TGT GTT TTG TGT GTA TAA AAT ATT TTT TTT TTT TT...      0\n",
      "4959  CTA TAG AGT GTG TGC GCC CCT CTG TGG GGG GGA GA...      0\n",
      "4960  AGG GGT GTA TAA AAA AAA AAG AGT GTT TTT TTT TT...      0\n",
      "4961  AAA AAT ATT TTA TAA AAT ATT TTT TTG TGC GCT CT...      1\n",
      "\n",
      "[4962 rows x 2 columns]\n",
      "Fold 5\n",
      "Train shape: (4962, 2)\n",
      "Test shape: (1240, 2)\n",
      "                                               sequence  label\n",
      "0     TGT GTC TCT CTT TTT TTT TTA TAT ATC TCA CAT AT...      0\n",
      "1     GAA AAA AAC ACC CCT CTG TGC GCC CCT CTG TGT GT...      1\n",
      "2     AAA AAT ATC TCA CAT ATC TCT CTG TGT GTG TGT GT...      1\n",
      "3     TAA AAT ATC TCC CCC CCA CAC ACA CAG AGA GAA AA...      1\n",
      "4     TAT ATA TAT ATA TAT ATG TGT GTG TGA GAA AAT AT...      1\n",
      "...                                                 ...    ...\n",
      "4957  TCT CTC TCG CGG GGC GCT CTG TGC GCC CCG CGG GG...      0\n",
      "4958  TGT GTT TTG TGT GTA TAA AAT ATT TTT TTT TTT TT...      0\n",
      "4959  CTA TAG AGT GTG TGC GCC CCT CTG TGG GGG GGA GA...      0\n",
      "4960  AGG GGT GTA TAA AAA AAA AAG AGT GTT TTT TTT TT...      0\n",
      "4961  AAA AAT ATT TTA TAA AAT ATT TTT TTG TGC GCT CT...      1\n",
      "\n",
      "[4962 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "pos_fasta = \"./data/positives.fa\"\n",
    "neg_fasta = \"./data/negatives.fa\"\n",
    "\n",
    "# Load FASTA sequences into dictionaries.\n",
    "pos_seqs_dic = read_fasta_into_dic(pos_fasta, \n",
    "                                    convert_to_rna=True, \n",
    "                                    all_uc=True,\n",
    "                                    skip_n_seqs=True)\n",
    "neg_seqs_dic = read_fasta_into_dic(neg_fasta,\n",
    "                                    convert_to_rna=True, \n",
    "                                    all_uc=True,\n",
    "                                    skip_n_seqs=True)\n",
    "\n",
    "print(\"# positive sequences:  %i\" %(len(pos_seqs_dic)))\n",
    "print(\"# negative sequences:  %i\" %(len(neg_seqs_dic)))\n",
    "\n",
    "# Split train and test data into 5 folds\n",
    "folds = merge_pos_neg_kfold(pos_seqs_dic, neg_seqs_dic, n_splits=5)\n",
    "\n",
    "for i, (train_df, test_df) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    print(\"Train shape:\", train_df.shape)\n",
    "    print(\"Test shape:\", test_df.shape)\n",
    "    print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune DNABERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will download a pre-trained version of DNABERT and set it up locally to fine-tune it on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pre-trained DNABERT3 from the following link and unzip it: https://drive.google.com/file/d/1nVBaIoiJpnwQxiz4dSq6Sv9kBKfXhZuM/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./dnabert3/3-new-12w-0.zip\n",
      "replace ./3-new-12w-0/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip ./dnabert3/3-new-12w-0.zip -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shell file will run `finetune.py` to fine-tune the pre-trained model we downloaded. The shell file contains all the parameters needed to fine-tune DNABERT, including the path to the pre-trained model downloaded previously and our data we preprocessed. Modify the paths to the files as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd DNABERT/examples\n",
    "\n",
    "export KMER=3\n",
    "export MODEL_PATH='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/dnabert3/3-new-12w-0'\n",
    "export DATA_PATH='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data'\n",
    "export OUTPUT_PATH='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output'\n",
    "\n",
    "# Run the finetuning 5 times for 5-fold cross-validation\n",
    "for i in 1 2 3 4 5\n",
    "do\n",
    "    echo \"=== Starting training for fold $i ===\"\n",
    "\n",
    "    # Copy the correct fold's train & test files into DNABERT's train/test filenames\n",
    "    cp $DATA_PATH/train_fold${i}.tsv $DATA_PATH/train.tsv\n",
    "    cp $DATA_PATH/test_fold${i}.tsv  $DATA_PATH/test.tsv\n",
    "\n",
    "    # Run DNABERT\n",
    "    python run_finetune.py \\\n",
    "        --model_type dna \\\n",
    "        --tokenizer_name=dna$KMER \\\n",
    "        --model_name_or_path $MODEL_PATH \\\n",
    "        --task_name dnaprom \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --data_dir $DATA_PATH \\\n",
    "        --max_seq_length 100 \\\n",
    "        --per_gpu_eval_batch_size=32 \\\n",
    "        --per_gpu_train_batch_size=32 \\\n",
    "        --learning_rate 2e-4 \\\n",
    "        --num_train_epochs 5.0 \\\n",
    "        --output_dir $OUTPUT_PATH/fold_${i} \\\n",
    "        --evaluate_during_training \\\n",
    "        --logging_steps 100 \\\n",
    "        --save_steps 4000 \\\n",
    "        --warmup_percent 0.1 \\\n",
    "        --hidden_dropout_prob 0.1 \\\n",
    "        --overwrite_output \\\n",
    "        --weight_decay 0.01 \\\n",
    "        --n_process 8\n",
    "done\n",
    "\n",
    "echo \"=== All folds completed! ===\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you may instead uncomment the cell below to run the same fine-tuning script for DNABERT on the full dataset (without cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "{\"eval_acc\": 0.9008863819500403, \"eval_f1\": 0.9008861245249425, \"eval_mcc\": 0.8028972501166461, \"eval_auc\": 0.9608326186949403, \"eval_precision\": 0.901480969361764, \"eval_recall\": 0.9014162833606195, \"learning_rate\": 0.00019373219373219373, \"loss\": 0.5076427334547042, \"step\": 100}\n",
      "{\"eval_acc\": 0.9016921837228042, \"eval_f1\": 0.9016308020167368, \"eval_mcc\": 0.8032662931269479, \"eval_auc\": 0.9572646241002053, \"eval_precision\": 0.9016655057336231, \"eval_recall\": 0.9016007900002598, \"learning_rate\": 0.00016524216524216523, \"loss\": 0.2715525978431106, \"step\": 200}\n",
      "{\"eval_acc\": 0.9145850120870266, \"eval_f1\": 0.9142549108562905, \"eval_mcc\": 0.8312696990541014, \"eval_auc\": 0.9699553026168758, \"eval_precision\": 0.9176453764490415, \"eval_recall\": 0.9136340011953952, \"learning_rate\": 0.00013675213675213676, \"loss\": 0.25035233695060016, \"step\": 300}\n",
      "{\"eval_acc\": 0.9121676067687349, \"eval_f1\": 0.911887262804842, \"eval_mcc\": 0.8257795050944344, \"eval_auc\": 0.9698266677061407, \"eval_precision\": 0.9144381316437137, \"eval_recall\": 0.911347158337881, \"learning_rate\": 0.00010826210826210828, \"loss\": 0.1639057238586247, \"step\": 400}\n",
      "{\"eval_acc\": 0.9137792103142627, \"eval_f1\": 0.9134880060355841, \"eval_mcc\": 0.8291862804272823, \"eval_auc\": 0.9663132454977781, \"eval_precision\": 0.9162710825382354, \"eval_recall\": 0.9129219614874873, \"learning_rate\": 7.977207977207978e-05, \"loss\": 0.1345354856015183, \"step\": 500}\n",
      "{\"eval_acc\": 0.9234488315874295, \"eval_f1\": 0.9234201902559589, \"eval_mcc\": 0.8468703821049481, \"eval_auc\": 0.969267950417089, \"eval_precision\": 0.9233691840074819, \"eval_recall\": 0.9235012083885554, \"learning_rate\": 5.128205128205128e-05, \"loss\": 0.07473652851069346, \"step\": 600}\n",
      "{\"eval_acc\": 0.9250604351329573, \"eval_f1\": 0.9249661117774983, \"eval_mcc\": 0.8502031845757121, \"eval_auc\": 0.9699462072191471, \"eval_precision\": 0.9254666146036705, \"eval_recall\": 0.9247368831371326, \"learning_rate\": 2.2792022792022794e-05, \"loss\": 0.038427784763043746, \"step\": 700}\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2025 17:11:21 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "01/24/2025 17:11:21 - INFO - transformers.configuration_utils -   loading configuration file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/dnabert3/3-new-12w-0/config.json\n",
      "01/24/2025 17:11:21 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 10,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 69\n",
      "}\n",
      "\n",
      "01/24/2025 17:11:21 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-3/vocab.txt from cache at /home/koeksalr/.cache/torch/transformers/e1e7221d086d0af09215b2c6ef3ded41de274c79ace1930c48dfce242a7b36fa.b24b7bce4d95258cccdbc46b651c8283db3a0f1324fb97567c8b22b19970f82c\n",
      "01/24/2025 17:11:21 - INFO - transformers.modeling_utils -   loading weights file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/dnabert3/3-new-12w-0/pytorch_model.bin\n",
      "01/24/2025 17:11:22 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "01/24/2025 17:11:22 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "01/24/2025 17:11:22 - INFO - __main__ -   finish loading model\n",
      "01/24/2025 17:11:22 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, attention_probs_dropout_prob=0.1, beta1=0.9, beta2=0.999, cache_dir='', config_name='', data_dir='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data', device=device(type='cpu'), do_ensemble_pred=False, do_eval=True, do_lower_case=False, do_predict=False, do_train=True, do_visualize=False, early_stop=0, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hidden_dropout_prob=0.1, learning_rate=0.0002, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=100, max_steps=-1, model_name_or_path='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/dnabert3/3-new-12w-0', model_type='dna', n_gpu=0, n_process=8, no_cuda=False, num_rnn_layer=2, num_train_epochs=5.0, output_dir='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_pred_batch_size=8, per_gpu_train_batch_size=32, predict_dir=None, predict_scan_size=1, result_dir=None, rnn='lstm', rnn_dropout=0.0, rnn_hidden=768, save_steps=4000, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, task_name='dnaprom', tokenizer_name='dna3', visualize_data_dir=None, visualize_models=None, visualize_train=False, warmup_percent=0.1, warmup_steps=0, weight_decay=0.01)\n",
      "01/24/2025 17:11:22 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_train_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 17:11:23 - INFO - __main__ -   ***** Running training *****\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Num examples = 4961\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Num Epochs = 5\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Total optimization steps = 780\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Continuing training from epoch 0\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Continuing training from global step 0\n",
      "01/24/2025 17:11:23 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/156 [00:00<?, ?it/s]\u001b[A/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/DNABERT/src/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1664817695526/work/torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "\n",
      "Iteration:   1%|          | 1/156 [00:07<19:36,  7.59s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 2/156 [00:14<19:11,  7.48s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 3/156 [00:22<18:57,  7.43s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/156 [00:29<18:22,  7.26s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 5/156 [00:36<17:55,  7.12s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 6/156 [00:43<17:40,  7.07s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 7/156 [00:50<17:28,  7.04s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 8/156 [00:57<17:15,  7.00s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 9/156 [01:04<17:11,  7.01s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 10/156 [01:11<17:00,  6.99s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 11/156 [01:18<16:51,  6.98s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 12/156 [01:25<16:53,  7.04s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 13/156 [01:32<16:39,  6.99s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 14/156 [01:39<16:29,  6.97s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 15/156 [01:46<16:26,  6.99s/it]\u001b[A\n",
      "Iteration:  10%|█         | 16/156 [01:52<16:15,  6.97s/it]\u001b[A\n",
      "Iteration:  11%|█         | 17/156 [02:00<16:16,  7.02s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/156 [02:07<16:04,  6.99s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/156 [02:13<15:55,  6.98s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/156 [02:21<15:53,  7.01s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 21/156 [02:28<15:43,  6.99s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/156 [02:34<15:34,  6.97s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/156 [02:41<15:25,  6.96s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 24/156 [02:48<15:17,  6.95s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/156 [02:55<15:07,  6.93s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/156 [03:02<14:59,  6.92s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 27/156 [03:09<14:54,  6.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/156 [03:16<14:49,  6.95s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 29/156 [03:23<14:45,  6.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/156 [03:30<14:36,  6.96s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 31/156 [03:37<14:28,  6.95s/it]\u001b[A\n",
      "Iteration:  21%|██        | 32/156 [03:44<14:22,  6.96s/it]\u001b[A\n",
      "Iteration:  21%|██        | 33/156 [03:51<14:14,  6.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/156 [03:58<14:06,  6.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 35/156 [04:05<13:59,  6.93s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/156 [04:12<13:55,  6.96s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 37/156 [04:19<13:48,  6.96s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 38/156 [04:26<13:49,  7.03s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/156 [04:33<13:39,  7.01s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/156 [04:40<13:30,  6.99s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 41/156 [04:47<13:21,  6.97s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/156 [04:54<13:16,  6.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/156 [05:01<13:07,  6.97s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 44/156 [05:08<13:00,  6.97s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/156 [05:15<12:56,  6.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 46/156 [05:22<12:51,  7.01s/it]\u001b[A\n",
      "Iteration:  30%|███       | 47/156 [05:29<12:40,  6.98s/it]\u001b[A\n",
      "Iteration:  31%|███       | 48/156 [05:36<12:31,  6.96s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 49/156 [05:43<12:27,  6.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/156 [05:49<12:17,  6.96s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/156 [05:56<12:10,  6.95s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 52/156 [06:03<12:01,  6.94s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/156 [06:10<11:55,  6.95s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 54/156 [06:17<11:48,  6.95s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 55/156 [06:24<11:43,  6.97s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 56/156 [06:31<11:34,  6.94s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/156 [06:38<11:27,  6.94s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 58/156 [06:45<11:20,  6.94s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/156 [06:52<11:12,  6.93s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 60/156 [06:59<11:06,  6.94s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 61/156 [07:06<10:58,  6.93s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 62/156 [07:13<10:53,  6.95s/it]\u001b[A\n",
      "Iteration:  40%|████      | 63/156 [07:20<10:53,  7.03s/it]\u001b[A\n",
      "Iteration:  41%|████      | 64/156 [07:27<10:46,  7.03s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/156 [07:34<10:36,  6.99s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 66/156 [07:41<10:28,  6.98s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 67/156 [07:48<10:19,  6.96s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 68/156 [07:55<10:11,  6.95s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 69/156 [08:02<10:04,  6.95s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 70/156 [08:09<09:56,  6.93s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/156 [08:16<09:50,  6.95s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 72/156 [08:23<09:52,  7.05s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/156 [08:30<09:41,  7.00s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 74/156 [08:37<09:31,  6.97s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 75/156 [08:44<09:24,  6.97s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 76/156 [08:50<09:16,  6.95s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 77/156 [08:57<09:08,  6.94s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 78/156 [09:04<09:01,  6.94s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 79/156 [09:11<08:54,  6.94s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 80/156 [09:18<08:47,  6.94s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 81/156 [09:25<08:42,  6.97s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/156 [09:32<08:34,  6.96s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 83/156 [09:39<08:27,  6.95s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 84/156 [09:46<08:19,  6.94s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 85/156 [09:53<08:13,  6.95s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 86/156 [10:00<08:05,  6.94s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 87/156 [10:07<07:57,  6.92s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 88/156 [10:14<07:51,  6.94s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 89/156 [10:21<07:47,  6.97s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/156 [10:28<07:41,  6.99s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 91/156 [10:35<07:32,  6.97s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 92/156 [10:42<07:26,  6.98s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 93/156 [10:49<07:18,  6.96s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 94/156 [10:56<07:11,  6.96s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 95/156 [11:03<07:04,  6.95s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/156 [11:10<06:56,  6.95s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 97/156 [11:16<06:49,  6.93s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 98/156 [11:24<06:44,  6.98s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 99/156 [11:30<06:35,  6.95s/it]\u001b[A01/24/2025 17:23:00 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_dev_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 17:23:00 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/24/2025 17:23:00 - INFO - __main__ -     Num examples = 1241\n",
      "01/24/2025 17:23:00 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   3%|▎         | 1/39 [00:02<01:17,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   5%|▌         | 2/39 [00:04<01:14,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 3/39 [00:06<01:13,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 4/39 [00:08<01:10,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  13%|█▎        | 5/39 [00:10<01:08,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  15%|█▌        | 6/39 [00:12<01:05,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 7/39 [00:14<01:03,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  21%|██        | 8/39 [00:16<01:01,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  23%|██▎       | 9/39 [00:18<00:59,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 10/39 [00:19<00:57,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 11/39 [00:21<00:55,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  31%|███       | 12/39 [00:23<00:53,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|███▎      | 13/39 [00:25<00:51,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 14/39 [00:27<00:49,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 15/39 [00:29<00:47,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  41%|████      | 16/39 [00:31<00:45,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▎     | 17/39 [00:33<00:43,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 18/39 [00:35<00:41,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  49%|████▊     | 19/39 [00:38<00:41,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  51%|█████▏    | 20/39 [00:40<00:40,  2.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 21/39 [00:42<00:37,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▋    | 22/39 [00:44<00:35,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  59%|█████▉    | 23/39 [00:46<00:32,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 24/39 [00:48<00:30,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 25/39 [00:50<00:27,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|██████▋   | 26/39 [00:52<00:25,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  69%|██████▉   | 27/39 [00:54<00:23,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 28/39 [00:56<00:21,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 29/39 [00:58<00:19,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  77%|███████▋  | 30/39 [01:00<00:17,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  79%|███████▉  | 31/39 [01:02<00:15,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 32/39 [01:04<00:13,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  85%|████████▍ | 33/39 [01:05<00:11,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  87%|████████▋ | 34/39 [01:07<00:09,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|████████▉ | 35/39 [01:09<00:07,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 36/39 [01:11<00:05,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  95%|█████████▍| 37/39 [01:13<00:03,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  97%|█████████▋| 38/39 [01:15<00:01,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 39/39 [01:17<00:00,  1.98s/it]\u001b[A\u001b[A\n",
      "01/24/2025 17:24:18 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/24/2025 17:24:18 - INFO - __main__ -     acc = 0.9008863819500403\n",
      "01/24/2025 17:24:18 - INFO - __main__ -     auc = 0.9608326186949403\n",
      "01/24/2025 17:24:18 - INFO - __main__ -     f1 = 0.9008861245249425\n",
      "01/24/2025 17:24:18 - INFO - __main__ -     mcc = 0.8028972501166461\n",
      "01/24/2025 17:24:18 - INFO - __main__ -     precision = 0.901480969361764\n",
      "01/24/2025 17:24:18 - INFO - __main__ -     recall = 0.9014162833606195\n",
      "/home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  64%|██████▍   | 100/156 [12:55<28:08, 30.16s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 101/156 [13:02<21:15, 23.19s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 102/156 [13:09<16:28, 18.30s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 103/156 [13:16<13:09, 14.90s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 104/156 [13:23<10:53, 12.56s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 105/156 [13:30<09:14, 10.87s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 106/156 [13:36<08:03,  9.67s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 107/156 [13:43<07:14,  8.87s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 108/156 [13:50<06:37,  8.29s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 109/156 [13:57<06:10,  7.89s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 110/156 [14:04<05:49,  7.60s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 111/156 [14:11<05:33,  7.41s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 112/156 [14:18<05:19,  7.27s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 113/156 [14:25<05:09,  7.20s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 114/156 [14:32<04:58,  7.11s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 115/156 [14:39<04:49,  7.06s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 116/156 [14:46<04:40,  7.02s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 117/156 [14:53<04:32,  6.98s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 118/156 [15:00<04:24,  6.97s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 119/156 [15:07<04:17,  6.95s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 120/156 [15:14<04:10,  6.95s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 121/156 [15:21<04:04,  6.99s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 122/156 [15:28<03:56,  6.96s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 123/156 [15:35<03:49,  6.95s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 124/156 [15:41<03:42,  6.96s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 125/156 [15:48<03:35,  6.94s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 126/156 [15:55<03:28,  6.96s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 127/156 [16:02<03:21,  6.94s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 128/156 [16:09<03:14,  6.93s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 129/156 [16:16<03:07,  6.95s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 130/156 [16:23<03:02,  7.00s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 131/156 [16:30<02:54,  6.99s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 132/156 [16:37<02:47,  6.98s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 133/156 [16:44<02:40,  6.98s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 134/156 [16:51<02:33,  6.97s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 135/156 [16:58<02:26,  6.97s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 136/156 [17:05<02:19,  6.98s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 137/156 [17:12<02:13,  7.00s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 138/156 [17:19<02:06,  7.00s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 139/156 [17:26<01:59,  7.01s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 140/156 [17:33<01:51,  6.98s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 141/156 [17:40<01:44,  6.96s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 142/156 [17:47<01:38,  7.01s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 143/156 [17:54<01:30,  6.99s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 144/156 [18:01<01:23,  6.99s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 145/156 [18:08<01:16,  6.98s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 146/156 [18:15<01:09,  6.99s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 147/156 [18:22<01:02,  6.98s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 148/156 [18:29<00:55,  6.96s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 149/156 [18:36<00:48,  6.94s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 150/156 [18:43<00:41,  7.00s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 151/156 [18:50<00:34,  6.97s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 152/156 [18:57<00:27,  6.97s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 153/156 [19:04<00:20,  6.97s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 154/156 [19:11<00:13,  6.95s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 155/156 [19:18<00:06,  6.96s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 156/156 [19:18<00:00,  7.43s/it]\u001b[A\n",
      "Epoch:  20%|██        | 1/5 [19:18<1:17:15, 1158.89s/it]\n",
      "Iteration:   0%|          | 0/156 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/156 [00:07<18:29,  7.16s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 2/156 [00:14<18:07,  7.06s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 3/156 [00:21<17:55,  7.03s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/156 [00:28<17:43,  7.00s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 5/156 [00:35<17:33,  6.98s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 6/156 [00:41<17:24,  6.96s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 7/156 [00:49<17:22,  6.99s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 8/156 [00:56<17:14,  6.99s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 9/156 [01:03<17:18,  7.07s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 10/156 [01:10<17:06,  7.03s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 11/156 [01:17<16:54,  6.99s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 12/156 [01:24<16:47,  7.00s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 13/156 [01:31<16:38,  6.98s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 14/156 [01:37<16:29,  6.97s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 15/156 [01:44<16:19,  6.95s/it]\u001b[A\n",
      "Iteration:  10%|█         | 16/156 [01:52<16:30,  7.08s/it]\u001b[A\n",
      "Iteration:  11%|█         | 17/156 [01:59<16:20,  7.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/156 [02:06<16:15,  7.07s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/156 [02:13<16:01,  7.02s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/156 [02:20<15:51,  6.99s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 21/156 [02:27<15:44,  6.99s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/156 [02:34<15:36,  6.99s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/156 [02:41<15:26,  6.97s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 24/156 [02:48<15:20,  6.97s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/156 [02:55<15:12,  6.96s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/156 [03:02<15:11,  7.01s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 27/156 [03:09<15:22,  7.15s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/156 [03:16<15:12,  7.13s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 29/156 [03:23<14:58,  7.08s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/156 [03:30<14:48,  7.05s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 31/156 [03:37<14:38,  7.03s/it]\u001b[A\n",
      "Iteration:  21%|██        | 32/156 [03:44<14:35,  7.06s/it]\u001b[A\n",
      "Iteration:  21%|██        | 33/156 [03:51<14:24,  7.03s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/156 [03:58<14:16,  7.02s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 35/156 [04:05<14:15,  7.07s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/156 [04:13<14:11,  7.10s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 37/156 [04:20<13:59,  7.05s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 38/156 [04:27<13:54,  7.07s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/156 [04:34<13:43,  7.04s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/156 [04:41<13:33,  7.01s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 41/156 [04:47<13:23,  6.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/156 [04:54<13:16,  6.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/156 [05:02<13:12,  7.02s/it]\u001b[A01/24/2025 17:35:51 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_dev_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 17:35:51 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/24/2025 17:35:51 - INFO - __main__ -     Num examples = 1241\n",
      "01/24/2025 17:35:51 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   3%|▎         | 1/39 [00:01<01:15,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   5%|▌         | 2/39 [00:03<01:13,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 3/39 [00:05<01:11,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 4/39 [00:07<01:09,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  13%|█▎        | 5/39 [00:09<01:07,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  15%|█▌        | 6/39 [00:11<01:06,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 7/39 [00:14<01:04,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  21%|██        | 8/39 [00:16<01:02,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  23%|██▎       | 9/39 [00:18<01:00,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 10/39 [00:20<00:58,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 11/39 [00:22<00:56,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  31%|███       | 12/39 [00:24<00:55,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|███▎      | 13/39 [00:26<00:52,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 14/39 [00:28<00:50,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 15/39 [00:30<00:48,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  41%|████      | 16/39 [00:32<00:46,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▎     | 17/39 [00:34<00:44,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 18/39 [00:36<00:42,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  49%|████▊     | 19/39 [00:38<00:40,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  51%|█████▏    | 20/39 [00:40<00:38,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 21/39 [00:42<00:36,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▋    | 22/39 [00:44<00:34,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  59%|█████▉    | 23/39 [00:46<00:32,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 24/39 [00:48<00:30,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 25/39 [00:50<00:28,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|██████▋   | 26/39 [00:52<00:26,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  69%|██████▉   | 27/39 [00:54<00:25,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 28/39 [00:56<00:22,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 29/39 [00:58<00:20,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  77%|███████▋  | 30/39 [01:00<00:18,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  79%|███████▉  | 31/39 [01:02<00:16,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 32/39 [01:04<00:14,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  85%|████████▍ | 33/39 [01:06<00:12,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  87%|████████▋ | 34/39 [01:08<00:10,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|████████▉ | 35/39 [01:10<00:08,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 36/39 [01:12<00:06,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  95%|█████████▍| 37/39 [01:14<00:04,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  97%|█████████▋| 38/39 [01:16<00:02,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 39/39 [01:18<00:00,  2.01s/it]\u001b[A\u001b[A\n",
      "01/24/2025 17:37:09 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/24/2025 17:37:09 - INFO - __main__ -     acc = 0.9016921837228042\n",
      "01/24/2025 17:37:09 - INFO - __main__ -     auc = 0.9572646241002053\n",
      "01/24/2025 17:37:09 - INFO - __main__ -     f1 = 0.9016308020167368\n",
      "01/24/2025 17:37:09 - INFO - __main__ -     mcc = 0.8032662931269479\n",
      "01/24/2025 17:37:09 - INFO - __main__ -     precision = 0.9016655057336231\n",
      "01/24/2025 17:37:09 - INFO - __main__ -     recall = 0.9016007900002598\n",
      "/home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  28%|██▊       | 44/156 [06:27<57:00, 30.54s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/156 [06:34<43:22, 23.45s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 46/156 [06:41<33:54, 18.49s/it]\u001b[A\n",
      "Iteration:  30%|███       | 47/156 [06:48<27:17, 15.03s/it]\u001b[A\n",
      "Iteration:  31%|███       | 48/156 [06:55<22:42, 12.62s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 49/156 [07:02<19:39, 11.02s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/156 [07:09<17:18,  9.80s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/156 [07:16<15:37,  8.93s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 52/156 [07:23<14:29,  8.36s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/156 [07:30<13:39,  7.96s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 54/156 [07:37<13:01,  7.66s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 55/156 [07:44<12:31,  7.44s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 56/156 [07:51<12:09,  7.29s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/156 [07:58<11:50,  7.18s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 58/156 [08:05<11:46,  7.21s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/156 [08:12<11:31,  7.13s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 60/156 [08:19<11:18,  7.07s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 61/156 [08:26<11:09,  7.04s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 62/156 [08:33<10:59,  7.02s/it]\u001b[A\n",
      "Iteration:  40%|████      | 63/156 [08:40<10:50,  7.00s/it]\u001b[A\n",
      "Iteration:  41%|████      | 64/156 [08:47<10:41,  6.97s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/156 [08:54<10:33,  6.97s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 66/156 [09:01<10:27,  6.97s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 67/156 [09:08<10:20,  6.98s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 68/156 [09:15<10:13,  6.97s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 69/156 [09:22<10:06,  6.98s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 70/156 [09:28<09:59,  6.98s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/156 [09:35<09:50,  6.95s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 72/156 [09:42<09:43,  6.94s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/156 [09:49<09:36,  6.95s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 74/156 [09:56<09:29,  6.95s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 75/156 [10:03<09:25,  6.99s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 76/156 [10:10<09:18,  6.98s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 77/156 [10:17<09:09,  6.96s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 78/156 [10:24<09:04,  6.98s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 79/156 [10:31<08:58,  6.99s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 80/156 [10:38<08:48,  6.95s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 81/156 [10:45<08:41,  6.95s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/156 [10:52<08:37,  6.99s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 83/156 [10:59<08:28,  6.97s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 84/156 [11:06<08:25,  7.01s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 85/156 [11:13<08:16,  6.99s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 86/156 [11:20<08:07,  6.96s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 87/156 [11:27<08:03,  7.01s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 88/156 [11:34<07:55,  7.00s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 89/156 [11:41<07:49,  7.00s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/156 [11:48<07:40,  6.98s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 91/156 [11:55<07:34,  6.99s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 92/156 [12:02<07:29,  7.02s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 93/156 [12:09<07:22,  7.02s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 94/156 [12:16<07:13,  6.99s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 95/156 [12:23<07:05,  6.98s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/156 [12:30<07:02,  7.04s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 97/156 [12:37<06:53,  7.00s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 98/156 [12:44<06:45,  6.99s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 99/156 [12:51<06:37,  6.98s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 100/156 [12:58<06:31,  7.00s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 101/156 [13:05<06:24,  6.99s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 102/156 [13:12<06:16,  6.98s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 103/156 [13:19<06:12,  7.03s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 104/156 [13:26<06:05,  7.02s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 105/156 [13:33<05:57,  7.00s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 106/156 [13:40<05:48,  6.98s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 107/156 [13:47<05:41,  6.98s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 108/156 [13:54<05:36,  7.00s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 109/156 [14:01<05:29,  7.01s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 110/156 [14:08<05:22,  7.02s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 111/156 [14:15<05:15,  7.01s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 112/156 [14:22<05:07,  6.99s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 113/156 [14:29<05:00,  7.00s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 114/156 [14:36<04:54,  7.02s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 115/156 [14:43<04:46,  7.00s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 116/156 [14:50<04:39,  6.98s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 117/156 [14:57<04:32,  6.98s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 118/156 [15:04<04:28,  7.06s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 119/156 [15:12<04:27,  7.22s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 120/156 [15:19<04:16,  7.13s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 121/156 [15:26<04:08,  7.09s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 122/156 [15:33<03:59,  7.04s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 123/156 [15:40<03:50,  7.00s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 124/156 [15:47<03:43,  6.97s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 125/156 [15:54<03:36,  6.98s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 126/156 [16:00<03:28,  6.97s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 127/156 [16:07<03:22,  6.97s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 128/156 [16:14<03:14,  6.96s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 129/156 [16:21<03:07,  6.96s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 130/156 [16:28<03:00,  6.95s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 131/156 [16:35<02:53,  6.95s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 132/156 [16:42<02:46,  6.94s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 133/156 [16:49<02:39,  6.94s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 134/156 [16:56<02:32,  6.95s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 135/156 [17:03<02:28,  7.06s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 136/156 [17:10<02:20,  7.02s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 137/156 [17:17<02:12,  6.98s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 138/156 [17:24<02:06,  7.02s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 139/156 [17:31<01:59,  7.02s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 140/156 [17:38<01:51,  6.99s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 141/156 [17:45<01:44,  6.97s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 142/156 [17:52<01:37,  6.97s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 143/156 [17:59<01:30,  6.97s/it]\u001b[A01/24/2025 17:48:48 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_dev_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 17:48:48 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/24/2025 17:48:48 - INFO - __main__ -     Num examples = 1241\n",
      "01/24/2025 17:48:48 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   3%|▎         | 1/39 [00:01<01:15,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   5%|▌         | 2/39 [00:04<01:14,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 3/39 [00:05<01:11,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 4/39 [00:08<01:11,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  13%|█▎        | 5/39 [00:10<01:08,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  15%|█▌        | 6/39 [00:12<01:05,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 7/39 [00:14<01:03,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  21%|██        | 8/39 [00:15<01:01,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  23%|██▎       | 9/39 [00:18<01:00,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 10/39 [00:20<00:58,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 11/39 [00:22<00:56,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  31%|███       | 12/39 [00:24<00:53,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|███▎      | 13/39 [00:25<00:51,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 14/39 [00:27<00:49,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 15/39 [00:29<00:47,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  41%|████      | 16/39 [00:31<00:45,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▎     | 17/39 [00:33<00:43,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 18/39 [00:35<00:41,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  49%|████▊     | 19/39 [00:37<00:39,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  51%|█████▏    | 20/39 [00:39<00:37,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 21/39 [00:41<00:35,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▋    | 22/39 [00:43<00:33,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  59%|█████▉    | 23/39 [00:45<00:31,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 24/39 [00:47<00:29,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 25/39 [00:49<00:28,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|██████▋   | 26/39 [00:51<00:25,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  69%|██████▉   | 27/39 [00:53<00:23,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 28/39 [00:55<00:21,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 29/39 [00:57<00:19,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  77%|███████▋  | 30/39 [00:59<00:17,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  79%|███████▉  | 31/39 [01:01<00:15,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 32/39 [01:03<00:13,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  85%|████████▍ | 33/39 [01:05<00:11,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  87%|████████▋ | 34/39 [01:07<00:09,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|████████▉ | 35/39 [01:09<00:07,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 36/39 [01:11<00:05,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  95%|█████████▍| 37/39 [01:13<00:03,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  97%|█████████▋| 38/39 [01:15<00:01,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 39/39 [01:17<00:00,  1.98s/it]\u001b[A\u001b[A\n",
      "01/24/2025 17:50:05 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/24/2025 17:50:05 - INFO - __main__ -     acc = 0.9145850120870266\n",
      "01/24/2025 17:50:05 - INFO - __main__ -     auc = 0.9699553026168758\n",
      "01/24/2025 17:50:05 - INFO - __main__ -     f1 = 0.9142549108562905\n",
      "01/24/2025 17:50:05 - INFO - __main__ -     mcc = 0.8312696990541014\n",
      "01/24/2025 17:50:05 - INFO - __main__ -     precision = 0.9176453764490415\n",
      "01/24/2025 17:50:05 - INFO - __main__ -     recall = 0.9136340011953952\n",
      "/home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  92%|█████████▏| 144/156 [19:23<06:02, 30.18s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 145/156 [19:30<04:15, 23.23s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 146/156 [19:37<03:03, 18.33s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 147/156 [19:44<02:14, 14.90s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 148/156 [19:51<01:40, 12.53s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 149/156 [19:58<01:16, 10.87s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 150/156 [20:05<00:58,  9.69s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 151/156 [20:12<00:44,  8.87s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 152/156 [20:19<00:33,  8.29s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 153/156 [20:26<00:23,  7.90s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 154/156 [20:33<00:15,  7.60s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 155/156 [20:40<00:07,  7.39s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 156/156 [20:40<00:00,  7.95s/it]\u001b[A\n",
      "Epoch:  40%|████      | 2/5 [39:59<1:00:21, 1207.17s/it]\n",
      "Iteration:   0%|          | 0/156 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/156 [00:06<17:51,  6.91s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 2/156 [00:13<17:54,  6.98s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 3/156 [00:20<17:52,  7.01s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/156 [00:28<17:49,  7.04s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 5/156 [00:35<17:39,  7.02s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 6/156 [00:42<17:31,  7.01s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 7/156 [00:48<17:21,  6.99s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 8/156 [00:55<17:11,  6.97s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 9/156 [01:02<17:00,  6.94s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 10/156 [01:09<17:00,  6.99s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 11/156 [01:16<16:54,  7.00s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 12/156 [01:23<16:46,  6.99s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 13/156 [01:30<16:37,  6.98s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 14/156 [01:37<16:29,  6.97s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 15/156 [01:44<16:26,  6.99s/it]\u001b[A\n",
      "Iteration:  10%|█         | 16/156 [01:52<16:28,  7.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 17/156 [01:58<16:16,  7.03s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/156 [02:05<16:03,  6.98s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/156 [02:12<15:56,  6.98s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/156 [02:19<15:54,  7.02s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 21/156 [02:27<15:51,  7.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/156 [02:34<15:50,  7.09s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/156 [02:41<15:37,  7.05s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 24/156 [02:48<15:28,  7.03s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/156 [02:55<15:17,  7.00s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/156 [03:02<15:09,  6.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 27/156 [03:09<15:00,  6.98s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/156 [03:16<14:54,  6.99s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 29/156 [03:23<14:50,  7.01s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/156 [03:30<14:43,  7.01s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 31/156 [03:37<14:34,  6.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 32/156 [03:44<14:25,  6.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 33/156 [03:51<14:17,  6.97s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/156 [03:57<14:09,  6.97s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 35/156 [04:04<14:01,  6.96s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/156 [04:12<14:04,  7.04s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 37/156 [04:19<13:54,  7.01s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 38/156 [04:26<13:48,  7.02s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/156 [04:33<13:40,  7.01s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/156 [04:40<13:31,  6.99s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 41/156 [04:47<13:25,  7.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/156 [04:53<13:14,  6.97s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/156 [05:00<13:06,  6.96s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 44/156 [05:07<12:58,  6.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/156 [05:15<13:00,  7.03s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 46/156 [05:22<12:56,  7.05s/it]\u001b[A\n",
      "Iteration:  30%|███       | 47/156 [05:29<12:45,  7.02s/it]\u001b[A\n",
      "Iteration:  31%|███       | 48/156 [05:36<12:40,  7.04s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 49/156 [05:43<12:30,  7.01s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/156 [05:50<12:21,  6.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/156 [05:57<12:12,  6.97s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 52/156 [06:03<12:05,  6.97s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/156 [06:10<11:58,  6.97s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 54/156 [06:17<11:51,  6.97s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 55/156 [06:24<11:46,  7.00s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 56/156 [06:32<11:47,  7.07s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/156 [06:39<11:40,  7.07s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 58/156 [06:46<11:29,  7.04s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/156 [06:53<11:19,  7.01s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 60/156 [07:00<11:11,  7.00s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 61/156 [07:07<11:02,  6.97s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 62/156 [07:14<10:56,  6.98s/it]\u001b[A\n",
      "Iteration:  40%|████      | 63/156 [07:21<10:51,  7.00s/it]\u001b[A\n",
      "Iteration:  41%|████      | 64/156 [07:28<10:41,  6.98s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/156 [07:35<10:36,  7.00s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 66/156 [07:42<10:29,  6.99s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 67/156 [07:49<10:21,  6.99s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 68/156 [07:56<10:18,  7.03s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 69/156 [08:03<10:08,  6.99s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 70/156 [08:10<10:01,  6.99s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/156 [08:17<09:54,  6.99s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 72/156 [08:24<09:48,  7.01s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/156 [08:31<09:39,  6.98s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 74/156 [08:37<09:30,  6.96s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 75/156 [08:44<09:24,  6.97s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 76/156 [08:51<09:17,  6.97s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 77/156 [08:58<09:10,  6.97s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 78/156 [09:05<09:02,  6.96s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 79/156 [09:12<08:56,  6.97s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 80/156 [09:19<08:49,  6.97s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 81/156 [09:26<08:43,  6.98s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/156 [09:33<08:38,  7.01s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 83/156 [09:40<08:31,  7.01s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 84/156 [09:47<08:23,  6.99s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 85/156 [09:54<08:15,  6.98s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 86/156 [10:01<08:08,  6.98s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 87/156 [10:08<08:00,  6.96s/it]\u001b[A01/24/2025 18:01:38 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_dev_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 18:01:38 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/24/2025 18:01:38 - INFO - __main__ -     Num examples = 1241\n",
      "01/24/2025 18:01:38 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   3%|▎         | 1/39 [00:02<01:16,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   5%|▌         | 2/39 [00:04<01:14,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 3/39 [00:06<01:14,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 4/39 [00:08<01:11,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  13%|█▎        | 5/39 [00:10<01:08,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  15%|█▌        | 6/39 [00:12<01:06,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 7/39 [00:14<01:04,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  21%|██        | 8/39 [00:16<01:02,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  23%|██▎       | 9/39 [00:18<01:00,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 10/39 [00:20<00:58,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 11/39 [00:22<00:56,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  31%|███       | 12/39 [00:24<00:54,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|███▎      | 13/39 [00:26<00:52,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 14/39 [00:28<00:50,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 15/39 [00:30<00:48,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  41%|████      | 16/39 [00:32<00:46,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▎     | 17/39 [00:34<00:44,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 18/39 [00:36<00:42,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  49%|████▊     | 19/39 [00:38<00:40,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  51%|█████▏    | 20/39 [00:40<00:38,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 21/39 [00:42<00:36,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▋    | 22/39 [00:44<00:34,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  59%|█████▉    | 23/39 [00:46<00:32,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 24/39 [00:48<00:30,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 25/39 [00:50<00:28,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|██████▋   | 26/39 [00:52<00:26,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  69%|██████▉   | 27/39 [00:54<00:24,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 28/39 [00:56<00:22,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 29/39 [00:58<00:20,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  77%|███████▋  | 30/39 [01:00<00:18,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  79%|███████▉  | 31/39 [01:02<00:16,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 32/39 [01:04<00:14,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  85%|████████▍ | 33/39 [01:06<00:12,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  87%|████████▋ | 34/39 [01:08<00:10,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|████████▉ | 35/39 [01:10<00:08,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 36/39 [01:12<00:06,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  95%|█████████▍| 37/39 [01:14<00:04,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  97%|█████████▋| 38/39 [01:16<00:02,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 39/39 [01:18<00:00,  2.00s/it]\u001b[A\u001b[A\n",
      "01/24/2025 18:02:56 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/24/2025 18:02:56 - INFO - __main__ -     acc = 0.9121676067687349\n",
      "01/24/2025 18:02:56 - INFO - __main__ -     auc = 0.9698266677061407\n",
      "01/24/2025 18:02:56 - INFO - __main__ -     f1 = 0.911887262804842\n",
      "01/24/2025 18:02:56 - INFO - __main__ -     mcc = 0.8257795050944344\n",
      "01/24/2025 18:02:56 - INFO - __main__ -     precision = 0.9144381316437137\n",
      "01/24/2025 18:02:56 - INFO - __main__ -     recall = 0.911347158337881\n",
      "/home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  56%|█████▋    | 88/156 [11:33<34:30, 30.44s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 89/156 [11:40<26:07, 23.39s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/156 [11:47<20:19, 18.47s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 91/156 [11:54<16:17, 15.04s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 92/156 [12:01<13:27, 12.62s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 93/156 [12:08<11:27, 10.91s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 94/156 [12:15<10:04,  9.75s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 95/156 [12:22<09:05,  8.94s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/156 [12:29<08:21,  8.35s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 97/156 [12:36<07:51,  8.00s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 98/156 [12:44<07:26,  7.70s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 99/156 [12:50<07:05,  7.47s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 100/156 [12:57<06:49,  7.31s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 101/156 [13:04<06:36,  7.21s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 102/156 [13:11<06:25,  7.14s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 103/156 [13:18<06:15,  7.09s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 104/156 [13:25<06:07,  7.07s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 105/156 [13:32<05:58,  7.04s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 106/156 [13:39<05:50,  7.02s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 107/156 [13:46<05:42,  7.00s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 108/156 [13:53<05:35,  6.99s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 109/156 [14:00<05:27,  6.97s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 110/156 [14:07<05:20,  6.96s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 111/156 [14:14<05:14,  6.99s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 112/156 [14:21<05:08,  7.01s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 113/156 [14:28<04:59,  6.97s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 114/156 [14:35<04:52,  6.96s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 115/156 [14:42<04:45,  6.96s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 116/156 [14:49<04:37,  6.94s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 117/156 [14:56<04:31,  6.95s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 118/156 [15:03<04:27,  7.03s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 119/156 [15:10<04:19,  7.03s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 120/156 [15:17<04:12,  7.01s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 121/156 [15:24<04:05,  7.02s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 122/156 [15:31<04:00,  7.07s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 123/156 [15:38<03:53,  7.07s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 124/156 [15:45<03:45,  7.03s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 125/156 [15:52<03:39,  7.08s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 126/156 [16:00<03:32,  7.08s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 127/156 [16:07<03:24,  7.05s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 128/156 [16:13<03:16,  7.02s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 129/156 [16:21<03:09,  7.03s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 130/156 [16:27<03:02,  7.00s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 131/156 [16:34<02:54,  6.97s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 132/156 [16:41<02:47,  6.97s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 133/156 [16:48<02:40,  6.97s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 134/156 [16:55<02:33,  6.96s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 135/156 [17:02<02:25,  6.95s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 136/156 [17:09<02:19,  6.96s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 137/156 [17:16<02:13,  7.02s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 138/156 [17:23<02:07,  7.07s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 139/156 [17:30<01:59,  7.03s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 140/156 [17:37<01:52,  7.03s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 141/156 [17:45<01:45,  7.05s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 142/156 [17:51<01:38,  7.01s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 143/156 [17:58<01:30,  6.98s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 144/156 [18:05<01:23,  6.96s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 145/156 [18:12<01:16,  6.96s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 146/156 [18:19<01:09,  6.98s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 147/156 [18:26<01:02,  7.00s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 148/156 [18:34<00:56,  7.07s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 149/156 [18:41<00:49,  7.05s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 150/156 [18:47<00:42,  7.01s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 151/156 [18:54<00:35,  7.00s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 152/156 [19:01<00:27,  6.99s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 153/156 [19:08<00:20,  6.96s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 154/156 [19:15<00:13,  6.97s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 155/156 [19:22<00:06,  6.98s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 156/156 [19:23<00:00,  7.46s/it]\u001b[A\n",
      "Epoch:  60%|██████    | 3/5 [59:23<39:34, 1187.22s/it]  \n",
      "Iteration:   0%|          | 0/156 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/156 [00:06<18:04,  7.00s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 2/156 [00:14<18:02,  7.03s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 3/156 [00:21<17:55,  7.03s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/156 [00:28<17:42,  6.99s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 5/156 [00:34<17:31,  6.97s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 6/156 [00:41<17:24,  6.96s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 7/156 [00:48<17:16,  6.96s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 8/156 [00:55<17:08,  6.95s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 9/156 [01:02<17:04,  6.97s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 10/156 [01:09<16:55,  6.96s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 11/156 [01:16<16:49,  6.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 12/156 [01:23<16:43,  6.97s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 13/156 [01:30<16:34,  6.96s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 14/156 [01:37<16:27,  6.95s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 15/156 [01:44<16:24,  6.99s/it]\u001b[A\n",
      "Iteration:  10%|█         | 16/156 [01:51<16:16,  6.97s/it]\u001b[A\n",
      "Iteration:  11%|█         | 17/156 [01:58<16:19,  7.04s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/156 [02:05<16:07,  7.01s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/156 [02:12<16:01,  7.02s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/156 [02:19<15:54,  7.02s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 21/156 [02:26<15:45,  7.00s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/156 [02:33<15:34,  6.98s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/156 [02:40<15:25,  6.96s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 24/156 [02:47<15:21,  6.98s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/156 [02:54<15:17,  7.00s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/156 [03:01<15:11,  7.01s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 27/156 [03:08<15:00,  6.98s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/156 [03:15<14:53,  6.98s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 29/156 [03:22<14:47,  6.98s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/156 [03:29<14:38,  6.97s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 31/156 [03:36<14:28,  6.95s/it]\u001b[A01/24/2025 18:14:29 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_dev_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 18:14:29 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/24/2025 18:14:29 - INFO - __main__ -     Num examples = 1241\n",
      "01/24/2025 18:14:29 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   3%|▎         | 1/39 [00:01<01:15,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   5%|▌         | 2/39 [00:03<01:13,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 3/39 [00:05<01:11,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 4/39 [00:07<01:09,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  13%|█▎        | 5/39 [00:09<01:07,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  15%|█▌        | 6/39 [00:11<01:05,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 7/39 [00:14<01:04,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  21%|██        | 8/39 [00:16<01:03,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  23%|██▎       | 9/39 [00:18<01:00,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 10/39 [00:20<00:58,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 11/39 [00:22<00:56,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  31%|███       | 12/39 [00:24<00:53,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|███▎      | 13/39 [00:25<00:51,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 14/39 [00:27<00:49,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 15/39 [00:29<00:47,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  41%|████      | 16/39 [00:31<00:45,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▎     | 17/39 [00:33<00:43,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 18/39 [00:35<00:41,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  49%|████▊     | 19/39 [00:37<00:40,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  51%|█████▏    | 20/39 [00:39<00:37,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 21/39 [00:41<00:35,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▋    | 22/39 [00:43<00:33,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  59%|█████▉    | 23/39 [00:45<00:31,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 24/39 [00:47<00:29,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 25/39 [00:49<00:27,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|██████▋   | 26/39 [00:51<00:25,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  69%|██████▉   | 27/39 [00:53<00:23,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 28/39 [00:55<00:21,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 29/39 [00:57<00:19,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  77%|███████▋  | 30/39 [00:59<00:17,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  79%|███████▉  | 31/39 [01:01<00:15,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 32/39 [01:03<00:13,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  85%|████████▍ | 33/39 [01:05<00:11,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  87%|████████▋ | 34/39 [01:07<00:09,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|████████▉ | 35/39 [01:09<00:07,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 36/39 [01:11<00:05,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  95%|█████████▍| 37/39 [01:13<00:04,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  97%|█████████▋| 38/39 [01:15<00:02,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 39/39 [01:17<00:00,  1.99s/it]\u001b[A\u001b[A\n",
      "01/24/2025 18:15:47 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/24/2025 18:15:47 - INFO - __main__ -     acc = 0.9137792103142627\n",
      "01/24/2025 18:15:47 - INFO - __main__ -     auc = 0.9663132454977781\n",
      "01/24/2025 18:15:47 - INFO - __main__ -     f1 = 0.9134880060355841\n",
      "01/24/2025 18:15:47 - INFO - __main__ -     mcc = 0.8291862804272823\n",
      "01/24/2025 18:15:47 - INFO - __main__ -     precision = 0.9162710825382354\n",
      "01/24/2025 18:15:47 - INFO - __main__ -     recall = 0.9129219614874873\n",
      "/home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  21%|██        | 32/156 [05:00<1:02:23, 30.19s/it]\u001b[A\n",
      "Iteration:  21%|██        | 33/156 [05:07<47:43, 23.28s/it]  \u001b[A\n",
      "Iteration:  22%|██▏       | 34/156 [05:15<37:26, 18.41s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 35/156 [05:22<30:15, 15.00s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/156 [05:29<25:10, 12.58s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 37/156 [05:35<21:36, 10.89s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 38/156 [05:42<19:05,  9.71s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/156 [05:49<17:19,  8.89s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/156 [05:56<16:06,  8.33s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 41/156 [06:04<15:15,  7.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/156 [06:10<14:32,  7.65s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/156 [06:17<14:02,  7.45s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 44/156 [06:24<13:39,  7.31s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/156 [06:31<13:24,  7.24s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 46/156 [06:38<13:06,  7.15s/it]\u001b[A\n",
      "Iteration:  30%|███       | 47/156 [06:45<12:52,  7.09s/it]\u001b[A\n",
      "Iteration:  31%|███       | 48/156 [06:52<12:43,  7.07s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 49/156 [06:59<12:34,  7.05s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/156 [07:06<12:23,  7.01s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/156 [07:13<12:13,  6.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 52/156 [07:20<12:07,  6.99s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/156 [07:27<12:01,  7.00s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 54/156 [07:34<11:51,  6.97s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 55/156 [07:41<11:43,  6.97s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 56/156 [07:48<11:40,  7.00s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/156 [07:55<11:32,  6.99s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 58/156 [08:02<11:29,  7.03s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/156 [08:09<11:20,  7.02s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 60/156 [08:17<11:21,  7.10s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 61/156 [08:24<11:10,  7.06s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 62/156 [08:31<10:59,  7.02s/it]\u001b[A\n",
      "Iteration:  40%|████      | 63/156 [08:37<10:51,  7.00s/it]\u001b[A\n",
      "Iteration:  41%|████      | 64/156 [08:44<10:43,  6.99s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/156 [08:51<10:37,  7.00s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 66/156 [08:59<10:35,  7.06s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 67/156 [09:06<10:25,  7.03s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 68/156 [09:13<10:15,  6.99s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 69/156 [09:20<10:08,  7.00s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 70/156 [09:27<10:03,  7.01s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/156 [09:33<09:53,  6.98s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 72/156 [09:40<09:46,  6.98s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/156 [09:48<09:45,  7.06s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 74/156 [09:55<09:38,  7.06s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 75/156 [10:02<09:31,  7.06s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 76/156 [10:09<09:21,  7.02s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 77/156 [10:16<09:13,  7.01s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 78/156 [10:23<09:09,  7.04s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 79/156 [10:30<08:59,  7.01s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 80/156 [10:37<08:51,  6.99s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 81/156 [10:44<08:42,  6.97s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/156 [10:51<08:37,  6.99s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 83/156 [10:58<08:32,  7.02s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 84/156 [11:05<08:24,  7.00s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 85/156 [11:12<08:16,  6.99s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 86/156 [11:19<08:09,  7.00s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 87/156 [11:26<08:01,  6.98s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 88/156 [11:33<07:52,  6.96s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 89/156 [11:40<07:46,  6.96s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/156 [11:47<07:40,  6.97s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 91/156 [11:53<07:32,  6.97s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 92/156 [12:01<07:27,  6.99s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 93/156 [12:08<07:20,  6.99s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 94/156 [12:14<07:12,  6.97s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 95/156 [12:21<07:05,  6.98s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/156 [12:28<06:58,  6.98s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 97/156 [12:35<06:50,  6.96s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 98/156 [12:42<06:43,  6.96s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 99/156 [12:49<06:38,  6.99s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 100/156 [12:56<06:30,  6.98s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 101/156 [13:03<06:24,  7.00s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 102/156 [13:10<06:17,  7.00s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 103/156 [13:17<06:09,  6.98s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 104/156 [13:24<06:05,  7.02s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 105/156 [13:31<05:57,  7.01s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 106/156 [13:38<05:49,  6.99s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 107/156 [13:45<05:41,  6.97s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 108/156 [13:52<05:35,  6.98s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 109/156 [13:59<05:31,  7.05s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 110/156 [14:06<05:22,  7.02s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 111/156 [14:13<05:16,  7.03s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 112/156 [14:20<05:08,  7.02s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 113/156 [14:27<05:00,  6.99s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 114/156 [14:34<04:54,  7.02s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 115/156 [14:41<04:47,  7.00s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 116/156 [14:48<04:40,  7.02s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 117/156 [14:55<04:33,  7.01s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 118/156 [15:03<04:27,  7.04s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 119/156 [15:10<04:20,  7.03s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 120/156 [15:16<04:11,  7.00s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 121/156 [15:23<04:04,  6.98s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 122/156 [15:30<03:57,  6.97s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 123/156 [15:37<03:49,  6.95s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 124/156 [15:44<03:43,  6.97s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 125/156 [15:51<03:36,  6.99s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 126/156 [15:58<03:30,  7.00s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 127/156 [16:05<03:22,  6.97s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 128/156 [16:12<03:14,  6.96s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 129/156 [16:19<03:07,  6.96s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 130/156 [16:26<03:00,  6.95s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 131/156 [16:33<02:53,  6.94s/it]\u001b[A01/24/2025 18:27:26 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_dev_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 18:27:26 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/24/2025 18:27:26 - INFO - __main__ -     Num examples = 1241\n",
      "01/24/2025 18:27:26 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   3%|▎         | 1/39 [00:01<01:14,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   5%|▌         | 2/39 [00:03<01:12,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 3/39 [00:06<01:15,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 4/39 [00:08<01:13,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  13%|█▎        | 5/39 [00:10<01:09,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  15%|█▌        | 6/39 [00:12<01:06,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 7/39 [00:14<01:04,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  21%|██        | 8/39 [00:16<01:01,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  23%|██▎       | 9/39 [00:18<01:00,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 10/39 [00:20<00:57,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 11/39 [00:22<00:55,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  31%|███       | 12/39 [00:24<00:53,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|███▎      | 13/39 [00:26<00:51,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 14/39 [00:28<00:49,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 15/39 [00:30<00:47,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  41%|████      | 16/39 [00:32<00:45,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▎     | 17/39 [00:34<00:43,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 18/39 [00:36<00:41,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  49%|████▊     | 19/39 [00:38<00:39,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  51%|█████▏    | 20/39 [00:40<00:37,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 21/39 [00:42<00:36,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▋    | 22/39 [00:44<00:33,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  59%|█████▉    | 23/39 [00:45<00:31,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 24/39 [00:47<00:29,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 25/39 [00:49<00:27,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|██████▋   | 26/39 [00:51<00:25,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  69%|██████▉   | 27/39 [00:53<00:23,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 28/39 [00:55<00:21,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 29/39 [00:57<00:19,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  77%|███████▋  | 30/39 [00:59<00:17,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  79%|███████▉  | 31/39 [01:01<00:15,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 32/39 [01:03<00:13,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  85%|████████▍ | 33/39 [01:05<00:11,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  87%|████████▋ | 34/39 [01:07<00:09,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|████████▉ | 35/39 [01:09<00:07,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 36/39 [01:11<00:05,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  95%|█████████▍| 37/39 [01:13<00:03,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  97%|█████████▋| 38/39 [01:15<00:02,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 39/39 [01:17<00:00,  1.99s/it]\u001b[A\u001b[A\n",
      "01/24/2025 18:28:44 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/24/2025 18:28:44 - INFO - __main__ -     acc = 0.9234488315874295\n",
      "01/24/2025 18:28:44 - INFO - __main__ -     auc = 0.969267950417089\n",
      "01/24/2025 18:28:44 - INFO - __main__ -     f1 = 0.9234201902559589\n",
      "01/24/2025 18:28:44 - INFO - __main__ -     mcc = 0.8468703821049481\n",
      "01/24/2025 18:28:44 - INFO - __main__ -     precision = 0.9233691840074819\n",
      "01/24/2025 18:28:44 - INFO - __main__ -     recall = 0.9235012083885554\n",
      "/home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  85%|████████▍ | 132/156 [17:57<12:04, 30.19s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 133/156 [18:04<08:53, 23.21s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 134/156 [18:11<06:43, 18.34s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 135/156 [18:18<05:13, 14.94s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 136/156 [18:25<04:10, 12.55s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 137/156 [18:32<03:26, 10.87s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 138/156 [18:39<02:54,  9.68s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 139/156 [18:46<02:30,  8.88s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 140/156 [18:53<02:12,  8.31s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 141/156 [19:00<01:58,  7.92s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 142/156 [19:07<01:46,  7.61s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 143/156 [19:14<01:36,  7.41s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 144/156 [19:21<01:27,  7.29s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 145/156 [19:28<01:19,  7.19s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 146/156 [19:35<01:11,  7.12s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 147/156 [19:42<01:04,  7.12s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 148/156 [19:49<00:56,  7.07s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 149/156 [19:56<00:49,  7.02s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 150/156 [20:03<00:42,  7.05s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 151/156 [20:10<00:35,  7.04s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 152/156 [20:17<00:28,  7.04s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 153/156 [20:24<00:21,  7.02s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 154/156 [20:31<00:14,  7.00s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 155/156 [20:38<00:06,  6.99s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 156/156 [20:39<00:00,  7.94s/it]\u001b[A\n",
      "Epoch:  80%|████████  | 4/5 [1:20:02<20:07, 1207.72s/it]\n",
      "Iteration:   0%|          | 0/156 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/156 [00:07<18:13,  7.05s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 2/156 [00:14<18:16,  7.12s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 3/156 [00:21<18:17,  7.17s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/156 [00:28<17:55,  7.08s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 5/156 [00:35<17:40,  7.03s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 6/156 [00:42<17:32,  7.01s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 7/156 [00:49<17:22,  7.00s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 8/156 [00:56<17:13,  6.98s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 9/156 [01:03<17:05,  6.97s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 10/156 [01:10<17:00,  6.99s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 11/156 [01:17<16:53,  6.99s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 12/156 [01:24<16:44,  6.98s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 13/156 [01:31<16:36,  6.97s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 14/156 [01:38<16:30,  6.97s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 15/156 [01:45<16:24,  6.98s/it]\u001b[A\n",
      "Iteration:  10%|█         | 16/156 [01:52<16:15,  6.97s/it]\u001b[A\n",
      "Iteration:  11%|█         | 17/156 [01:58<16:08,  6.97s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/156 [02:05<16:03,  6.98s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/156 [02:13<15:59,  7.00s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/156 [02:20<15:59,  7.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 21/156 [02:27<15:50,  7.04s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/156 [02:34<15:42,  7.03s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/156 [02:41<15:37,  7.05s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 24/156 [02:48<15:25,  7.01s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/156 [02:55<15:15,  6.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/156 [03:02<15:04,  6.96s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 27/156 [03:09<15:01,  6.98s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/156 [03:16<14:52,  6.98s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 29/156 [03:23<14:51,  7.02s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/156 [03:30<14:43,  7.01s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 31/156 [03:37<14:36,  7.01s/it]\u001b[A\n",
      "Iteration:  21%|██        | 32/156 [03:44<14:28,  7.00s/it]\u001b[A\n",
      "Iteration:  21%|██        | 33/156 [03:51<14:21,  7.00s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/156 [03:58<14:16,  7.02s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 35/156 [04:05<14:04,  6.98s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/156 [04:12<13:58,  6.99s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 37/156 [04:19<13:54,  7.01s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 38/156 [04:26<13:44,  6.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/156 [04:33<13:36,  6.98s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/156 [04:40<13:29,  6.98s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 41/156 [04:47<13:22,  6.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/156 [04:54<13:16,  6.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/156 [05:01<13:11,  7.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 44/156 [05:08<13:05,  7.01s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/156 [05:15<12:56,  7.00s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 46/156 [05:22<12:53,  7.03s/it]\u001b[A\n",
      "Iteration:  30%|███       | 47/156 [05:29<12:44,  7.02s/it]\u001b[A\n",
      "Iteration:  31%|███       | 48/156 [05:36<12:36,  7.00s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 49/156 [05:43<12:30,  7.01s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/156 [05:50<12:21,  7.00s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/156 [05:57<12:13,  6.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 52/156 [06:04<12:05,  6.98s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/156 [06:11<12:09,  7.08s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 54/156 [06:18<12:02,  7.09s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 55/156 [06:25<11:53,  7.06s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 56/156 [06:32<11:43,  7.03s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/156 [06:39<11:43,  7.11s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 58/156 [06:46<11:32,  7.07s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/156 [06:53<11:22,  7.04s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 60/156 [07:00<11:17,  7.05s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 61/156 [07:07<11:10,  7.06s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 62/156 [07:14<11:02,  7.04s/it]\u001b[A\n",
      "Iteration:  40%|████      | 63/156 [07:21<10:56,  7.06s/it]\u001b[A\n",
      "Iteration:  41%|████      | 64/156 [07:29<10:53,  7.10s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/156 [07:36<10:48,  7.12s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 66/156 [07:43<10:37,  7.08s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 67/156 [07:50<10:26,  7.04s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 68/156 [07:57<10:17,  7.02s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 69/156 [08:04<10:10,  7.02s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 70/156 [08:11<10:03,  7.02s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/156 [08:18<09:56,  7.02s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 72/156 [08:25<09:47,  7.00s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/156 [08:32<09:40,  6.99s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 74/156 [08:39<09:37,  7.04s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 75/156 [08:46<09:28,  7.02s/it]\u001b[A01/24/2025 18:40:18 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_dev_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 18:40:18 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/24/2025 18:40:18 - INFO - __main__ -     Num examples = 1241\n",
      "01/24/2025 18:40:18 - INFO - __main__ -     Batch size = 32\n",
      "\n",
      "\n",
      "Evaluating:   0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   3%|▎         | 1/39 [00:01<01:15,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   5%|▌         | 2/39 [00:04<01:14,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:   8%|▊         | 3/39 [00:06<01:12,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  10%|█         | 4/39 [00:07<01:09,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  13%|█▎        | 5/39 [00:09<01:07,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  15%|█▌        | 6/39 [00:11<01:05,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  18%|█▊        | 7/39 [00:13<01:03,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  21%|██        | 8/39 [00:15<01:02,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  23%|██▎       | 9/39 [00:18<01:00,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  26%|██▌       | 10/39 [00:20<00:58,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  28%|██▊       | 11/39 [00:22<00:56,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  31%|███       | 12/39 [00:24<00:54,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  33%|███▎      | 13/39 [00:26<00:53,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  36%|███▌      | 14/39 [00:28<00:50,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  38%|███▊      | 15/39 [00:30<00:48,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  41%|████      | 16/39 [00:32<00:46,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  44%|████▎     | 17/39 [00:34<00:44,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  46%|████▌     | 18/39 [00:36<00:42,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  49%|████▊     | 19/39 [00:38<00:40,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  51%|█████▏    | 20/39 [00:40<00:37,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  54%|█████▍    | 21/39 [00:42<00:36,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  56%|█████▋    | 22/39 [00:44<00:34,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  59%|█████▉    | 23/39 [00:46<00:32,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  62%|██████▏   | 24/39 [00:48<00:30,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  64%|██████▍   | 25/39 [00:50<00:28,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  67%|██████▋   | 26/39 [00:52<00:26,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  69%|██████▉   | 27/39 [00:54<00:24,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  72%|███████▏  | 28/39 [00:56<00:22,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  74%|███████▍  | 29/39 [00:58<00:20,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  77%|███████▋  | 30/39 [01:00<00:18,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  79%|███████▉  | 31/39 [01:02<00:15,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  82%|████████▏ | 32/39 [01:04<00:14,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  85%|████████▍ | 33/39 [01:06<00:12,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  87%|████████▋ | 34/39 [01:08<00:10,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  90%|████████▉ | 35/39 [01:10<00:08,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  92%|█████████▏| 36/39 [01:12<00:06,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  95%|█████████▍| 37/39 [01:14<00:04,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating:  97%|█████████▋| 38/39 [01:16<00:02,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating: 100%|██████████| 39/39 [01:17<00:00,  2.00s/it]\u001b[A\u001b[A\n",
      "01/24/2025 18:41:36 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/24/2025 18:41:36 - INFO - __main__ -     acc = 0.9250604351329573\n",
      "01/24/2025 18:41:36 - INFO - __main__ -     auc = 0.9699462072191471\n",
      "01/24/2025 18:41:36 - INFO - __main__ -     f1 = 0.9249661117774983\n",
      "01/24/2025 18:41:36 - INFO - __main__ -     mcc = 0.8502031845757121\n",
      "01/24/2025 18:41:36 - INFO - __main__ -     precision = 0.9254666146036705\n",
      "01/24/2025 18:41:36 - INFO - __main__ -     recall = 0.9247368831371326\n",
      "/home/koeksalr/miniconda3/envs/dnabert2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration:  49%|████▊     | 76/156 [10:11<40:30, 30.38s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 77/156 [10:18<30:48, 23.40s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 78/156 [10:25<24:01, 18.48s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 79/156 [10:32<19:16, 15.02s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 80/156 [10:39<15:59, 12.63s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 81/156 [10:46<13:40, 10.93s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/156 [10:53<12:00,  9.74s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 83/156 [11:00<10:50,  8.91s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 84/156 [11:07<09:59,  8.32s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 85/156 [11:14<09:21,  7.91s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 86/156 [11:21<08:56,  7.66s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 87/156 [11:28<08:34,  7.46s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 88/156 [11:35<08:18,  7.33s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 89/156 [11:42<08:06,  7.27s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/156 [11:49<07:55,  7.20s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 91/156 [11:56<07:43,  7.13s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 92/156 [12:03<07:33,  7.09s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 93/156 [12:10<07:25,  7.06s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 94/156 [12:17<07:16,  7.05s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 95/156 [12:24<07:10,  7.05s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/156 [12:31<07:02,  7.05s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 97/156 [12:38<06:54,  7.03s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 98/156 [12:45<06:46,  7.00s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 99/156 [12:52<06:38,  7.00s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 100/156 [12:59<06:31,  6.99s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 101/156 [13:06<06:23,  6.97s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 102/156 [13:13<06:17,  6.99s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 103/156 [13:20<06:11,  7.01s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 104/156 [13:27<06:03,  6.99s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 105/156 [13:34<05:56,  6.98s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 106/156 [13:41<05:50,  7.01s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 107/156 [13:48<05:42,  6.98s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 108/156 [13:55<05:34,  6.97s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 109/156 [14:02<05:27,  6.97s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 110/156 [14:09<05:20,  6.97s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 111/156 [14:16<05:13,  6.97s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 112/156 [14:23<05:11,  7.08s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 113/156 [14:30<05:02,  7.04s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 114/156 [14:37<04:56,  7.05s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 115/156 [14:44<04:49,  7.05s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 116/156 [14:51<04:40,  7.02s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 117/156 [14:58<04:32,  6.99s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 118/156 [15:05<04:25,  6.99s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 119/156 [15:12<04:19,  7.00s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 120/156 [15:19<04:12,  7.02s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 121/156 [15:26<04:06,  7.04s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 122/156 [15:33<03:58,  7.01s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 123/156 [15:40<03:51,  7.01s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 124/156 [15:47<03:44,  7.00s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 125/156 [15:54<03:36,  7.00s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 126/156 [16:01<03:29,  6.98s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 127/156 [16:08<03:22,  6.97s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 128/156 [16:15<03:17,  7.05s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 129/156 [16:22<03:10,  7.05s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 130/156 [16:29<03:02,  7.02s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 131/156 [16:36<02:55,  7.02s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 132/156 [16:43<02:49,  7.06s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 133/156 [16:50<02:41,  7.03s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 134/156 [16:57<02:34,  7.01s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 135/156 [17:04<02:26,  7.00s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 136/156 [17:11<02:19,  7.00s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 137/156 [17:18<02:14,  7.08s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 138/156 [17:26<02:07,  7.11s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 139/156 [17:33<02:00,  7.06s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 140/156 [17:40<01:53,  7.07s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 141/156 [17:47<01:47,  7.18s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 142/156 [17:54<01:39,  7.13s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 143/156 [18:01<01:32,  7.08s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 144/156 [18:08<01:25,  7.09s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 145/156 [18:15<01:17,  7.06s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 146/156 [18:22<01:10,  7.05s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 147/156 [18:29<01:03,  7.03s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 148/156 [18:36<00:56,  7.03s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 149/156 [18:43<00:49,  7.04s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 150/156 [18:50<00:42,  7.01s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 151/156 [18:57<00:34,  6.98s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 152/156 [19:04<00:27,  6.98s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 153/156 [19:11<00:20,  6.98s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 154/156 [19:18<00:14,  7.04s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 155/156 [19:25<00:07,  7.01s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 156/156 [19:26<00:00,  7.48s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 5/5 [1:39:28<00:00, 1193.78s/it]\n",
      "01/24/2025 18:50:51 - INFO - __main__ -    global_step = 780, average loss = 0.18734751643414477\n",
      "01/24/2025 18:50:51 - INFO - __main__ -   Saving model checkpoint to /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output\n",
      "01/24/2025 18:50:51 - INFO - transformers.configuration_utils -   Configuration saved in /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/config.json\n",
      "01/24/2025 18:50:52 - INFO - transformers.modeling_utils -   Model weights saved in /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/pytorch_model.bin\n",
      "01/24/2025 18:50:52 - INFO - transformers.configuration_utils -   loading configuration file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/config.json\n",
      "01/24/2025 18:50:52 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 69\n",
      "}\n",
      "\n",
      "01/24/2025 18:50:52 - INFO - transformers.modeling_utils -   loading weights file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/pytorch_model.bin\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   Model name '/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming '/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   Didn't find file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/added_tokens.json. We won't load it.\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   loading file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/vocab.txt\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   loading file None\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   loading file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/special_tokens_map.json\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   loading file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/tokenizer_config.json\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   Model name '/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming '/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   Didn't find file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/added_tokens.json. We won't load it.\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   loading file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/vocab.txt\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   loading file None\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   loading file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/special_tokens_map.json\n",
      "01/24/2025 18:50:53 - INFO - transformers.tokenization_utils -   loading file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/tokenizer_config.json\n",
      "01/24/2025 18:50:53 - INFO - __main__ -   Evaluate the following checkpoints: ['/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output']\n",
      "01/24/2025 18:50:53 - INFO - transformers.configuration_utils -   loading configuration file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/config.json\n",
      "01/24/2025 18:50:53 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": \"dnaprom\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 69\n",
      "}\n",
      "\n",
      "01/24/2025 18:50:53 - INFO - transformers.modeling_utils -   loading weights file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output/pytorch_model.bin\n",
      "01/24/2025 18:50:54 - INFO - __main__ -   Loading features from cached file /home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data/cached_dev_3-new-12w-0_100_dnaprom\n",
      "01/24/2025 18:50:54 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "01/24/2025 18:50:54 - INFO - __main__ -     Num examples = 1241\n",
      "01/24/2025 18:50:54 - INFO - __main__ -     Batch size = 32\n",
      "Evaluating: 100%|██████████| 39/39 [01:17<00:00,  1.99s/it]\n",
      "01/24/2025 18:52:12 - INFO - __main__ -   ***** Eval results  *****\n",
      "01/24/2025 18:52:12 - INFO - __main__ -     acc = 0.9258662369057212\n",
      "01/24/2025 18:52:12 - INFO - __main__ -     auc = 0.9692029832904551\n",
      "01/24/2025 18:52:12 - INFO - __main__ -     f1 = 0.9257852314092564\n",
      "01/24/2025 18:52:12 - INFO - __main__ -     mcc = 0.8517439672365056\n",
      "01/24/2025 18:52:12 - INFO - __main__ -     precision = 0.9261444949234748\n",
      "01/24/2025 18:52:12 - INFO - __main__ -     recall = 0.9255996465788311\n"
     ]
    }
   ],
   "source": [
    "# %%bash \n",
    "# cd DNABERT/examples\n",
    "\n",
    "# export KMER=3\n",
    "# export MODEL_PATH='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/dnabert3/3-new-12w-0'\n",
    "# export DATA_PATH='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/data'\n",
    "# export OUTPUT_PATH='/home/koeksalr/Downloads/ml_in_life_sciences/ML_LS_resources/exercise_11_rna_protein_transformer_hands_on/output'\n",
    "\n",
    "# python run_finetune.py \\\n",
    "#     --model_type dna \\\n",
    "#     --tokenizer_name=dna$KMER \\\n",
    "#     --model_name_or_path $MODEL_PATH \\\n",
    "#     --task_name dnaprom \\\n",
    "#     --do_train \\\n",
    "#     --do_eval \\\n",
    "#     --data_dir $DATA_PATH \\\n",
    "#     --max_seq_length 100 \\\n",
    "#     --per_gpu_eval_batch_size=32   \\\n",
    "#     --per_gpu_train_batch_size=32   \\\n",
    "#     --learning_rate 2e-4 \\\n",
    "#     --num_train_epochs 5.0 \\\n",
    "#     --output_dir $OUTPUT_PATH \\\n",
    "#     --evaluate_during_training \\\n",
    "#     --logging_steps 100 \\\n",
    "#     --save_steps 4000 \\\n",
    "#     --warmup_percent 0.1 \\\n",
    "#     --hidden_dropout_prob 0.1 \\\n",
    "#     --overwrite_output \\\n",
    "#     --weight_decay 0.01 \\\n",
    "#     --n_process 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement and Train RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement and set up local RNN (LSTM) to compare against DNABERT. We will implement an LSTM-based sequence classifier for this with the following architecture:\n",
    "\n",
    "1) Embedding\n",
    "2) LSTM\n",
    "3) FC hidden\n",
    "4) ReLU\n",
    "5) Linear \n",
    "6) Sigmoid\n",
    "\n",
    "RNNSeqClassifier(\n",
    "  (embedding): Embedding(4097, 128)\n",
    "  (lstm): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
    "  (fc_hidden): Linear(in_features=200, out_features=64, bias=True)\n",
    "  (fc_relu): ReLU()\n",
    "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
    "  (sigmoid): Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_path = \"./data/train.tsv\"\n",
    "test_path = \"./data/test.tsv\"\n",
    "vocab = \"AGCT\"\n",
    "batch_size = 32\n",
    "hidden_dim = 100\n",
    "fc_hidden_dim = 64\n",
    "n_layers = 2\n",
    "learning_rate = 2e-4\n",
    "n_epochs = 50\n",
    "dropout = 0.1\n",
    "embedding_dim = 128\n",
    "bidirectional = True\n",
    "vocab_size = 4096 + 1\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "class RNNSeqClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(RNNSeqClassifier, self).__init__()\n",
    "        # embedding layer: vector representation of each kmer index\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM: recurrent layer that learns connections\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers = n_layers,\n",
    "            bidirectional = bidirectional,\n",
    "            dropout = dropout,\n",
    "            batch_first = True\n",
    "        )\n",
    "        # dense layer for assigning class to each sequence of kmers\n",
    "        self.fc_hidden = torch.nn.Linear(hidden_dim * 2, fc_hidden_dim)\n",
    "        self.fc_relu = torch.nn.ReLU()\n",
    "        self.fc_out = torch.nn.Linear(fc_hidden_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        _, (hidden_state, cell_state) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        fc_hidden = self.fc_hidden(hidden)\n",
    "        fc_hidden = self.fc_relu(fc_hidden)\n",
    "        dense_outputs = self.fc_out(fc_hidden)\n",
    "        outputs = self.sigmoid(dense_outputs)\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for the RNN\n",
    "def get_all_possible_words(vocab, kmer_size=3):\n",
    "    '''\n",
    "    Get an exhaustive list of combinations of \"AGCT\" as kmers of size defined by kmer_size\n",
    "    '''\n",
    "    all_com = [''.join(c) for c in product(vocab, repeat=kmer_size)]\n",
    "    kmer_f_dict = {i + 1: all_com[i] for i in range(0, len(all_com))}\n",
    "    kmer_r_dict = {all_com[i]: i + 1  for i in range(0, len(all_com))}\n",
    "    return kmer_f_dict, kmer_r_dict\n",
    "\n",
    "\n",
    "def convert_seq_2_integers(dataframe, r_dict):\n",
    "    seq_mat = list()\n",
    "    for index, item in dataframe.iterrows():\n",
    "        kmers = item[\"sequence\"].split(\" \")\n",
    "        kmers_integers = [r_dict[k] for k in kmers]\n",
    "        seq_mat.append(kmers_integers)\n",
    "    labels = dataframe[\"label\"].tolist()\n",
    "    return seq_mat, labels\n",
    "\n",
    "def preprocess_data():\n",
    "    '''\n",
    "    Read raw data files and create Pytorch dataset\n",
    "    '''\n",
    "    f_dict, r_dict = get_all_possible_words(vocab, 3)\n",
    "    train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "    train_mat, train_labels = convert_seq_2_integers(train_df, r_dict)\n",
    "    test_df = pd.read_csv(test_path, sep=\"\\t\")\n",
    "    test_mat, test_labels = convert_seq_2_integers(test_df, r_dict)\n",
    "    return train_mat, train_labels, test_mat, test_labels\n",
    "\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "\t'''\n",
    "\tHandles batches of dataset\n",
    "\t'''\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Evaluation \n",
    "def evaluate_model(model, loader_test):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    # no gradient update while in test mode\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader_test:\n",
    "            x = x_batch.type(torch.LongTensor)\n",
    "            y = y_batch.type(torch.FloatTensor)\n",
    "            y_pred = model(x)\n",
    "            predictions += list(y_pred.detach().numpy())\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def calculate_accuray(ground_truth, predictions):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    for true, pred in zip(ground_truth, predictions):\n",
    "        if (pred > 0.5) and (true == 1):\n",
    "            true_positives += 1\n",
    "        elif (pred < 0.5) and (true == 0):\n",
    "            true_negatives += 1\n",
    "        else:\n",
    "            pass\n",
    "    auc_score = np.round(roc_auc_score(ground_truth, predictions), 4)\n",
    "    accuracy = np.round((true_positives + true_negatives) / len(ground_truth), 4)\n",
    "    return auc_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN train\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    Train model using train sequences and their corresponding labels.\n",
    "    Evaluate trained model using test sequences and their labels\n",
    "    '''\n",
    "    model = RNNSeqClassifier()\n",
    "    print(\"RNN classifier architecture: \\n\")\n",
    "    print(model)\n",
    "    print()\n",
    "    X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    training_set = DatasetMaper(X_train, y_train)\n",
    "    test_set = DatasetMaper(X_test, y_test)\n",
    "\t\t\n",
    "    loader_training = DataLoader(training_set, batch_size=batch_size)\n",
    "    loader_test = DataLoader(test_set)\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    loss_epo = list()\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_bat = list()\n",
    "        predictions = []\n",
    "        # training mode\n",
    "        model.train()\n",
    "        for x_batch, y_batch in loader_training:\n",
    "            x = x_batch.type(torch.LongTensor)\n",
    "            y = y_batch.type(torch.FloatTensor)\n",
    "            y_pred = model(x)\n",
    "            y_pred = torch.reshape(y_pred, (y_pred.shape[0], ))\n",
    "            loss = F.binary_cross_entropy(y_pred, y)\n",
    "            loss_bat.append(loss.detach().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        loss_epo.append(np.mean(loss_bat))\n",
    "        # evaluate model after each epoch of training on test data\n",
    "        pred = evaluate_model(model, loader_test)\n",
    "        auc_score, acc = calculate_accuray(y_test, pred)\n",
    "        mean_batch_loss = np.mean(np.round(loss_bat, 2))\n",
    "        # output training and evaluation results\n",
    "        print(\"Training: Loss at epoch {} : {}\".format(epoch+1, np.round(mean_batch_loss, 4)))\n",
    "        print(\"Test: Accuracy at epoch {} : {}\".format(epoch+1, acc))\n",
    "        print(\"Test: ROC AUC score at epoch {} : {}\".format(epoch+1, auc_score))\n",
    "        print()\n",
    "    print(\"Training: Loss after {} epochs: {}\".format(epoch+1, np.mean(loss_epo)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains and evaluates the RNN using 5-fold cross-validation. The following cell is an alternative to train and evaluate on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FOLD 1 / 5 ===\n",
      "RNN classifier architecture: \n",
      "\n",
      "RNNSeqClassifier(\n",
      "  (embedding): Embedding(4097, 128)\n",
      "  (lstm): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc_hidden): Linear(in_features=200, out_features=64, bias=True)\n",
      "  (fc_relu): ReLU()\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Training: Loss at epoch 1 : 0.5496000051498413\n",
      "Test: Accuracy at epoch 1 : 0.8364\n",
      "Test: ROC AUC score at epoch 1 : 0.8703\n",
      "\n",
      "Training: Loss at epoch 2 : 0.5911999940872192\n",
      "Test: Accuracy at epoch 2 : 0.6406\n",
      "Test: ROC AUC score at epoch 2 : 0.7235\n",
      "\n",
      "Training: Loss at epoch 3 : 0.609000027179718\n",
      "Test: Accuracy at epoch 3 : 0.6841\n",
      "Test: ROC AUC score at epoch 3 : 0.7543\n",
      "\n",
      "Training: Loss at epoch 4 : 0.527899980545044\n",
      "Test: Accuracy at epoch 4 : 0.805\n",
      "Test: ROC AUC score at epoch 4 : 0.8447\n",
      "\n",
      "Training: Loss at epoch 5 : 0.4422999918460846\n",
      "Test: Accuracy at epoch 5 : 0.8396\n",
      "Test: ROC AUC score at epoch 5 : 0.888\n",
      "\n",
      "Training: Loss at epoch 6 : 0.4560999870300293\n",
      "Test: Accuracy at epoch 6 : 0.7905\n",
      "Test: ROC AUC score at epoch 6 : 0.8688\n",
      "\n",
      "Training: Loss at epoch 7 : 0.40470001101493835\n",
      "Test: Accuracy at epoch 7 : 0.8517\n",
      "Test: ROC AUC score at epoch 7 : 0.8929\n",
      "\n",
      "Training: Loss at epoch 8 : 0.3736000061035156\n",
      "Test: Accuracy at epoch 8 : 0.8832\n",
      "Test: ROC AUC score at epoch 8 : 0.9184\n",
      "\n",
      "Training: Loss at epoch 9 : 0.3499999940395355\n",
      "Test: Accuracy at epoch 9 : 0.8888\n",
      "Test: ROC AUC score at epoch 9 : 0.9331\n",
      "\n",
      "Training: Loss at epoch 10 : 0.32499998807907104\n",
      "Test: Accuracy at epoch 10 : 0.8815\n",
      "Test: ROC AUC score at epoch 10 : 0.9278\n",
      "\n",
      "Training: Loss at epoch 11 : 0.3140999972820282\n",
      "Test: Accuracy at epoch 11 : 0.8832\n",
      "Test: ROC AUC score at epoch 11 : 0.9292\n",
      "\n",
      "Training: Loss at epoch 12 : 0.30090001225471497\n",
      "Test: Accuracy at epoch 12 : 0.8936\n",
      "Test: ROC AUC score at epoch 12 : 0.9417\n",
      "\n",
      "Training: Loss at epoch 13 : 0.2833000123500824\n",
      "Test: Accuracy at epoch 13 : 0.8848\n",
      "Test: ROC AUC score at epoch 13 : 0.9391\n",
      "\n",
      "Training: Loss at epoch 14 : 0.27489998936653137\n",
      "Test: Accuracy at epoch 14 : 0.8977\n",
      "Test: ROC AUC score at epoch 14 : 0.9446\n",
      "\n",
      "Training: Loss at epoch 15 : 0.26489999890327454\n",
      "Test: Accuracy at epoch 15 : 0.8912\n",
      "Test: ROC AUC score at epoch 15 : 0.9431\n",
      "\n",
      "Training: Loss at epoch 16 : 0.2581000030040741\n",
      "Test: Accuracy at epoch 16 : 0.8832\n",
      "Test: ROC AUC score at epoch 16 : 0.9403\n",
      "\n",
      "Training: Loss at epoch 17 : 0.24699999392032623\n",
      "Test: Accuracy at epoch 17 : 0.8888\n",
      "Test: ROC AUC score at epoch 17 : 0.9403\n",
      "\n",
      "Training: Loss at epoch 18 : 0.24420000612735748\n",
      "Test: Accuracy at epoch 18 : 0.8912\n",
      "Test: ROC AUC score at epoch 18 : 0.9424\n",
      "\n",
      "Training: Loss at epoch 19 : 0.2296999990940094\n",
      "Test: Accuracy at epoch 19 : 0.8896\n",
      "Test: ROC AUC score at epoch 19 : 0.9414\n",
      "\n",
      "Training: Loss at epoch 20 : 0.22020000219345093\n",
      "Test: Accuracy at epoch 20 : 0.8888\n",
      "Test: ROC AUC score at epoch 20 : 0.9397\n",
      "\n",
      "Training: Loss at epoch 21 : 0.21119999885559082\n",
      "Test: Accuracy at epoch 21 : 0.8904\n",
      "Test: ROC AUC score at epoch 21 : 0.9393\n",
      "\n",
      "Training: Loss at epoch 22 : 0.2045000046491623\n",
      "Test: Accuracy at epoch 22 : 0.8872\n",
      "Test: ROC AUC score at epoch 22 : 0.9424\n",
      "\n",
      "Training: Loss at epoch 23 : 0.19110000133514404\n",
      "Test: Accuracy at epoch 23 : 0.888\n",
      "Test: ROC AUC score at epoch 23 : 0.9424\n",
      "\n",
      "Training: Loss at epoch 24 : 0.17679999768733978\n",
      "Test: Accuracy at epoch 24 : 0.8743\n",
      "Test: ROC AUC score at epoch 24 : 0.9353\n",
      "\n",
      "Training: Loss at epoch 25 : 0.16619999706745148\n",
      "Test: Accuracy at epoch 25 : 0.8856\n",
      "Test: ROC AUC score at epoch 25 : 0.9385\n",
      "\n",
      "Training: Loss at epoch 26 : 0.1632000058889389\n",
      "Test: Accuracy at epoch 26 : 0.8727\n",
      "Test: ROC AUC score at epoch 26 : 0.9373\n",
      "\n",
      "Training: Loss at epoch 27 : 0.147599995136261\n",
      "Test: Accuracy at epoch 27 : 0.8824\n",
      "Test: ROC AUC score at epoch 27 : 0.9373\n",
      "\n",
      "Training: Loss at epoch 28 : 0.1436000019311905\n",
      "Test: Accuracy at epoch 28 : 0.8864\n",
      "Test: ROC AUC score at epoch 28 : 0.9339\n",
      "\n",
      "Training: Loss at epoch 29 : 0.13680000603199005\n",
      "Test: Accuracy at epoch 29 : 0.8775\n",
      "Test: ROC AUC score at epoch 29 : 0.9326\n",
      "\n",
      "Training: Loss at epoch 30 : 0.12129999697208405\n",
      "Test: Accuracy at epoch 30 : 0.8848\n",
      "Test: ROC AUC score at epoch 30 : 0.9322\n",
      "\n",
      "Training: Loss at epoch 31 : 0.11309999972581863\n",
      "Test: Accuracy at epoch 31 : 0.8783\n",
      "Test: ROC AUC score at epoch 31 : 0.9278\n",
      "\n",
      "Training: Loss at epoch 32 : 0.10369999706745148\n",
      "Test: Accuracy at epoch 32 : 0.8864\n",
      "Test: ROC AUC score at epoch 32 : 0.9287\n",
      "\n",
      "Training: Loss at epoch 33 : 0.09600000083446503\n",
      "Test: Accuracy at epoch 33 : 0.884\n",
      "Test: ROC AUC score at epoch 33 : 0.9286\n",
      "\n",
      "Training: Loss at epoch 34 : 0.10100000351667404\n",
      "Test: Accuracy at epoch 34 : 0.8799\n",
      "Test: ROC AUC score at epoch 34 : 0.9265\n",
      "\n",
      "Training: Loss at epoch 35 : 0.07909999787807465\n",
      "Test: Accuracy at epoch 35 : 0.8791\n",
      "Test: ROC AUC score at epoch 35 : 0.9245\n",
      "\n",
      "Training: Loss at epoch 36 : 0.08399999886751175\n",
      "Test: Accuracy at epoch 36 : 0.8856\n",
      "Test: ROC AUC score at epoch 36 : 0.9297\n",
      "\n",
      "Training: Loss at epoch 37 : 0.062300000339746475\n",
      "Test: Accuracy at epoch 37 : 0.8888\n",
      "Test: ROC AUC score at epoch 37 : 0.9269\n",
      "\n",
      "Training: Loss at epoch 38 : 0.07000000029802322\n",
      "Test: Accuracy at epoch 38 : 0.8783\n",
      "Test: ROC AUC score at epoch 38 : 0.924\n",
      "\n",
      "Training: Loss at epoch 39 : 0.055399999022483826\n",
      "Test: Accuracy at epoch 39 : 0.8767\n",
      "Test: ROC AUC score at epoch 39 : 0.9269\n",
      "\n",
      "Training: Loss at epoch 40 : 0.04650000110268593\n",
      "Test: Accuracy at epoch 40 : 0.8735\n",
      "Test: ROC AUC score at epoch 40 : 0.9167\n",
      "\n",
      "Training: Loss at epoch 41 : 0.04830000177025795\n",
      "Test: Accuracy at epoch 41 : 0.8767\n",
      "Test: ROC AUC score at epoch 41 : 0.9248\n",
      "\n",
      "Training: Loss at epoch 42 : 0.04360000044107437\n",
      "Test: Accuracy at epoch 42 : 0.8791\n",
      "Test: ROC AUC score at epoch 42 : 0.9246\n",
      "\n",
      "Training: Loss at epoch 43 : 0.039400000125169754\n",
      "Test: Accuracy at epoch 43 : 0.8678\n",
      "Test: ROC AUC score at epoch 43 : 0.9197\n",
      "\n",
      "Training: Loss at epoch 44 : 0.04580000042915344\n",
      "Test: Accuracy at epoch 44 : 0.8783\n",
      "Test: ROC AUC score at epoch 44 : 0.9277\n",
      "\n",
      "Training: Loss at epoch 45 : 0.03840000182390213\n",
      "Test: Accuracy at epoch 45 : 0.8896\n",
      "Test: ROC AUC score at epoch 45 : 0.9285\n",
      "\n",
      "Training: Loss at epoch 46 : 0.027699999511241913\n",
      "Test: Accuracy at epoch 46 : 0.8904\n",
      "Test: ROC AUC score at epoch 46 : 0.9327\n",
      "\n",
      "Training: Loss at epoch 47 : 0.027300000190734863\n",
      "Test: Accuracy at epoch 47 : 0.8735\n",
      "Test: ROC AUC score at epoch 47 : 0.9261\n",
      "\n",
      "Training: Loss at epoch 48 : 0.03370000049471855\n",
      "Test: Accuracy at epoch 48 : 0.8751\n",
      "Test: ROC AUC score at epoch 48 : 0.9267\n",
      "\n",
      "Training: Loss at epoch 49 : 0.021199999377131462\n",
      "Test: Accuracy at epoch 49 : 0.8832\n",
      "Test: ROC AUC score at epoch 49 : 0.9322\n",
      "\n",
      "Training: Loss at epoch 50 : 0.03689999878406525\n",
      "Test: Accuracy at epoch 50 : 0.8872\n",
      "Test: ROC AUC score at epoch 50 : 0.929\n",
      "\n",
      "Training: Loss after 50 epochs: 0.20211315155029297\n",
      "\n",
      "=== FOLD 2 / 5 ===\n",
      "RNN classifier architecture: \n",
      "\n",
      "RNNSeqClassifier(\n",
      "  (embedding): Embedding(4097, 128)\n",
      "  (lstm): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc_hidden): Linear(in_features=200, out_features=64, bias=True)\n",
      "  (fc_relu): ReLU()\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Training: Loss at epoch 1 : 0.557200014591217\n",
      "Test: Accuracy at epoch 1 : 0.6672\n",
      "Test: ROC AUC score at epoch 1 : 0.8485\n",
      "\n",
      "Training: Loss at epoch 2 : 0.4388999938964844\n",
      "Test: Accuracy at epoch 2 : 0.8243\n",
      "Test: ROC AUC score at epoch 2 : 0.8908\n",
      "\n",
      "Training: Loss at epoch 3 : 0.44620001316070557\n",
      "Test: Accuracy at epoch 3 : 0.834\n",
      "Test: ROC AUC score at epoch 3 : 0.8936\n",
      "\n",
      "Training: Loss at epoch 4 : 0.4106000065803528\n",
      "Test: Accuracy at epoch 4 : 0.8292\n",
      "Test: ROC AUC score at epoch 4 : 0.8724\n",
      "\n",
      "Training: Loss at epoch 5 : 0.40220001339912415\n",
      "Test: Accuracy at epoch 5 : 0.863\n",
      "Test: ROC AUC score at epoch 5 : 0.8918\n",
      "\n",
      "Training: Loss at epoch 6 : 0.3677999973297119\n",
      "Test: Accuracy at epoch 6 : 0.8711\n",
      "Test: ROC AUC score at epoch 6 : 0.9111\n",
      "\n",
      "Training: Loss at epoch 7 : 0.34940001368522644\n",
      "Test: Accuracy at epoch 7 : 0.7921\n",
      "Test: ROC AUC score at epoch 7 : 0.9051\n",
      "\n",
      "Training: Loss at epoch 8 : 0.3815000057220459\n",
      "Test: Accuracy at epoch 8 : 0.8783\n",
      "Test: ROC AUC score at epoch 8 : 0.932\n",
      "\n",
      "Training: Loss at epoch 9 : 0.3336000144481659\n",
      "Test: Accuracy at epoch 9 : 0.8864\n",
      "Test: ROC AUC score at epoch 9 : 0.9345\n",
      "\n",
      "Training: Loss at epoch 10 : 0.3366999924182892\n",
      "Test: Accuracy at epoch 10 : 0.8936\n",
      "Test: ROC AUC score at epoch 10 : 0.9366\n",
      "\n",
      "Training: Loss at epoch 11 : 0.3077000081539154\n",
      "Test: Accuracy at epoch 11 : 0.9001\n",
      "Test: ROC AUC score at epoch 11 : 0.9403\n",
      "\n",
      "Training: Loss at epoch 12 : 0.37130001187324524\n",
      "Test: Accuracy at epoch 12 : 0.8461\n",
      "Test: ROC AUC score at epoch 12 : 0.9034\n",
      "\n",
      "Training: Loss at epoch 13 : 0.3352999985218048\n",
      "Test: Accuracy at epoch 13 : 0.888\n",
      "Test: ROC AUC score at epoch 13 : 0.9286\n",
      "\n",
      "Training: Loss at epoch 14 : 0.30059999227523804\n",
      "Test: Accuracy at epoch 14 : 0.8912\n",
      "Test: ROC AUC score at epoch 14 : 0.9385\n",
      "\n",
      "Training: Loss at epoch 15 : 0.29190000891685486\n",
      "Test: Accuracy at epoch 15 : 0.8928\n",
      "Test: ROC AUC score at epoch 15 : 0.9399\n",
      "\n",
      "Training: Loss at epoch 16 : 0.28290000557899475\n",
      "Test: Accuracy at epoch 16 : 0.8888\n",
      "Test: ROC AUC score at epoch 16 : 0.9425\n",
      "\n",
      "Training: Loss at epoch 17 : 0.2799000144004822\n",
      "Test: Accuracy at epoch 17 : 0.8969\n",
      "Test: ROC AUC score at epoch 17 : 0.9455\n",
      "\n",
      "Training: Loss at epoch 18 : 0.27129998803138733\n",
      "Test: Accuracy at epoch 18 : 0.9001\n",
      "Test: ROC AUC score at epoch 18 : 0.9465\n",
      "\n",
      "Training: Loss at epoch 19 : 0.2669999897480011\n",
      "Test: Accuracy at epoch 19 : 0.8977\n",
      "Test: ROC AUC score at epoch 19 : 0.9427\n",
      "\n",
      "Training: Loss at epoch 20 : 0.26420000195503235\n",
      "Test: Accuracy at epoch 20 : 0.8985\n",
      "Test: ROC AUC score at epoch 20 : 0.9431\n",
      "\n",
      "Training: Loss at epoch 21 : 0.2606000006198883\n",
      "Test: Accuracy at epoch 21 : 0.9009\n",
      "Test: ROC AUC score at epoch 21 : 0.9411\n",
      "\n",
      "Training: Loss at epoch 22 : 0.2578999996185303\n",
      "Test: Accuracy at epoch 22 : 0.9017\n",
      "Test: ROC AUC score at epoch 22 : 0.9434\n",
      "\n",
      "Training: Loss at epoch 23 : 0.24740000069141388\n",
      "Test: Accuracy at epoch 23 : 0.9049\n",
      "Test: ROC AUC score at epoch 23 : 0.9429\n",
      "\n",
      "Training: Loss at epoch 24 : 0.24549999833106995\n",
      "Test: Accuracy at epoch 24 : 0.9017\n",
      "Test: ROC AUC score at epoch 24 : 0.9439\n",
      "\n",
      "Training: Loss at epoch 25 : 0.23919999599456787\n",
      "Test: Accuracy at epoch 25 : 0.9041\n",
      "Test: ROC AUC score at epoch 25 : 0.9426\n",
      "\n",
      "Training: Loss at epoch 26 : 0.23260000348091125\n",
      "Test: Accuracy at epoch 26 : 0.9057\n",
      "Test: ROC AUC score at epoch 26 : 0.9456\n",
      "\n",
      "Training: Loss at epoch 27 : 0.22599999606609344\n",
      "Test: Accuracy at epoch 27 : 0.9041\n",
      "Test: ROC AUC score at epoch 27 : 0.9443\n",
      "\n",
      "Training: Loss at epoch 28 : 0.21649999916553497\n",
      "Test: Accuracy at epoch 28 : 0.8961\n",
      "Test: ROC AUC score at epoch 28 : 0.9459\n",
      "\n",
      "Training: Loss at epoch 29 : 0.21549999713897705\n",
      "Test: Accuracy at epoch 29 : 0.9017\n",
      "Test: ROC AUC score at epoch 29 : 0.9479\n",
      "\n",
      "Training: Loss at epoch 30 : 0.20360000431537628\n",
      "Test: Accuracy at epoch 30 : 0.9017\n",
      "Test: ROC AUC score at epoch 30 : 0.9462\n",
      "\n",
      "Training: Loss at epoch 31 : 0.193900004029274\n",
      "Test: Accuracy at epoch 31 : 0.8961\n",
      "Test: ROC AUC score at epoch 31 : 0.9438\n",
      "\n",
      "Training: Loss at epoch 32 : 0.18719999492168427\n",
      "Test: Accuracy at epoch 32 : 0.9081\n",
      "Test: ROC AUC score at epoch 32 : 0.9457\n",
      "\n",
      "Training: Loss at epoch 33 : 0.17470000684261322\n",
      "Test: Accuracy at epoch 33 : 0.9081\n",
      "Test: ROC AUC score at epoch 33 : 0.9474\n",
      "\n",
      "Training: Loss at epoch 34 : 0.17030000686645508\n",
      "Test: Accuracy at epoch 34 : 0.9081\n",
      "Test: ROC AUC score at epoch 34 : 0.9499\n",
      "\n",
      "Training: Loss at epoch 35 : 0.163100004196167\n",
      "Test: Accuracy at epoch 35 : 0.9057\n",
      "Test: ROC AUC score at epoch 35 : 0.9532\n",
      "\n",
      "Training: Loss at epoch 36 : 0.14740000665187836\n",
      "Test: Accuracy at epoch 36 : 0.9138\n",
      "Test: ROC AUC score at epoch 36 : 0.953\n",
      "\n",
      "Training: Loss at epoch 37 : 0.13940000534057617\n",
      "Test: Accuracy at epoch 37 : 0.9138\n",
      "Test: ROC AUC score at epoch 37 : 0.9521\n",
      "\n",
      "Training: Loss at epoch 38 : 0.14139999449253082\n",
      "Test: Accuracy at epoch 38 : 0.9081\n",
      "Test: ROC AUC score at epoch 38 : 0.9525\n",
      "\n",
      "Training: Loss at epoch 39 : 0.1348000019788742\n",
      "Test: Accuracy at epoch 39 : 0.9138\n",
      "Test: ROC AUC score at epoch 39 : 0.9538\n",
      "\n",
      "Training: Loss at epoch 40 : 0.1340000033378601\n",
      "Test: Accuracy at epoch 40 : 0.9122\n",
      "Test: ROC AUC score at epoch 40 : 0.9521\n",
      "\n",
      "Training: Loss at epoch 41 : 0.12870000302791595\n",
      "Test: Accuracy at epoch 41 : 0.9073\n",
      "Test: ROC AUC score at epoch 41 : 0.9521\n",
      "\n",
      "Training: Loss at epoch 42 : 0.1282999962568283\n",
      "Test: Accuracy at epoch 42 : 0.9049\n",
      "Test: ROC AUC score at epoch 42 : 0.9505\n",
      "\n",
      "Training: Loss at epoch 43 : 0.1289999932050705\n",
      "Test: Accuracy at epoch 43 : 0.9098\n",
      "Test: ROC AUC score at epoch 43 : 0.9532\n",
      "\n",
      "Training: Loss at epoch 44 : 0.1168999969959259\n",
      "Test: Accuracy at epoch 44 : 0.9057\n",
      "Test: ROC AUC score at epoch 44 : 0.9525\n",
      "\n",
      "Training: Loss at epoch 45 : 0.11509999632835388\n",
      "Test: Accuracy at epoch 45 : 0.9025\n",
      "Test: ROC AUC score at epoch 45 : 0.9512\n",
      "\n",
      "Training: Loss at epoch 46 : 0.09939999878406525\n",
      "Test: Accuracy at epoch 46 : 0.8961\n",
      "Test: ROC AUC score at epoch 46 : 0.9508\n",
      "\n",
      "Training: Loss at epoch 47 : 0.10220000147819519\n",
      "Test: Accuracy at epoch 47 : 0.9033\n",
      "Test: ROC AUC score at epoch 47 : 0.9501\n",
      "\n",
      "Training: Loss at epoch 48 : 0.08209999650716782\n",
      "Test: Accuracy at epoch 48 : 0.9033\n",
      "Test: ROC AUC score at epoch 48 : 0.9503\n",
      "\n",
      "Training: Loss at epoch 49 : 0.07699999958276749\n",
      "Test: Accuracy at epoch 49 : 0.8936\n",
      "Test: ROC AUC score at epoch 49 : 0.951\n",
      "\n",
      "Training: Loss at epoch 50 : 0.08169999718666077\n",
      "Test: Accuracy at epoch 50 : 0.9001\n",
      "Test: ROC AUC score at epoch 50 : 0.9501\n",
      "\n",
      "Training: Loss after 50 epochs: 0.2397872805595398\n",
      "\n",
      "=== FOLD 3 / 5 ===\n",
      "RNN classifier architecture: \n",
      "\n",
      "RNNSeqClassifier(\n",
      "  (embedding): Embedding(4097, 128)\n",
      "  (lstm): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc_hidden): Linear(in_features=200, out_features=64, bias=True)\n",
      "  (fc_relu): ReLU()\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Training: Loss at epoch 1 : 0.604200005531311\n",
      "Test: Accuracy at epoch 1 : 0.6806\n",
      "Test: ROC AUC score at epoch 1 : 0.7913\n",
      "\n",
      "Training: Loss at epoch 2 : 0.5343999862670898\n",
      "Test: Accuracy at epoch 2 : 0.7266\n",
      "Test: ROC AUC score at epoch 2 : 0.8127\n",
      "\n",
      "Training: Loss at epoch 3 : 0.49129998683929443\n",
      "Test: Accuracy at epoch 3 : 0.8323\n",
      "Test: ROC AUC score at epoch 3 : 0.8887\n",
      "\n",
      "Training: Loss at epoch 4 : 0.4359999895095825\n",
      "Test: Accuracy at epoch 4 : 0.8371\n",
      "Test: ROC AUC score at epoch 4 : 0.8991\n",
      "\n",
      "Training: Loss at epoch 5 : 0.38760000467300415\n",
      "Test: Accuracy at epoch 5 : 0.8516\n",
      "Test: ROC AUC score at epoch 5 : 0.9169\n",
      "\n",
      "Training: Loss at epoch 6 : 0.3538999855518341\n",
      "Test: Accuracy at epoch 6 : 0.875\n",
      "Test: ROC AUC score at epoch 6 : 0.9313\n",
      "\n",
      "Training: Loss at epoch 7 : 0.3303000032901764\n",
      "Test: Accuracy at epoch 7 : 0.896\n",
      "Test: ROC AUC score at epoch 7 : 0.9356\n",
      "\n",
      "Training: Loss at epoch 8 : 0.30489999055862427\n",
      "Test: Accuracy at epoch 8 : 0.8871\n",
      "Test: ROC AUC score at epoch 8 : 0.9419\n",
      "\n",
      "Training: Loss at epoch 9 : 0.29269999265670776\n",
      "Test: Accuracy at epoch 9 : 0.8976\n",
      "Test: ROC AUC score at epoch 9 : 0.9442\n",
      "\n",
      "Training: Loss at epoch 10 : 0.28130000829696655\n",
      "Test: Accuracy at epoch 10 : 0.8992\n",
      "Test: ROC AUC score at epoch 10 : 0.9479\n",
      "\n",
      "Training: Loss at epoch 11 : 0.26759999990463257\n",
      "Test: Accuracy at epoch 11 : 0.9081\n",
      "Test: ROC AUC score at epoch 11 : 0.9528\n",
      "\n",
      "Training: Loss at epoch 12 : 0.2522999942302704\n",
      "Test: Accuracy at epoch 12 : 0.9089\n",
      "Test: ROC AUC score at epoch 12 : 0.9521\n",
      "\n",
      "Training: Loss at epoch 13 : 0.24740000069141388\n",
      "Test: Accuracy at epoch 13 : 0.9065\n",
      "Test: ROC AUC score at epoch 13 : 0.9519\n",
      "\n",
      "Training: Loss at epoch 14 : 0.23350000381469727\n",
      "Test: Accuracy at epoch 14 : 0.9137\n",
      "Test: ROC AUC score at epoch 14 : 0.9534\n",
      "\n",
      "Training: Loss at epoch 15 : 0.22030000388622284\n",
      "Test: Accuracy at epoch 15 : 0.9065\n",
      "Test: ROC AUC score at epoch 15 : 0.9546\n",
      "\n",
      "Training: Loss at epoch 16 : 0.21469999849796295\n",
      "Test: Accuracy at epoch 16 : 0.9137\n",
      "Test: ROC AUC score at epoch 16 : 0.9512\n",
      "\n",
      "Training: Loss at epoch 17 : 0.20649999380111694\n",
      "Test: Accuracy at epoch 17 : 0.9161\n",
      "Test: ROC AUC score at epoch 17 : 0.9536\n",
      "\n",
      "Training: Loss at epoch 18 : 0.1965000033378601\n",
      "Test: Accuracy at epoch 18 : 0.9056\n",
      "Test: ROC AUC score at epoch 18 : 0.9546\n",
      "\n",
      "Training: Loss at epoch 19 : 0.1873999983072281\n",
      "Test: Accuracy at epoch 19 : 0.9113\n",
      "Test: ROC AUC score at epoch 19 : 0.9559\n",
      "\n",
      "Training: Loss at epoch 20 : 0.17489999532699585\n",
      "Test: Accuracy at epoch 20 : 0.9105\n",
      "Test: ROC AUC score at epoch 20 : 0.9546\n",
      "\n",
      "Training: Loss at epoch 21 : 0.16990000009536743\n",
      "Test: Accuracy at epoch 21 : 0.9065\n",
      "Test: ROC AUC score at epoch 21 : 0.9544\n",
      "\n",
      "Training: Loss at epoch 22 : 0.163100004196167\n",
      "Test: Accuracy at epoch 22 : 0.9048\n",
      "Test: ROC AUC score at epoch 22 : 0.9551\n",
      "\n",
      "Training: Loss at epoch 23 : 0.15719999372959137\n",
      "Test: Accuracy at epoch 23 : 0.904\n",
      "Test: ROC AUC score at epoch 23 : 0.954\n",
      "\n",
      "Training: Loss at epoch 24 : 0.1477999985218048\n",
      "Test: Accuracy at epoch 24 : 0.904\n",
      "Test: ROC AUC score at epoch 24 : 0.9562\n",
      "\n",
      "Training: Loss at epoch 25 : 0.133200004696846\n",
      "Test: Accuracy at epoch 25 : 0.9073\n",
      "Test: ROC AUC score at epoch 25 : 0.9549\n",
      "\n",
      "Training: Loss at epoch 26 : 0.1387999951839447\n",
      "Test: Accuracy at epoch 26 : 0.904\n",
      "Test: ROC AUC score at epoch 26 : 0.9536\n",
      "\n",
      "Training: Loss at epoch 27 : 0.1200999990105629\n",
      "Test: Accuracy at epoch 27 : 0.9\n",
      "Test: ROC AUC score at epoch 27 : 0.9487\n",
      "\n",
      "Training: Loss at epoch 28 : 0.1080000028014183\n",
      "Test: Accuracy at epoch 28 : 0.8984\n",
      "Test: ROC AUC score at epoch 28 : 0.9488\n",
      "\n",
      "Training: Loss at epoch 29 : 0.11150000244379044\n",
      "Test: Accuracy at epoch 29 : 0.896\n",
      "Test: ROC AUC score at epoch 29 : 0.9477\n",
      "\n",
      "Training: Loss at epoch 30 : 0.10220000147819519\n",
      "Test: Accuracy at epoch 30 : 0.8976\n",
      "Test: ROC AUC score at epoch 30 : 0.9497\n",
      "\n",
      "Training: Loss at epoch 31 : 0.09040000289678574\n",
      "Test: Accuracy at epoch 31 : 0.8911\n",
      "Test: ROC AUC score at epoch 31 : 0.9433\n",
      "\n",
      "Training: Loss at epoch 32 : 0.0917000025510788\n",
      "Test: Accuracy at epoch 32 : 0.8919\n",
      "Test: ROC AUC score at epoch 32 : 0.9451\n",
      "\n",
      "Training: Loss at epoch 33 : 0.08089999854564667\n",
      "Test: Accuracy at epoch 33 : 0.8831\n",
      "Test: ROC AUC score at epoch 33 : 0.9396\n",
      "\n",
      "Training: Loss at epoch 34 : 0.07840000092983246\n",
      "Test: Accuracy at epoch 34 : 0.9016\n",
      "Test: ROC AUC score at epoch 34 : 0.9435\n",
      "\n",
      "Training: Loss at epoch 35 : 0.06809999793767929\n",
      "Test: Accuracy at epoch 35 : 0.8589\n",
      "Test: ROC AUC score at epoch 35 : 0.9237\n",
      "\n",
      "Training: Loss at epoch 36 : 0.06520000100135803\n",
      "Test: Accuracy at epoch 36 : 0.8903\n",
      "Test: ROC AUC score at epoch 36 : 0.9418\n",
      "\n",
      "Training: Loss at epoch 37 : 0.06629999727010727\n",
      "Test: Accuracy at epoch 37 : 0.8911\n",
      "Test: ROC AUC score at epoch 37 : 0.9338\n",
      "\n",
      "Training: Loss at epoch 38 : 0.05620000138878822\n",
      "Test: Accuracy at epoch 38 : 0.8984\n",
      "Test: ROC AUC score at epoch 38 : 0.9431\n",
      "\n",
      "Training: Loss at epoch 39 : 0.04989999905228615\n",
      "Test: Accuracy at epoch 39 : 0.896\n",
      "Test: ROC AUC score at epoch 39 : 0.9395\n",
      "\n",
      "Training: Loss at epoch 40 : 0.042399998754262924\n",
      "Test: Accuracy at epoch 40 : 0.8903\n",
      "Test: ROC AUC score at epoch 40 : 0.9342\n",
      "\n",
      "Training: Loss at epoch 41 : 0.0478999987244606\n",
      "Test: Accuracy at epoch 41 : 0.8589\n",
      "Test: ROC AUC score at epoch 41 : 0.9123\n",
      "\n",
      "Training: Loss at epoch 42 : 0.04699999839067459\n",
      "Test: Accuracy at epoch 42 : 0.879\n",
      "Test: ROC AUC score at epoch 42 : 0.9347\n",
      "\n",
      "Training: Loss at epoch 43 : 0.052400000393390656\n",
      "Test: Accuracy at epoch 43 : 0.8734\n",
      "Test: ROC AUC score at epoch 43 : 0.9299\n",
      "\n",
      "Training: Loss at epoch 44 : 0.04560000076889992\n",
      "Test: Accuracy at epoch 44 : 0.8887\n",
      "Test: ROC AUC score at epoch 44 : 0.9318\n",
      "\n",
      "Training: Loss at epoch 45 : 0.04820000007748604\n",
      "Test: Accuracy at epoch 45 : 0.879\n",
      "Test: ROC AUC score at epoch 45 : 0.9288\n",
      "\n",
      "Training: Loss at epoch 46 : 0.02759999968111515\n",
      "Test: Accuracy at epoch 46 : 0.8919\n",
      "Test: ROC AUC score at epoch 46 : 0.9363\n",
      "\n",
      "Training: Loss at epoch 47 : 0.02539999969303608\n",
      "Test: Accuracy at epoch 47 : 0.8831\n",
      "Test: ROC AUC score at epoch 47 : 0.9337\n",
      "\n",
      "Training: Loss at epoch 48 : 0.021700000390410423\n",
      "Test: Accuracy at epoch 48 : 0.8839\n",
      "Test: ROC AUC score at epoch 48 : 0.9391\n",
      "\n",
      "Training: Loss at epoch 49 : 0.041200000792741776\n",
      "Test: Accuracy at epoch 49 : 0.8823\n",
      "Test: ROC AUC score at epoch 49 : 0.9325\n",
      "\n",
      "Training: Loss at epoch 50 : 0.026599999517202377\n",
      "Test: Accuracy at epoch 50 : 0.8774\n",
      "Test: ROC AUC score at epoch 50 : 0.9262\n",
      "\n",
      "Training: Loss after 50 epochs: 0.17290855944156647\n",
      "\n",
      "=== FOLD 4 / 5 ===\n",
      "RNN classifier architecture: \n",
      "\n",
      "RNNSeqClassifier(\n",
      "  (embedding): Embedding(4097, 128)\n",
      "  (lstm): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc_hidden): Linear(in_features=200, out_features=64, bias=True)\n",
      "  (fc_relu): ReLU()\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Training: Loss at epoch 1 : 0.5523999929428101\n",
      "Test: Accuracy at epoch 1 : 0.8323\n",
      "Test: ROC AUC score at epoch 1 : 0.8658\n",
      "\n",
      "Training: Loss at epoch 2 : 0.4189000129699707\n",
      "Test: Accuracy at epoch 2 : 0.8508\n",
      "Test: ROC AUC score at epoch 2 : 0.8999\n",
      "\n",
      "Training: Loss at epoch 3 : 0.3937000036239624\n",
      "Test: Accuracy at epoch 3 : 0.8403\n",
      "Test: ROC AUC score at epoch 3 : 0.9027\n",
      "\n",
      "Training: Loss at epoch 4 : 0.37290000915527344\n",
      "Test: Accuracy at epoch 4 : 0.8371\n",
      "Test: ROC AUC score at epoch 4 : 0.9089\n",
      "\n",
      "Training: Loss at epoch 5 : 0.3450999855995178\n",
      "Test: Accuracy at epoch 5 : 0.8766\n",
      "Test: ROC AUC score at epoch 5 : 0.9186\n",
      "\n",
      "Training: Loss at epoch 6 : 0.33880001306533813\n",
      "Test: Accuracy at epoch 6 : 0.8863\n",
      "Test: ROC AUC score at epoch 6 : 0.9181\n",
      "\n",
      "Training: Loss at epoch 7 : 0.3203999996185303\n",
      "Test: Accuracy at epoch 7 : 0.8742\n",
      "Test: ROC AUC score at epoch 7 : 0.9225\n",
      "\n",
      "Training: Loss at epoch 8 : 0.3098999857902527\n",
      "Test: Accuracy at epoch 8 : 0.8879\n",
      "Test: ROC AUC score at epoch 8 : 0.9233\n",
      "\n",
      "Training: Loss at epoch 9 : 0.30550000071525574\n",
      "Test: Accuracy at epoch 9 : 0.8661\n",
      "Test: ROC AUC score at epoch 9 : 0.9253\n",
      "\n",
      "Training: Loss at epoch 10 : 0.29260000586509705\n",
      "Test: Accuracy at epoch 10 : 0.8903\n",
      "Test: ROC AUC score at epoch 10 : 0.9308\n",
      "\n",
      "Training: Loss at epoch 11 : 0.2831000089645386\n",
      "Test: Accuracy at epoch 11 : 0.9008\n",
      "Test: ROC AUC score at epoch 11 : 0.9398\n",
      "\n",
      "Training: Loss at epoch 12 : 0.26339998841285706\n",
      "Test: Accuracy at epoch 12 : 0.8992\n",
      "Test: ROC AUC score at epoch 12 : 0.943\n",
      "\n",
      "Training: Loss at epoch 13 : 0.25780001282691956\n",
      "Test: Accuracy at epoch 13 : 0.9008\n",
      "Test: ROC AUC score at epoch 13 : 0.9471\n",
      "\n",
      "Training: Loss at epoch 14 : 0.24709999561309814\n",
      "Test: Accuracy at epoch 14 : 0.9048\n",
      "Test: ROC AUC score at epoch 14 : 0.9497\n",
      "\n",
      "Training: Loss at epoch 15 : 0.2379000037908554\n",
      "Test: Accuracy at epoch 15 : 0.9\n",
      "Test: ROC AUC score at epoch 15 : 0.9478\n",
      "\n",
      "Training: Loss at epoch 16 : 0.23029999434947968\n",
      "Test: Accuracy at epoch 16 : 0.9024\n",
      "Test: ROC AUC score at epoch 16 : 0.9475\n",
      "\n",
      "Training: Loss at epoch 17 : 0.22190000116825104\n",
      "Test: Accuracy at epoch 17 : 0.9008\n",
      "Test: ROC AUC score at epoch 17 : 0.9485\n",
      "\n",
      "Training: Loss at epoch 18 : 0.21469999849796295\n",
      "Test: Accuracy at epoch 18 : 0.9056\n",
      "Test: ROC AUC score at epoch 18 : 0.9501\n",
      "\n",
      "Training: Loss at epoch 19 : 0.20600000023841858\n",
      "Test: Accuracy at epoch 19 : 0.8944\n",
      "Test: ROC AUC score at epoch 19 : 0.9448\n",
      "\n",
      "Training: Loss at epoch 20 : 0.20059999823570251\n",
      "Test: Accuracy at epoch 20 : 0.9073\n",
      "Test: ROC AUC score at epoch 20 : 0.9498\n",
      "\n",
      "Training: Loss at epoch 21 : 0.1972000002861023\n",
      "Test: Accuracy at epoch 21 : 0.9065\n",
      "Test: ROC AUC score at epoch 21 : 0.9502\n",
      "\n",
      "Training: Loss at epoch 22 : 0.18459999561309814\n",
      "Test: Accuracy at epoch 22 : 0.9008\n",
      "Test: ROC AUC score at epoch 22 : 0.9514\n",
      "\n",
      "Training: Loss at epoch 23 : 0.18289999663829803\n",
      "Test: Accuracy at epoch 23 : 0.7702\n",
      "Test: ROC AUC score at epoch 23 : 0.9253\n",
      "\n",
      "Training: Loss at epoch 24 : 0.21140000224113464\n",
      "Test: Accuracy at epoch 24 : 0.8887\n",
      "Test: ROC AUC score at epoch 24 : 0.95\n",
      "\n",
      "Training: Loss at epoch 25 : 0.17479999363422394\n",
      "Test: Accuracy at epoch 25 : 0.9048\n",
      "Test: ROC AUC score at epoch 25 : 0.9508\n",
      "\n",
      "Training: Loss at epoch 26 : 0.16259999573230743\n",
      "Test: Accuracy at epoch 26 : 0.9089\n",
      "Test: ROC AUC score at epoch 26 : 0.9489\n",
      "\n",
      "Training: Loss at epoch 27 : 0.163100004196167\n",
      "Test: Accuracy at epoch 27 : 0.9081\n",
      "Test: ROC AUC score at epoch 27 : 0.9514\n",
      "\n",
      "Training: Loss at epoch 28 : 0.15919999778270721\n",
      "Test: Accuracy at epoch 28 : 0.9048\n",
      "Test: ROC AUC score at epoch 28 : 0.9496\n",
      "\n",
      "Training: Loss at epoch 29 : 0.14659999310970306\n",
      "Test: Accuracy at epoch 29 : 0.9056\n",
      "Test: ROC AUC score at epoch 29 : 0.9469\n",
      "\n",
      "Training: Loss at epoch 30 : 0.15139999985694885\n",
      "Test: Accuracy at epoch 30 : 0.9016\n",
      "Test: ROC AUC score at epoch 30 : 0.9477\n",
      "\n",
      "Training: Loss at epoch 31 : 0.1356000006198883\n",
      "Test: Accuracy at epoch 31 : 0.904\n",
      "Test: ROC AUC score at epoch 31 : 0.946\n",
      "\n",
      "Training: Loss at epoch 32 : 0.13359999656677246\n",
      "Test: Accuracy at epoch 32 : 0.8976\n",
      "Test: ROC AUC score at epoch 32 : 0.9469\n",
      "\n",
      "Training: Loss at epoch 33 : 0.12020000070333481\n",
      "Test: Accuracy at epoch 33 : 0.904\n",
      "Test: ROC AUC score at epoch 33 : 0.9483\n",
      "\n",
      "Training: Loss at epoch 34 : 0.12210000306367874\n",
      "Test: Accuracy at epoch 34 : 0.9016\n",
      "Test: ROC AUC score at epoch 34 : 0.9502\n",
      "\n",
      "Training: Loss at epoch 35 : 0.13529999554157257\n",
      "Test: Accuracy at epoch 35 : 0.8839\n",
      "Test: ROC AUC score at epoch 35 : 0.9433\n",
      "\n",
      "Training: Loss at epoch 36 : 0.1281999945640564\n",
      "Test: Accuracy at epoch 36 : 0.9032\n",
      "Test: ROC AUC score at epoch 36 : 0.9501\n",
      "\n",
      "Training: Loss at epoch 37 : 0.10429999977350235\n",
      "Test: Accuracy at epoch 37 : 0.8968\n",
      "Test: ROC AUC score at epoch 37 : 0.9451\n",
      "\n",
      "Training: Loss at epoch 38 : 0.10289999842643738\n",
      "Test: Accuracy at epoch 38 : 0.8992\n",
      "Test: ROC AUC score at epoch 38 : 0.9469\n",
      "\n",
      "Training: Loss at epoch 39 : 0.10339999943971634\n",
      "Test: Accuracy at epoch 39 : 0.896\n",
      "Test: ROC AUC score at epoch 39 : 0.946\n",
      "\n",
      "Training: Loss at epoch 40 : 0.08609999716281891\n",
      "Test: Accuracy at epoch 40 : 0.8976\n",
      "Test: ROC AUC score at epoch 40 : 0.9472\n",
      "\n",
      "Training: Loss at epoch 41 : 0.0771000012755394\n",
      "Test: Accuracy at epoch 41 : 0.8992\n",
      "Test: ROC AUC score at epoch 41 : 0.9428\n",
      "\n",
      "Training: Loss at epoch 42 : 0.07970000058412552\n",
      "Test: Accuracy at epoch 42 : 0.8831\n",
      "Test: ROC AUC score at epoch 42 : 0.9413\n",
      "\n",
      "Training: Loss at epoch 43 : 0.07349999994039536\n",
      "Test: Accuracy at epoch 43 : 0.8944\n",
      "Test: ROC AUC score at epoch 43 : 0.9469\n",
      "\n",
      "Training: Loss at epoch 44 : 0.05950000137090683\n",
      "Test: Accuracy at epoch 44 : 0.9\n",
      "Test: ROC AUC score at epoch 44 : 0.9448\n",
      "\n",
      "Training: Loss at epoch 45 : 0.0608999989926815\n",
      "Test: Accuracy at epoch 45 : 0.8911\n",
      "Test: ROC AUC score at epoch 45 : 0.9405\n",
      "\n",
      "Training: Loss at epoch 46 : 0.05790000036358833\n",
      "Test: Accuracy at epoch 46 : 0.8863\n",
      "Test: ROC AUC score at epoch 46 : 0.9421\n",
      "\n",
      "Training: Loss at epoch 47 : 0.044599998742341995\n",
      "Test: Accuracy at epoch 47 : 0.9\n",
      "Test: ROC AUC score at epoch 47 : 0.9447\n",
      "\n",
      "Training: Loss at epoch 48 : 0.0478999987244606\n",
      "Test: Accuracy at epoch 48 : 0.8992\n",
      "Test: ROC AUC score at epoch 48 : 0.947\n",
      "\n",
      "Training: Loss at epoch 49 : 0.043699998408555984\n",
      "Test: Accuracy at epoch 49 : 0.9\n",
      "Test: ROC AUC score at epoch 49 : 0.9437\n",
      "\n",
      "Training: Loss at epoch 50 : 0.038100000470876694\n",
      "Test: Accuracy at epoch 50 : 0.9\n",
      "Test: ROC AUC score at epoch 50 : 0.9451\n",
      "\n",
      "Training: Loss after 50 epochs: 0.18998068571090698\n",
      "\n",
      "=== FOLD 5 / 5 ===\n",
      "RNN classifier architecture: \n",
      "\n",
      "RNNSeqClassifier(\n",
      "  (embedding): Embedding(4097, 128)\n",
      "  (lstm): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc_hidden): Linear(in_features=200, out_features=64, bias=True)\n",
      "  (fc_relu): ReLU()\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Training: Loss at epoch 1 : 0.5812000036239624\n",
      "Test: Accuracy at epoch 1 : 0.7887\n",
      "Test: ROC AUC score at epoch 1 : 0.854\n",
      "\n",
      "Training: Loss at epoch 2 : 0.4374000132083893\n",
      "Test: Accuracy at epoch 2 : 0.775\n",
      "Test: ROC AUC score at epoch 2 : 0.8642\n",
      "\n",
      "Training: Loss at epoch 3 : 0.359499990940094\n",
      "Test: Accuracy at epoch 3 : 0.8605\n",
      "Test: ROC AUC score at epoch 3 : 0.9007\n",
      "\n",
      "Training: Loss at epoch 4 : 0.3312000036239624\n",
      "Test: Accuracy at epoch 4 : 0.8621\n",
      "Test: ROC AUC score at epoch 4 : 0.9105\n",
      "\n",
      "Training: Loss at epoch 5 : 0.31360000371932983\n",
      "Test: Accuracy at epoch 5 : 0.8629\n",
      "Test: ROC AUC score at epoch 5 : 0.9209\n",
      "\n",
      "Training: Loss at epoch 6 : 0.2937999963760376\n",
      "Test: Accuracy at epoch 6 : 0.8685\n",
      "Test: ROC AUC score at epoch 6 : 0.9257\n",
      "\n",
      "Training: Loss at epoch 7 : 0.295199990272522\n",
      "Test: Accuracy at epoch 7 : 0.8726\n",
      "Test: ROC AUC score at epoch 7 : 0.9262\n",
      "\n",
      "Training: Loss at epoch 8 : 0.27239999175071716\n",
      "Test: Accuracy at epoch 8 : 0.8782\n",
      "Test: ROC AUC score at epoch 8 : 0.9326\n",
      "\n",
      "Training: Loss at epoch 9 : 0.26579999923706055\n",
      "Test: Accuracy at epoch 9 : 0.8758\n",
      "Test: ROC AUC score at epoch 9 : 0.9309\n",
      "\n",
      "Training: Loss at epoch 10 : 0.26089999079704285\n",
      "Test: Accuracy at epoch 10 : 0.871\n",
      "Test: ROC AUC score at epoch 10 : 0.9307\n",
      "\n",
      "Training: Loss at epoch 11 : 0.2556000053882599\n",
      "Test: Accuracy at epoch 11 : 0.8726\n",
      "Test: ROC AUC score at epoch 11 : 0.9339\n",
      "\n",
      "Training: Loss at epoch 12 : 0.25029999017715454\n",
      "Test: Accuracy at epoch 12 : 0.8758\n",
      "Test: ROC AUC score at epoch 12 : 0.9325\n",
      "\n",
      "Training: Loss at epoch 13 : 0.24009999632835388\n",
      "Test: Accuracy at epoch 13 : 0.8798\n",
      "Test: ROC AUC score at epoch 13 : 0.9339\n",
      "\n",
      "Training: Loss at epoch 14 : 0.23739999532699585\n",
      "Test: Accuracy at epoch 14 : 0.875\n",
      "Test: ROC AUC score at epoch 14 : 0.9392\n",
      "\n",
      "Training: Loss at epoch 15 : 0.22840000689029694\n",
      "Test: Accuracy at epoch 15 : 0.8774\n",
      "Test: ROC AUC score at epoch 15 : 0.9414\n",
      "\n",
      "Training: Loss at epoch 16 : 0.22030000388622284\n",
      "Test: Accuracy at epoch 16 : 0.8766\n",
      "Test: ROC AUC score at epoch 16 : 0.9394\n",
      "\n",
      "Training: Loss at epoch 17 : 0.21619999408721924\n",
      "Test: Accuracy at epoch 17 : 0.8855\n",
      "Test: ROC AUC score at epoch 17 : 0.9421\n",
      "\n",
      "Training: Loss at epoch 18 : 0.21040000021457672\n",
      "Test: Accuracy at epoch 18 : 0.8887\n",
      "Test: ROC AUC score at epoch 18 : 0.9447\n",
      "\n",
      "Training: Loss at epoch 19 : 0.20180000364780426\n",
      "Test: Accuracy at epoch 19 : 0.8895\n",
      "Test: ROC AUC score at epoch 19 : 0.9417\n",
      "\n",
      "Training: Loss at epoch 20 : 0.20340000092983246\n",
      "Test: Accuracy at epoch 20 : 0.8927\n",
      "Test: ROC AUC score at epoch 20 : 0.9439\n",
      "\n",
      "Training: Loss at epoch 21 : 0.18790000677108765\n",
      "Test: Accuracy at epoch 21 : 0.8935\n",
      "Test: ROC AUC score at epoch 21 : 0.942\n",
      "\n",
      "Training: Loss at epoch 22 : 0.1800999939441681\n",
      "Test: Accuracy at epoch 22 : 0.8984\n",
      "Test: ROC AUC score at epoch 22 : 0.9405\n",
      "\n",
      "Training: Loss at epoch 23 : 0.17720000445842743\n",
      "Test: Accuracy at epoch 23 : 0.8911\n",
      "Test: ROC AUC score at epoch 23 : 0.9396\n",
      "\n",
      "Training: Loss at epoch 24 : 0.16689999401569366\n",
      "Test: Accuracy at epoch 24 : 0.8944\n",
      "Test: ROC AUC score at epoch 24 : 0.9401\n",
      "\n",
      "Training: Loss at epoch 25 : 0.1762000024318695\n",
      "Test: Accuracy at epoch 25 : 0.8903\n",
      "Test: ROC AUC score at epoch 25 : 0.9391\n",
      "\n",
      "Training: Loss at epoch 26 : 0.15960000455379486\n",
      "Test: Accuracy at epoch 26 : 0.8968\n",
      "Test: ROC AUC score at epoch 26 : 0.9405\n",
      "\n",
      "Training: Loss at epoch 27 : 0.1597999930381775\n",
      "Test: Accuracy at epoch 27 : 0.8952\n",
      "Test: ROC AUC score at epoch 27 : 0.9403\n",
      "\n",
      "Training: Loss at epoch 28 : 0.1526000052690506\n",
      "Test: Accuracy at epoch 28 : 0.8927\n",
      "Test: ROC AUC score at epoch 28 : 0.9385\n",
      "\n",
      "Training: Loss at epoch 29 : 0.14219999313354492\n",
      "Test: Accuracy at epoch 29 : 0.8895\n",
      "Test: ROC AUC score at epoch 29 : 0.9353\n",
      "\n",
      "Training: Loss at epoch 30 : 0.13619999587535858\n",
      "Test: Accuracy at epoch 30 : 0.8903\n",
      "Test: ROC AUC score at epoch 30 : 0.9366\n",
      "\n",
      "Training: Loss at epoch 31 : 0.12189999967813492\n",
      "Test: Accuracy at epoch 31 : 0.8911\n",
      "Test: ROC AUC score at epoch 31 : 0.9348\n",
      "\n",
      "Training: Loss at epoch 32 : 0.12210000306367874\n",
      "Test: Accuracy at epoch 32 : 0.8895\n",
      "Test: ROC AUC score at epoch 32 : 0.9365\n",
      "\n",
      "Training: Loss at epoch 33 : 0.13079999387264252\n",
      "Test: Accuracy at epoch 33 : 0.8968\n",
      "Test: ROC AUC score at epoch 33 : 0.9373\n",
      "\n",
      "Training: Loss at epoch 34 : 0.1103999987244606\n",
      "Test: Accuracy at epoch 34 : 0.8944\n",
      "Test: ROC AUC score at epoch 34 : 0.94\n",
      "\n",
      "Training: Loss at epoch 35 : 0.10939999669790268\n",
      "Test: Accuracy at epoch 35 : 0.8863\n",
      "Test: ROC AUC score at epoch 35 : 0.9383\n",
      "\n",
      "Training: Loss at epoch 36 : 0.09480000287294388\n",
      "Test: Accuracy at epoch 36 : 0.8839\n",
      "Test: ROC AUC score at epoch 36 : 0.9359\n",
      "\n",
      "Training: Loss at epoch 37 : 0.08739999681711197\n",
      "Test: Accuracy at epoch 37 : 0.8863\n",
      "Test: ROC AUC score at epoch 37 : 0.9347\n",
      "\n",
      "Training: Loss at epoch 38 : 0.09600000083446503\n",
      "Test: Accuracy at epoch 38 : 0.8855\n",
      "Test: ROC AUC score at epoch 38 : 0.9316\n",
      "\n",
      "Training: Loss at epoch 39 : 0.0731000006198883\n",
      "Test: Accuracy at epoch 39 : 0.8911\n",
      "Test: ROC AUC score at epoch 39 : 0.9342\n",
      "\n",
      "Training: Loss at epoch 40 : 0.08910000324249268\n",
      "Test: Accuracy at epoch 40 : 0.8927\n",
      "Test: ROC AUC score at epoch 40 : 0.9341\n",
      "\n",
      "Training: Loss at epoch 41 : 0.06780000030994415\n",
      "Test: Accuracy at epoch 41 : 0.8919\n",
      "Test: ROC AUC score at epoch 41 : 0.9291\n",
      "\n",
      "Training: Loss at epoch 42 : 0.0892999991774559\n",
      "Test: Accuracy at epoch 42 : 0.8871\n",
      "Test: ROC AUC score at epoch 42 : 0.9302\n",
      "\n",
      "Training: Loss at epoch 43 : 0.07100000232458115\n",
      "Test: Accuracy at epoch 43 : 0.8968\n",
      "Test: ROC AUC score at epoch 43 : 0.9324\n",
      "\n",
      "Training: Loss at epoch 44 : 0.060100000351667404\n",
      "Test: Accuracy at epoch 44 : 0.8984\n",
      "Test: ROC AUC score at epoch 44 : 0.9297\n",
      "\n",
      "Training: Loss at epoch 45 : 0.061500001698732376\n",
      "Test: Accuracy at epoch 45 : 0.8976\n",
      "Test: ROC AUC score at epoch 45 : 0.9343\n",
      "\n",
      "Training: Loss at epoch 46 : 0.05999999865889549\n",
      "Test: Accuracy at epoch 46 : 0.8968\n",
      "Test: ROC AUC score at epoch 46 : 0.9345\n",
      "\n",
      "Training: Loss at epoch 47 : 0.043299999088048935\n",
      "Test: Accuracy at epoch 47 : 0.8911\n",
      "Test: ROC AUC score at epoch 47 : 0.9315\n",
      "\n",
      "Training: Loss at epoch 48 : 0.061500001698732376\n",
      "Test: Accuracy at epoch 48 : 0.896\n",
      "Test: ROC AUC score at epoch 48 : 0.9329\n",
      "\n",
      "Training: Loss at epoch 49 : 0.044199999421834946\n",
      "Test: Accuracy at epoch 49 : 0.8935\n",
      "Test: ROC AUC score at epoch 49 : 0.9297\n",
      "\n",
      "Training: Loss at epoch 50 : 0.048900000751018524\n",
      "Test: Accuracy at epoch 50 : 0.8871\n",
      "Test: ROC AUC score at epoch 50 : 0.9267\n",
      "\n",
      "Training: Loss after 50 epochs: 0.17915695905685425\n"
     ]
    }
   ],
   "source": [
    "# Alternative for RNN to train and evaluate using 5-fold cross-validation\n",
    "if __name__ == \"__main__\":\n",
    "    pos_fasta = \"./data/positives.fa\"\n",
    "    neg_fasta = \"./data/negatives.fa\"\n",
    "\n",
    "    pos_seqs_dic = read_fasta_into_dic(pos_fasta, \n",
    "                                       convert_to_rna=True, \n",
    "                                       all_uc=True,\n",
    "                                       skip_n_seqs=True)\n",
    "    neg_seqs_dic = read_fasta_into_dic(neg_fasta,\n",
    "                                       convert_to_rna=True, \n",
    "                                       all_uc=True,\n",
    "                                       skip_n_seqs=True)\n",
    "\n",
    "    # Create 5 folds\n",
    "    folds = merge_pos_neg_kfold(pos_seqs_dic, neg_seqs_dic, n_splits=5, k=3)\n",
    "\n",
    "    \n",
    "    vocab = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    f_dict, r_dict = get_all_possible_words(vocab, kmer_size=3)\n",
    "\n",
    "    \n",
    "    all_aucs = []\n",
    "    all_accs = []\n",
    "\n",
    "    for i, (train_df, test_df) in enumerate(folds):\n",
    "        print(f\"\\n=== FOLD {i+1} / {len(folds)} ===\")\n",
    "\n",
    "        # Convert train_df & test_df sequences into integer matrices\n",
    "        X_train, y_train = convert_seq_2_integers(train_df, r_dict)\n",
    "        X_test, y_test = convert_seq_2_integers(test_df, r_dict)\n",
    "\n",
    "        train_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell to train and evaluate the RNN on thr full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN classifier architecture: \n",
      "\n",
      "RNNSeqClassifier(\n",
      "  (embedding): Embedding(4097, 128)\n",
      "  (lstm): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc_hidden): Linear(in_features=200, out_features=64, bias=True)\n",
      "  (fc_relu): ReLU()\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Training: Loss at epoch 1 : 0.5515999794006348\n",
      "Test: Accuracy at epoch 1 : 0.8308\n",
      "Test: ROC AUC score at epoch 1 : 0.8789\n",
      "\n",
      "Training: Loss at epoch 2 : 0.4058000147342682\n",
      "Test: Accuracy at epoch 2 : 0.8501\n",
      "Test: ROC AUC score at epoch 2 : 0.8872\n",
      "\n",
      "Training: Loss at epoch 3 : 0.3767000138759613\n",
      "Test: Accuracy at epoch 3 : 0.867\n",
      "Test: ROC AUC score at epoch 3 : 0.9014\n",
      "\n",
      "Training: Loss at epoch 4 : 0.358599990606308\n",
      "Test: Accuracy at epoch 4 : 0.884\n",
      "Test: ROC AUC score at epoch 4 : 0.9122\n",
      "\n",
      "Training: Loss at epoch 5 : 0.366100013256073\n",
      "Test: Accuracy at epoch 5 : 0.8453\n",
      "Test: ROC AUC score at epoch 5 : 0.9125\n",
      "\n",
      "Training: Loss at epoch 6 : 0.33500000834465027\n",
      "Test: Accuracy at epoch 6 : 0.8791\n",
      "Test: ROC AUC score at epoch 6 : 0.9253\n",
      "\n",
      "Training: Loss at epoch 7 : 0.3059999942779541\n",
      "Test: Accuracy at epoch 7 : 0.8952\n",
      "Test: ROC AUC score at epoch 7 : 0.9337\n",
      "\n",
      "Training: Loss at epoch 8 : 0.29179999232292175\n",
      "Test: Accuracy at epoch 8 : 0.8896\n",
      "Test: ROC AUC score at epoch 8 : 0.9308\n",
      "\n",
      "Training: Loss at epoch 9 : 0.274399995803833\n",
      "Test: Accuracy at epoch 9 : 0.8985\n",
      "Test: ROC AUC score at epoch 9 : 0.9373\n",
      "\n",
      "Training: Loss at epoch 10 : 0.2653999924659729\n",
      "Test: Accuracy at epoch 10 : 0.8993\n",
      "Test: ROC AUC score at epoch 10 : 0.9406\n",
      "\n",
      "Training: Loss at epoch 11 : 0.25279998779296875\n",
      "Test: Accuracy at epoch 11 : 0.9009\n",
      "Test: ROC AUC score at epoch 11 : 0.9414\n",
      "\n",
      "Training: Loss at epoch 12 : 0.24480000138282776\n",
      "Test: Accuracy at epoch 12 : 0.8969\n",
      "Test: ROC AUC score at epoch 12 : 0.9415\n",
      "\n",
      "Training: Loss at epoch 13 : 0.2363000065088272\n",
      "Test: Accuracy at epoch 13 : 0.8961\n",
      "Test: ROC AUC score at epoch 13 : 0.9436\n",
      "\n",
      "Training: Loss at epoch 14 : 0.226500004529953\n",
      "Test: Accuracy at epoch 14 : 0.8977\n",
      "Test: ROC AUC score at epoch 14 : 0.9416\n",
      "\n",
      "Training: Loss at epoch 15 : 0.21819999814033508\n",
      "Test: Accuracy at epoch 15 : 0.8952\n",
      "Test: ROC AUC score at epoch 15 : 0.9435\n",
      "\n",
      "Training: Loss at epoch 16 : 0.21250000596046448\n",
      "Test: Accuracy at epoch 16 : 0.8952\n",
      "Test: ROC AUC score at epoch 16 : 0.9485\n",
      "\n",
      "Training: Loss at epoch 17 : 0.20309999585151672\n",
      "Test: Accuracy at epoch 17 : 0.9017\n",
      "Test: ROC AUC score at epoch 17 : 0.9476\n",
      "\n",
      "Training: Loss at epoch 18 : 0.18940000236034393\n",
      "Test: Accuracy at epoch 18 : 0.9009\n",
      "Test: ROC AUC score at epoch 18 : 0.9473\n",
      "\n",
      "Training: Loss at epoch 19 : 0.18400000035762787\n",
      "Test: Accuracy at epoch 19 : 0.9049\n",
      "Test: ROC AUC score at epoch 19 : 0.9485\n",
      "\n",
      "Training: Loss at epoch 20 : 0.17030000686645508\n",
      "Test: Accuracy at epoch 20 : 0.9057\n",
      "Test: ROC AUC score at epoch 20 : 0.9488\n",
      "\n",
      "Training: Loss at epoch 21 : 0.16300000250339508\n",
      "Test: Accuracy at epoch 21 : 0.9009\n",
      "Test: ROC AUC score at epoch 21 : 0.9498\n",
      "\n",
      "Training: Loss at epoch 22 : 0.16040000319480896\n",
      "Test: Accuracy at epoch 22 : 0.9009\n",
      "Test: ROC AUC score at epoch 22 : 0.9497\n",
      "\n",
      "Training: Loss at epoch 23 : 0.1412999927997589\n",
      "Test: Accuracy at epoch 23 : 0.8944\n",
      "Test: ROC AUC score at epoch 23 : 0.9479\n",
      "\n",
      "Training: Loss at epoch 24 : 0.13570000231266022\n",
      "Test: Accuracy at epoch 24 : 0.8936\n",
      "Test: ROC AUC score at epoch 24 : 0.9461\n",
      "\n",
      "Training: Loss at epoch 25 : 0.1324000060558319\n",
      "Test: Accuracy at epoch 25 : 0.8977\n",
      "Test: ROC AUC score at epoch 25 : 0.9479\n",
      "\n",
      "Training: Loss at epoch 26 : 0.12720000743865967\n",
      "Test: Accuracy at epoch 26 : 0.8993\n",
      "Test: ROC AUC score at epoch 26 : 0.9477\n",
      "\n",
      "Training: Loss at epoch 27 : 0.11999999731779099\n",
      "Test: Accuracy at epoch 27 : 0.8952\n",
      "Test: ROC AUC score at epoch 27 : 0.9441\n",
      "\n",
      "Training: Loss at epoch 28 : 0.10920000076293945\n",
      "Test: Accuracy at epoch 28 : 0.9025\n",
      "Test: ROC AUC score at epoch 28 : 0.9415\n",
      "\n",
      "Training: Loss at epoch 29 : 0.11890000104904175\n",
      "Test: Accuracy at epoch 29 : 0.8944\n",
      "Test: ROC AUC score at epoch 29 : 0.944\n",
      "\n",
      "Training: Loss at epoch 30 : 0.09440000355243683\n",
      "Test: Accuracy at epoch 30 : 0.8799\n",
      "Test: ROC AUC score at epoch 30 : 0.9394\n",
      "\n",
      "Training: Loss at epoch 31 : 0.08579999953508377\n",
      "Test: Accuracy at epoch 31 : 0.8936\n",
      "Test: ROC AUC score at epoch 31 : 0.9425\n",
      "\n",
      "Training: Loss at epoch 32 : 0.09939999878406525\n",
      "Test: Accuracy at epoch 32 : 0.8904\n",
      "Test: ROC AUC score at epoch 32 : 0.9357\n",
      "\n",
      "Training: Loss at epoch 33 : 0.08389999717473984\n",
      "Test: Accuracy at epoch 33 : 0.8888\n",
      "Test: ROC AUC score at epoch 33 : 0.9409\n",
      "\n",
      "Training: Loss at epoch 34 : 0.08789999783039093\n",
      "Test: Accuracy at epoch 34 : 0.8864\n",
      "Test: ROC AUC score at epoch 34 : 0.9361\n",
      "\n",
      "Training: Loss at epoch 35 : 0.07339999824762344\n",
      "Test: Accuracy at epoch 35 : 0.8928\n",
      "Test: ROC AUC score at epoch 35 : 0.9351\n",
      "\n",
      "Training: Loss at epoch 36 : 0.06599999964237213\n",
      "Test: Accuracy at epoch 36 : 0.8936\n",
      "Test: ROC AUC score at epoch 36 : 0.9382\n",
      "\n",
      "Training: Loss at epoch 37 : 0.07370000332593918\n",
      "Test: Accuracy at epoch 37 : 0.8695\n",
      "Test: ROC AUC score at epoch 37 : 0.9278\n",
      "\n",
      "Training: Loss at epoch 38 : 0.06830000132322311\n",
      "Test: Accuracy at epoch 38 : 0.8904\n",
      "Test: ROC AUC score at epoch 38 : 0.94\n",
      "\n",
      "Training: Loss at epoch 39 : 0.05959999933838844\n",
      "Test: Accuracy at epoch 39 : 0.8904\n",
      "Test: ROC AUC score at epoch 39 : 0.942\n",
      "\n",
      "Training: Loss at epoch 40 : 0.04780000075697899\n",
      "Test: Accuracy at epoch 40 : 0.8872\n",
      "Test: ROC AUC score at epoch 40 : 0.9444\n",
      "\n",
      "Training: Loss at epoch 41 : 0.04809999838471413\n",
      "Test: Accuracy at epoch 41 : 0.9001\n",
      "Test: ROC AUC score at epoch 41 : 0.946\n",
      "\n",
      "Training: Loss at epoch 42 : 0.038100000470876694\n",
      "Test: Accuracy at epoch 42 : 0.8783\n",
      "Test: ROC AUC score at epoch 42 : 0.937\n",
      "\n",
      "Training: Loss at epoch 43 : 0.04619999974966049\n",
      "Test: Accuracy at epoch 43 : 0.8815\n",
      "Test: ROC AUC score at epoch 43 : 0.9418\n",
      "\n",
      "Training: Loss at epoch 44 : 0.03739999979734421\n",
      "Test: Accuracy at epoch 44 : 0.884\n",
      "Test: ROC AUC score at epoch 44 : 0.9431\n",
      "\n",
      "Training: Loss at epoch 45 : 0.03240000084042549\n",
      "Test: Accuracy at epoch 45 : 0.8848\n",
      "Test: ROC AUC score at epoch 45 : 0.9454\n",
      "\n",
      "Training: Loss at epoch 46 : 0.03709999844431877\n",
      "Test: Accuracy at epoch 46 : 0.8888\n",
      "Test: ROC AUC score at epoch 46 : 0.9411\n",
      "\n",
      "Training: Loss at epoch 47 : 0.03660000115633011\n",
      "Test: Accuracy at epoch 47 : 0.8936\n",
      "Test: ROC AUC score at epoch 47 : 0.9446\n",
      "\n",
      "Training: Loss at epoch 48 : 0.03440000116825104\n",
      "Test: Accuracy at epoch 48 : 0.8791\n",
      "Test: ROC AUC score at epoch 48 : 0.9392\n",
      "\n",
      "Training: Loss at epoch 49 : 0.039400000125169754\n",
      "Test: Accuracy at epoch 49 : 0.8896\n",
      "Test: ROC AUC score at epoch 49 : 0.9428\n",
      "\n",
      "Training: Loss at epoch 50 : 0.04309999942779541\n",
      "Test: Accuracy at epoch 50 : 0.8711\n",
      "Test: ROC AUC score at epoch 50 : 0.9358\n",
      "\n",
      "Training: Loss after 50 epochs: 0.16222748160362244\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data and then train and evaluate the RNN (without cross-validation)\n",
    "# X_train, y_train, X_test, y_test = preprocess_data()\n",
    "# train_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training and evaluation, our models achieved the following results:\n",
    "1) RNN \\\n",
    "Accuracy: 0.8952 \\\n",
    "ROC AUC: 0.9411\n",
    "\n",
    "\n",
    "2) DNABERT \\\n",
    "Accuracy: 0.9259 \\\n",
    "ROC AUC: 0.9692 \n",
    "\n",
    "\n",
    "Thus, DNABERT performed better for predicting the binding sites of this protein, outperforming the RNN approach by 2% in accuracy and nearly 3% in ROC AUC. \n",
    "\n",
    "BONUS: The current maximum sequence length is set to 100nt. Try increasing this gradually and compare the difference in performance of the two models again. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
