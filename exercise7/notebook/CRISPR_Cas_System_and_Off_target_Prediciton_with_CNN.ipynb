{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nn770wkP-SfL"
   },
   "source": [
    "# CRISPR-Cas System and Off-target Prediciton with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgCCkSUO-Wgf"
   },
   "source": [
    "## Install necessary libraries\n",
    "This cell installs specific versions of the following Python libraries:\n",
    "- scikit-learn: For machine learning tasks like clustering and evaluation.\n",
    "- torch and torchvision: For building and training neural networks (CNN).\n",
    "- matplotlib: For data visualization and plotting.\n",
    "- requests: For handling HTTP requests, e.g., downloading datasets.\n",
    "\n",
    "The `-U` flag ensures that the latest compatible version is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImBrjfGI-OAe",
    "outputId": "bd95115a-0986-4bf4-df43-22bef4a7f9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.3.2 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: torch==2.5.1 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from scikit-learn==1.3.2) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from scikit-learn==1.3.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from scikit-learn==1.3.2) (3.5.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from torchvision==0.20.1) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from matplotlib==3.9.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from matplotlib==3.9.2) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from matplotlib==3.9.2) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from matplotlib==3.9.2) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from matplotlib==3.9.2) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/data/miniconda3/envs/crisprcas/lib/python3.10/site-packages (from jinja2->torch==2.5.1) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn==1.3.2 torch==2.5.1 torchvision==0.20.1 matplotlib==3.9.2 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rN3iSDnJ-L6w"
   },
   "outputs": [],
   "source": [
    "# For file handling, data fetching, and processing\n",
    "import os\n",
    "from requests import get\n",
    "\n",
    "# For numerical operations\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For building and training deep learning models\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For evaluation and data splitting\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHjltGsi-dZ4"
   },
   "source": [
    "## Configurable Constants\n",
    "Define key constants for the notebook, such as file paths, model parameters, or dataset configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2ZoAE71r-edn"
   },
   "outputs": [],
   "source": [
    "# DATASET_FILE_URL: Link to the HEK cell dataset.\n",
    "DATASET_FILE_URL = \"https://github.com/BackofenLab/MLLS-exercise-SS22/raw/refs/heads/main/week_6/data/hg19_samples_hek.csv\"\n",
    "\n",
    "# DATASET_FILE_K562_URL: Link to the K562 cell dataset.\n",
    "DATASET_FILE_K562_URL = \"https://github.com/BackofenLab/MLLS-exercise-SS22/raw/refs/heads/main/week_6/data/hg19_samples_k562.csv\"\n",
    "\n",
    "# FOLD_NUM: Number of folds for k-fold cross-validation.\n",
    "FOLD_NUM = 3\n",
    "\n",
    "# EPOCH_NUM: Number of epochs for training the model.\n",
    "EPOCH_NUM = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaUqXL7w-6aJ"
   },
   "source": [
    "## Setup GPU\n",
    "Check for GPU availability and ensures that the computations are optimized for CUDA if a compatible GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KGJXBSx1-7iz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEhQJB1Y-9VH"
   },
   "source": [
    "## Download dataset files\n",
    "\n",
    "Checks if the required datasets (HEK and K562) are already present in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzJZ3u_k_EE5",
    "outputId": "7927f99d-f6ac-4c60-a0cc-73fcdefe240f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hg19_samples_hek file downloaded\n",
      "hg19_samples_k562 file downloaded\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATASET_FILE_URL):\n",
    "  open(\"hg19_samples_hek.csv\", \"wb\").write(get(DATASET_FILE_URL).content)\n",
    "  print(\"hg19_samples_hek file downloaded\")\n",
    "else:\n",
    "  print(\"hg19_samples_hek file already exists\")\n",
    "\n",
    "if not os.path.exists(DATASET_FILE_K562_URL):\n",
    "  open(\"hg19_samples_k562.csv\", \"wb\").write(get(DATASET_FILE_K562_URL).content)\n",
    "  print(\"hg19_samples_k562 file downloaded\")\n",
    "else:\n",
    "  print(\"hg19_samples_k562 file already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5AexmtO_jsJ"
   },
   "source": [
    "## Dataset preparation\n",
    "Loading, preprocessing, and preparing the scRNA-seq dataset for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "t5R34jKy_kYG"
   },
   "outputs": [],
   "source": [
    "# Define a custom Dataset class for loading and preprocessing the data.\n",
    "class MyDataset(Dataset):\n",
    "    # Initialize the dataset by loading, encoding, and combining HEK and K562 data.\n",
    "    def __init__(self, dataset_file, dataset_file_k562):\n",
    "        # Load HEK dataset and encode its sequences\n",
    "        self.dataset_hek = pd.read_csv(dataset_file)\n",
    "        self.x_value_hek = self.encode_data(np.array(self.dataset_hek[\"Guide-Seq\"]), np.array(self.dataset_hek[\"false_seq\"]))\n",
    "        self.guide_hot_one_hek, self.target_hot_one_hek = self.hot_one_encoding(np.array(self.dataset_hek[\"Guide-Seq\"]), np.array(self.dataset_hek[\"false_seq\"]))\n",
    "        self.y_value_hek = self.dataset_hek[\"label\"]\n",
    "        self.y_value_class_hek = self.y_value_hek\n",
    "\n",
    "        # Load K562 dataset and encode its sequences\n",
    "        self.dataset_k562 = pd.read_csv(dataset_file_k562)\n",
    "        self.x_value_k562 = self.encode_data(np.array(self.dataset_k562[\"Guide-Seq\"]), np.array(self.dataset_k562[\"false_seq\"]))\n",
    "        self.guide_hot_one_k562, self.target_hot_one_k562 = self.hot_one_encoding(np.array(self.dataset_k562[\"Guide-Seq\"]), np.array(self.dataset_k562[\"false_seq\"]))\n",
    "        self.y_value_k562 = self.dataset_k562[\"label\"]\n",
    "        self.y_value_cass_k562 = [y + 2 for y in self.y_value_k562]\n",
    "\n",
    "        # Combine HEK and K562 datasets\n",
    "        self.full_dataset = []\n",
    "        self.full_dataset.extend(self.dataset_hek)\n",
    "        self.full_dataset.extend(self.dataset_k562)\n",
    "\n",
    "        # Combine all features and labels from HEK and K562 datasets\n",
    "        self.full_x_value = []\n",
    "        self.full_x_value.extend(self.x_value_hek)\n",
    "        self.full_x_value.extend(self.x_value_k562)\n",
    "\n",
    "        self.full_y_value = []\n",
    "        self.full_y_value.extend(self.y_value_hek)\n",
    "        self.full_y_value.extend(self.y_value_k562)\n",
    "\n",
    "        self.full_y_value_class = []\n",
    "        self.full_y_value_class.extend(self.y_value_class_hek)\n",
    "        self.full_y_value_class.extend(self.y_value_cass_k562)\n",
    "\n",
    "        # Combine hot-one encoded sequences for guide and target\n",
    "        self.full_guide_hot_one = []\n",
    "        self.full_guide_hot_one.extend(self.guide_hot_one_hek)\n",
    "        self.full_guide_hot_one.extend(self.guide_hot_one_k562)\n",
    "\n",
    "        self.full_target_hot_one = []\n",
    "        self.full_target_hot_one.extend(self.target_hot_one_hek)\n",
    "        self.full_target_hot_one.extend(self.target_hot_one_k562)\n",
    "\n",
    "        # Shuffle the combined dataset\n",
    "        self.shuffle_all()\n",
    "\n",
    "    def shuffle_all(self):\n",
    "        # Shuffle all data to randomize the dataset\n",
    "        shuffle_ind = list(range(len(self.full_y_value)))\n",
    "        random.shuffle(shuffle_ind)\n",
    "\n",
    "        # self.full_dataset = np.array(self.full_dataset)[shuffle_ind].tolist()\n",
    "\n",
    "        # Shuffle features, labels, and hot-one encoded sequences\n",
    "        self.full_x_value = torch.FloatTensor(np.array(self.full_x_value)[shuffle_ind].tolist())\n",
    "        self.full_y_value = torch.LongTensor(np.array(self.full_y_value)[shuffle_ind].tolist())\n",
    "        self.full_y_value_class = np.array(self.full_y_value_class)[shuffle_ind].tolist()\n",
    "        self.full_guide_hot_one = np.array(self.full_guide_hot_one)[shuffle_ind].tolist()\n",
    "        self.full_target_hot_one = np.array(self.full_target_hot_one)[shuffle_ind].tolist()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples.\n",
    "        return len(self.full_x_value)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a specific sample and its corresponding label.\n",
    "        return (self.full_x_value[idx], self.full_guide_hot_one[idx], self.full_target_hot_one[idx]), self.full_y_value[idx]\n",
    "\n",
    "    def return_class_y(self, indeces=[]):\n",
    "        # Return class-adjusted labels for specified indices or the entire dataset.\n",
    "        y_val = self.full_y_value_class\n",
    "\n",
    "        if len(indeces) > 0:\n",
    "            y_val = list(np.array(y_val)[indeces])\n",
    "\n",
    "        return y_val\n",
    "\n",
    "    def seperate_by_class(self, indeces):\n",
    "        # Separate indices by class: HEK (0, 1) and K562 (2, 3).\n",
    "        y_class_val = self.return_class_y(indeces=indeces)\n",
    "\n",
    "        indeces_hek = [indeces[en] for en, z in enumerate(y_class_val) if z == 0 or z == 1]\n",
    "        indeces_k562 = [indeces[en] for en, z in enumerate(y_class_val) if z == 2 or z == 3]\n",
    "\n",
    "        return indeces_hek, indeces_k562\n",
    "\n",
    "    def return_x(self):\n",
    "        return self.full_x_value.tolist()\n",
    "\n",
    "    def return_y(self, indeces=[]):\n",
    "        y_val = self.full_y_value.tolist()\n",
    "\n",
    "        if len(indeces) > 0:\n",
    "            y_val = list(np.array(y_val)[indeces])\n",
    "\n",
    "        return y_val\n",
    "\n",
    "    def hot_one(self, seq):\n",
    "        # Perform hot-one encoding for a single sequence.\n",
    "        hot_one_dict = {\"A\": 0, \"T\": 1, \"G\": 2, \"C\": 3}\n",
    "        hot_one = [[0, 0, 0, 0] for s in seq]\n",
    "        for enum, s in enumerate(seq):\n",
    "            hot_one[enum][hot_one_dict[s]] = 1\n",
    "\n",
    "        return hot_one\n",
    "\n",
    "    def hot_one_encoding(self, guide_seqs, seqs):\n",
    "        # Perform hot-one encoding for guide and target sequences.\n",
    "        hot_ones_target = []\n",
    "        hot_ones_guide = []\n",
    "\n",
    "        for en, guide in enumerate(guide_seqs):\n",
    "            guide_hot_one = self.hot_one(guide)\n",
    "            target_hot_one = self.hot_one(seqs[en])\n",
    "\n",
    "            hot_ones_guide.append(guide_hot_one)\n",
    "            hot_ones_target.append(target_hot_one)\n",
    "\n",
    "        return torch.tensor(hot_ones_guide), torch.tensor(hot_ones_target)\n",
    "\n",
    "    def encode_data(self, guide_seqs, seqs):\n",
    "        # Encode sequences based on mismatches between guide and target sequences.\n",
    "        encoded_seqs = []\n",
    "\n",
    "        for seq_num, seq in enumerate(seqs):\n",
    "            encoded_seqs.append([float(1) if nuc1 != guide_seqs[seq_num][en] else float(0) for en, nuc1 in enumerate(seq)])\n",
    "\n",
    "        return encoded_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iJ8QujlqCKhj"
   },
   "outputs": [],
   "source": [
    "# Initialize the dataset with HEK and K562 samples\n",
    "dataset = MyDataset(\"./hg19_samples_hek.csv\", \"./hg19_samples_k562.csv\")\n",
    "\n",
    "# Create indices for splitting the dataset into training and validation sets\n",
    "indices = list(range(len(dataset)))  # Generate a list of all indices in the dataset\n",
    "split = int(np.floor(0.3 * len(dataset)))  # Calculate the size of the validation set (30% of the dataset)\n",
    "np.random.shuffle(indices)  # Shuffle the indices to randomize the dataset\n",
    "\n",
    "# Initialize lists to store fold results and evaluation metrics\n",
    "fold_res = []  # Stores results for each fold\n",
    "average_precision_score_folds = []  # Stores average precision scores across folds\n",
    "\n",
    "# Set up Stratified K-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=FOLD_NUM, shuffle=False, random_state=None)\n",
    "\n",
    "\n",
    "# Initialize lists to store AUPRC (Area Under Precision-Recall Curve) for HEK and K562 samples\n",
    "# These lists are split based on three cross-validation folds\n",
    "cv_list1_auprc_k562 = []  # Fold 1: AUPRC for K562 samples\n",
    "cv_list2_auprc_k562 = []  # Fold 2: AUPRC for K562 samples\n",
    "cv_list3_auprc_k562 = []  # Fold 3: AUPRC for K562 samples\n",
    "\n",
    "cv_list1_auprc_hek = []  # Fold 1: AUPRC for HEK samples\n",
    "cv_list2_auprc_hek = []  # Fold 2: AUPRC for HEK samples\n",
    "cv_list3_auprc_hek = []  # Fold 3: AUPRC for HEK samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MfvaW2e6Dzb7"
   },
   "outputs": [],
   "source": [
    "def get_oversample_mask(indeces, dataset):\n",
    "    \"\"\"\n",
    "      Generate an oversampled mask for imbalanced datasets.\n",
    "\n",
    "      Args:\n",
    "          indeces (list): List of sample indices to consider.\n",
    "          dataset (MyDataset): The dataset object containing labels and data.\n",
    "\n",
    "      Returns:\n",
    "          list: A shuffled list of indices with oversampled positive samples.\n",
    "    \"\"\"\n",
    "    full_list = []  # Initialize a list to store the oversampled indices\n",
    "\n",
    "    y_val = dataset.return_y(indeces)  # Get the labels for the provided indices\n",
    "\n",
    "    # Separate positive and negative sample indices\n",
    "    pos_indeces = indeces[np.array([en for en, y in enumerate(y_val) if y == 1])]\n",
    "    neg_indeces = indeces[np.array([en for en, y in enumerate(y_val) if y == 0])]\n",
    "\n",
    "    # Calculate the number of negative samples (majority class)\n",
    "    num_to_sample = len(neg_indeces)\n",
    "\n",
    "    # Oversample positive samples to balance the dataset\n",
    "    y_samples = []  # List to store the oversampled positive indices\n",
    "\n",
    "    for i in range(math.ceil(num_to_sample / len(pos_indeces))): # Repeat positive indices until reaching the required number\n",
    "        y_samples.extend(pos_indeces)\n",
    "\n",
    "    y_samples = y_samples[:num_to_sample]  # Trim the oversampled positives to match the size of the negative samples\n",
    "\n",
    "    # Combine oversampled positives and negatives\n",
    "    full_list.extend(y_samples)  # Add oversampled positive indices\n",
    "    full_list.extend(neg_indeces)  # Add all negative indices\n",
    "\n",
    "    # Shuffle the combined list of indices\n",
    "    random.shuffle(full_list)\n",
    "\n",
    "    return full_list  # Return the shuffled oversampled indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "exQl7OVQE1lN"
   },
   "outputs": [],
   "source": [
    "class cnn_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn_net, self).__init__()\n",
    "\n",
    "        # Define the first convolutional layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=3)  # Input: single channel, Output: 6 feature maps\n",
    "\n",
    "        # Define the second convolutional layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, kernel_size=3)  # Input: 6 feature maps, Output: 16 feature maps\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(304, 60)  # Fully connected layer 1: Reduce features to 60\n",
    "        self.fc2 = nn.Linear(60, 20)  # Fully connected layer 2: Reduce features to 20\n",
    "        self.fc3 = nn.Linear(20, 1)  # Fully connected layer 3: Output a single value (binary classification)\n",
    "\n",
    "        # Softmax layer (not used in this forward pass but available for multi-class classification)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            input_ (tuple): A tuple of input features, guide sequences, and target sequences.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The sigmoid-activated output for binary classification.\n",
    "        \"\"\"\n",
    "        x, guide, target = input_[0], input_[1], input_[2]  # Extract input components\n",
    "\n",
    "        # Add a channel dimension for convolutional layers\n",
    "        x = x.unsqueeze(1)  # Shape: (batch_size, 1, sequence_length)\n",
    "\n",
    "        # Apply the first convolutional layer followed by ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        # x = F.max_pool1d(x, 2)\n",
    "        # x = F.max_pool1d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        # Apply the second convolutional layer followed by ReLU activation\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        # Flatten the output from the convolutional layers\n",
    "        x = torch.flatten(x, 1)  # Shape: (batch_size, flattened_features)\n",
    "\n",
    "        # Apply the first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Apply the second fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Apply the final fully connected layer for binary classification\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # Apply sigmoid activation to produce probabilities for binary classification\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "V0nZXXXtFbhQ"
   },
   "outputs": [],
   "source": [
    "class siamese_cnn_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(siamese_cnn_net, self).__init__()\n",
    "\n",
    "        # Define convolutional layers for the first head (mismatch features)\n",
    "        self.conv1_head1 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=3)  # Input: 1 channel, Output: 6 channels\n",
    "        self.conv2_head1 = nn.Conv1d(in_channels=6, out_channels=16, kernel_size=3)  # Input: 6 channels, Output: 16 channels\n",
    "\n",
    "        # Define convolutional layers for the second head (target sequences)\n",
    "        self.conv1_head2 = nn.Conv1d(in_channels=4, out_channels=6, kernel_size=3)  # Input: 4 channels, Output: 6 channels\n",
    "        self.conv2_head2 = nn.Conv1d(in_channels=6, out_channels=16, kernel_size=3)  # Input: 6 channels, Output: 16 channels\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(608, 60)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(60, 20)   # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(20, 1)   # Output layer for binary classification\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Forward pass for the Siamese CNN.\n",
    "\n",
    "        Args:\n",
    "            input_ (tuple): A tuple containing mismatches, guide, and target sequences.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output probabilities for binary classification.\n",
    "        \"\"\"\n",
    "        mismatches, guide, target = input_[0], input_[1], input_[2]  # Extract inputs\n",
    "\n",
    "        ### head one\n",
    "        # guide = guide.permute(0, 2, 1)\n",
    "        # guide = F.relu(self.conv1_head1(guide))\n",
    "        # guide = F.max_pool1d(guide, 2)\n",
    "        # guide = F.max_pool1d(F.relu(self.conv2_head1(guide)), 2)\n",
    "        # guide = F.relu(self.conv2_head1(guide))\n",
    "\n",
    "        ### Head One (mismatch features)\n",
    "        mismatches = mismatches.unsqueeze(1)  # Add channel dimension for convolution\n",
    "        mismatches = F.relu(self.conv1_head1(mismatches))  # Apply first convolution + ReLU\n",
    "        mismatches = F.relu(self.conv2_head1(mismatches))  # Apply second convolution + ReLU\n",
    "\n",
    "        ### head two\n",
    "\n",
    "        ### Head Two (target sequences)\n",
    "        target = torch.stack([torch.stack(row) for row in target])\n",
    "        target = target.float()\n",
    "        target = torch.permute(target, (2, 1, 0))  # Rearrange dimensions for convolution (batch_size, channels, length)\n",
    "        target = F.relu(self.conv1_head2(target))  # Apply first convolution + ReLU\n",
    "        target = F.relu(self.conv2_head2(target))  # Apply second convolution + ReLU\n",
    "\n",
    "        # target = F.max_pool1d(target, 2)\n",
    "        # target = F.max_pool1d(F.relu(self.conv2_head2(target)), 2)\n",
    "\n",
    "        ### Combine Outputs\n",
    "        x = torch.cat([mismatches, target], dim=1)  # Concatenate outputs along the channel dimension\n",
    "\n",
    "        ### Fully Connected Layers\n",
    "        x = torch.flatten(x, 1)  # Flatten the combined feature maps for the fully connected layers\n",
    "        x = F.relu(self.fc1(x))  # Apply first fully connected layer + ReLU\n",
    "        x = F.relu(self.fc2(x))  # Apply second fully connected layer + ReLU\n",
    "        x = self.fc3(x)          # Apply final fully connected layer for classification\n",
    "\n",
    "        return torch.sigmoid(x)  # Sigmoid activation to output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Ee863AjtFg6F"
   },
   "outputs": [],
   "source": [
    "class sequence_cnn_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sequence_cnn_net, self).__init__()\n",
    "\n",
    "        # Define convolutional layers for the guide sequence (Head 1)\n",
    "        self.conv1_head1 = nn.Conv1d(in_channels=4, out_channels=6, kernel_size=3)  # Input: 4 channels (A, T, G, C), Output: 6 feature maps\n",
    "        self.conv2_head1 = nn.Conv1d(in_channels=6, out_channels=16, kernel_size=3)  # Input: 6 feature maps, Output: 16 feature maps\n",
    "\n",
    "        # Define convolutional layers for the target sequence (Head 2)\n",
    "        self.conv1_head2 = nn.Conv1d(in_channels=4, out_channels=6, kernel_size=3)  # Input: 4 channels (A, T, G, C), Output: 6 feature maps\n",
    "        self.conv2_head2 = nn.Conv1d(in_channels=6, out_channels=16, kernel_size=3)  # Input: 6 feature maps, Output: 16 feature maps\n",
    "\n",
    "        # Define convolutional layers for mismatch features (Head 3)\n",
    "        self.conv1_head3 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=3)  # Input: 1 channel, Output: 6 feature maps\n",
    "        self.conv2_head3 = nn.Conv1d(in_channels=6, out_channels=16, kernel_size=3)  # Input: 6 feature maps, Output: 16 feature maps\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(912, 60)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(60, 20)   # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(20, 1)   # Output layer for binary classification\n",
    "\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Forward pass for the Sequence CNN.\n",
    "\n",
    "        Args:\n",
    "            input_ (tuple): A tuple containing mismatches, guide, and target sequences.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output probabilities for binary classification.\n",
    "        \"\"\"\n",
    "        mismatches, guide, target = input_[0], input_[1], input_[2]  # Extract inputs\n",
    "\n",
    "        ### Head One (Guide Sequence)\n",
    "        guide = torch.stack([torch.stack(row) for row in guide])\n",
    "        guide = guide.float()\n",
    "        guide = torch.permute(guide, (2, 1, 0))  # Rearrange dimensions for convolution (batch_size, channels, length)\n",
    "        guide = F.relu(self.conv1_head1(guide))  # Apply first convolution + ReLU\n",
    "        guide = F.relu(self.conv2_head1(guide))  # Apply second convolution + ReLU\n",
    "\n",
    "        # guide = F.max_pool1d(guide, 2)\n",
    "        # guide = F.max_pool1d(F.relu(self.conv2_head1(guide)), 2)\n",
    "\n",
    "        ### Head Two (Target Sequence)\n",
    "        target = torch.stack([torch.stack(row) for row in target])\n",
    "        target = target.float()\n",
    "        target = torch.permute(target, (2, 1, 0))  # Rearrange dimensions for convolution (batch_size, channels, length)\n",
    "        target = F.relu(self.conv1_head2(target))  # Apply first convolution + ReLU\n",
    "        target = F.relu(self.conv2_head2(target))  # Apply second convolution + ReLU\n",
    "\n",
    "        # target = F.max_pool1d(target, 2)\n",
    "        # target = F.max_pool1d(F.relu(self.conv2_head2(target)), 2)\n",
    "\n",
    "        ### Head Three (Mismatch Features)\n",
    "        mismatches = mismatches.unsqueeze(1)  # Add channel dimension for convolution\n",
    "        mismatches = F.relu(self.conv1_head3(mismatches))  # Apply first convolution + ReLU\n",
    "        mismatches = F.relu(self.conv2_head3(mismatches))  # Apply second convolution + ReLU\n",
    "\n",
    "        # mismatches = F.max_pool1d(mismatches, 2)\n",
    "        # mismatches = F.max_pool1d(F.relu(self.conv2_head3(mismatches)), 2)\n",
    "\n",
    "        ### Combine Outputs\n",
    "        x = torch.cat([guide, target, mismatches], dim=1)  # Concatenate outputs along the channel dimension\n",
    "\n",
    "        ### Fully Connected Layers\n",
    "        x = torch.flatten(x, 1)  # Flatten the combined feature maps for the fully connected layers\n",
    "        x = F.relu(self.fc1(x))  # Apply first fully connected layer + ReLU\n",
    "        x = F.relu(self.fc2(x))  # Apply second fully connected layer + ReLU\n",
    "        x = self.fc3(x)          # Apply final fully connected layer for classification\n",
    "\n",
    "        return torch.sigmoid(x)  # Sigmoid activation to output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "R5fpvG7TFoNQ"
   },
   "outputs": [],
   "source": [
    "def train(trainloader, net, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training loop for a single epoch.\n",
    "\n",
    "    Args:\n",
    "        trainloader (DataLoader): DataLoader for the training dataset.\n",
    "        net (nn.Module): The neural network model.\n",
    "        criterion (nn.Module): The loss function (e.g., BCEWithLogitsLoss or MSELoss).\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        epoch (int): Current epoch number (for logging or scheduling purposes).\n",
    "\n",
    "    Returns:\n",
    "        net (nn.Module): The updated model after one epoch of training.\n",
    "        running_loss (float): The cumulative loss for this epoch.\n",
    "    \"\"\"\n",
    "    running_loss = 0.0  # Initialize the cumulative loss for the epoch\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # Unpack the batch: inputs (features) and labels (ground truth)\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Reset gradients for the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute model outputs\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Reshape labels to match the output dimension for loss computation\n",
    "        labels = labels.view(-1).float()\n",
    "\n",
    "        # Compute the loss between predictions and ground truth\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters using the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for reporting\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Return the updated model and the total loss for the epoch\n",
    "    return net, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sK9SKnayG3d7"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, net, criterion, epoch):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation/test dataset.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for the validation/test dataset.\n",
    "        net (nn.Module): The neural network model to be evaluated.\n",
    "        criterion (nn.Module): The loss function (e.g., BCEWithLogitsLoss).\n",
    "        epoch (int): Current epoch number (for logging purposes).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - y_values (list): Ground truth labels.\n",
    "            - predicted_list (list): Model predictions (probabilities or binary labels).\n",
    "            - running_loss (float): Cumulative loss over the evaluation dataset.\n",
    "            - mean_eval (float): Mean average precision score (AUPRC) across the dataset.\n",
    "            - mean_eval_auroc (float): Mean Area Under ROC Curve (AUROC) score across the dataset.\n",
    "    \"\"\"\n",
    "    correct = 0  # Track the number of correct predictions\n",
    "    y_values = []  # Store ground truth labels\n",
    "    running_loss = 0.0  # Cumulative loss for the evaluation phase\n",
    "    predicted_list = []  # Store predicted probabilities or binary labels\n",
    "    evals = []  # Store Average Precision Score (AUPRC) for each batch\n",
    "    evals_auroc = []  # Store AUROC score for each batch\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            # Unpack the batch: inputs (features) and labels (ground truth)\n",
    "            inputs, labels = data\n",
    "            labels_save = labels  # Save original labels for comparison later\n",
    "\n",
    "            # Forward pass: compute model outputs\n",
    "            outputs = net(inputs)\n",
    "            labels = labels.view(-1).float()  # Reshape labels for loss computation\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            # _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Generate binary predictions (0 or 1 based on 0.5 threshold)\n",
    "            predicted = [0 if p < 0.5 else 1 for p in outputs.data]\n",
    "\n",
    "            # predicted_list.extend(outputs.data)\n",
    "\n",
    "            # Append raw outputs (probabilities) to the predicted list\n",
    "            predicted_list.extend(outputs.data.tolist())\n",
    "\n",
    "            # Append ground truth labels to the y_values list\n",
    "            y_values.extend(labels.tolist())\n",
    "\n",
    "            # correct += (predicted == labels_save).sum().item()\n",
    "\n",
    "            # Accumulate the total loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Compute AUPRC (Average Precision Score) for the batch\n",
    "            eval_ = average_precision_score(labels.tolist(), outputs.data.tolist())\n",
    "            evals.append(eval_)\n",
    "\n",
    "            # Compute AUROC (Area Under ROC Curve) for the batch\n",
    "            eval_auroc = roc_auc_score(labels.tolist(), outputs.data.tolist())\n",
    "            evals_auroc.append(eval_auroc)\n",
    "\n",
    "    # Return evaluation metrics\n",
    "    return y_values, predicted_list, running_loss, np.mean(evals), np.mean(evals_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "JCrxjFryEmhj",
    "outputId": "2fe0f788-29cd-402d-b501-d9bbedfba5c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 0\n",
      "  Epoch 0\n",
      "cnn-net: AUPRC in epoch 0: 0.49401857335785754,  AUROC: 0.9846061725929557, val-Loss:  0.092, train-loss: 0.185\n",
      "siamese-net: AUPRC in epoch 0: 0.5834666612529305,  AUROC: 0.9935300671670746, val-Loss:  0.059, train-loss: 0.150\n",
      "sequence_cnn_net: AUPRC in epoch 0: 0.5608657377730834, AUROC: 0.9885253165876249, val-Loss:  0.076, train-loss: 0.140\n",
      "##############################################################################################\n",
      "  Epoch 1\n",
      "cnn-net: AUPRC in epoch 1: 0.539141030618665,  AUROC: 0.9864976630376678, val-Loss:  0.146, train-loss: 0.094\n",
      "siamese-net: AUPRC in epoch 1: 0.6191272408191139,  AUROC: 0.9944129420173792, val-Loss:  0.061, train-loss: 0.050\n",
      "sequence_cnn_net: AUPRC in epoch 1: 0.6170793367622649, AUROC: 0.9898657667125923, val-Loss:  0.046, train-loss: 0.049\n",
      "##############################################################################################\n",
      "  Epoch 2\n",
      "cnn-net: AUPRC in epoch 2: 0.5594194152373086,  AUROC: 0.9864637063126561, val-Loss:  0.089, train-loss: 0.074\n",
      "siamese-net: AUPRC in epoch 2: 0.649590015856504,  AUROC: 0.9949414565888486, val-Loss:  0.050, train-loss: 0.033\n",
      "sequence_cnn_net: AUPRC in epoch 2: 0.6373529943692154, AUROC: 0.9903283850652272, val-Loss:  0.040, train-loss: 0.033\n",
      "##############################################################################################\n",
      "  Epoch 3\n",
      "cnn-net: AUPRC in epoch 3: 0.5655959778881013,  AUROC: 0.9871149357814452, val-Loss:  0.068, train-loss: 0.061\n",
      "siamese-net: AUPRC in epoch 3: 0.6653624515135051,  AUROC: 0.9949185946353754, val-Loss:  0.028, train-loss: 0.024\n",
      "sequence_cnn_net: AUPRC in epoch 3: 0.6541585102621087, AUROC: 0.9908162188077223, val-Loss:  0.039, train-loss: 0.025\n",
      "##############################################################################################\n",
      "END OF TRAINING\n",
      "#######################################################################\n",
      "TEST ON HEK CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 0: 0.45086390863986875 , Test-Loss:  0.061\n",
      "siamese-net: AUPRC in fold 0:  0.5809090614524961, Test-Loss:  0.028\n",
      "sequence_cnn_net: AUPRC in fold 0:  0.6325240940815473, Test-Loss:  0.039\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON K562 CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 0: 0.575605201657355 , Test-Loss:  0.131\n",
      "siamese-net: AUPRC in fold 0:  0.608561650883543, Test-Loss:  0.041\n",
      "sequence_cnn_net: AUPRC in fold 0:  0.6577752123865971, Test-Loss:  0.045\n",
      "#######################################################################\n",
      "Processing fold 1\n",
      "  Epoch 0\n",
      "cnn-net: AUPRC in epoch 0: 0.44374610895250505,  AUROC: 0.9792905875039765, val-Loss:  0.121, train-loss: 0.187\n",
      "siamese-net: AUPRC in epoch 0: 0.43101352203483273,  AUROC: 0.9885303107612652, val-Loss:  0.107, train-loss: 0.157\n",
      "sequence_cnn_net: AUPRC in epoch 0: 0.3845081739602765, AUROC: 0.9877304884067525, val-Loss:  0.092, train-loss: 0.160\n",
      "##############################################################################################\n",
      "  Epoch 1\n",
      "cnn-net: AUPRC in epoch 1: 0.4332784052394242,  AUROC: 0.9805425287612359, val-Loss:  0.138, train-loss: 0.111\n",
      "siamese-net: AUPRC in epoch 1: 0.49614137377419626,  AUROC: 0.9912137878514712, val-Loss:  0.054, train-loss: 0.059\n",
      "sequence_cnn_net: AUPRC in epoch 1: 0.43952620993671027, AUROC: 0.9887534117748608, val-Loss:  0.053, train-loss: 0.060\n",
      "##############################################################################################\n",
      "  Epoch 2\n",
      "cnn-net: AUPRC in epoch 2: 0.43403417884058904,  AUROC: 0.9806347624096023, val-Loss:  0.109, train-loss: 0.091\n",
      "siamese-net: AUPRC in epoch 2: 0.5215106628640148,  AUROC: 0.9921392567986641, val-Loss:  0.068, train-loss: 0.039\n",
      "sequence_cnn_net: AUPRC in epoch 2: 0.46593159009375945, AUROC: 0.9896708755374786, val-Loss:  0.044, train-loss: 0.044\n",
      "##############################################################################################\n",
      "  Epoch 3\n",
      "cnn-net: AUPRC in epoch 3: 0.44037033291996935,  AUROC: 0.9790208475889428, val-Loss:  0.102, train-loss: 0.074\n",
      "siamese-net: AUPRC in epoch 3: 0.5176941304941488,  AUROC: 0.991999688145853, val-Loss:  0.053, train-loss: 0.029\n",
      "sequence_cnn_net: AUPRC in epoch 3: 0.47908704941013047, AUROC: 0.9897164702844068, val-Loss:  0.052, train-loss: 0.035\n",
      "##############################################################################################\n",
      "END OF TRAINING\n",
      "#######################################################################\n",
      "TEST ON HEK CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 1: 0.5353066522506134 , Test-Loss:  0.085\n",
      "siamese-net: AUPRC in fold 1:  0.6023155991002124, Test-Loss:  0.047\n",
      "sequence_cnn_net: AUPRC in fold 1:  0.5901362408755478, Test-Loss:  0.040\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON K562 CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 1: 0.7416209148053388 , Test-Loss:  0.135\n",
      "siamese-net: AUPRC in fold 1:  0.7078246824678838, Test-Loss:  0.065\n",
      "sequence_cnn_net: AUPRC in fold 1:  0.5919441836182877, Test-Loss:  0.035\n",
      "#######################################################################\n",
      "Processing fold 2\n",
      "  Epoch 0\n",
      "cnn-net: AUPRC in epoch 0: 0.43485174197005017,  AUROC: 0.9887855456895582, val-Loss:  0.171, train-loss: 0.217\n",
      "siamese-net: AUPRC in epoch 0: 0.5948244362180568,  AUROC: 0.9944822775687773, val-Loss:  0.115, train-loss: 0.154\n",
      "sequence_cnn_net: AUPRC in epoch 0: 0.5121401415396731, AUROC: 0.9932948317013666, val-Loss:  0.120, train-loss: 0.192\n",
      "##############################################################################################\n",
      "  Epoch 1\n",
      "cnn-net: AUPRC in epoch 1: 0.4496653043837167,  AUROC: 0.9890787707687323, val-Loss:  0.126, train-loss: 0.123\n",
      "siamese-net: AUPRC in epoch 1: 0.6168785382955235,  AUROC: 0.9947034360557756, val-Loss:  0.068, train-loss: 0.053\n",
      "sequence_cnn_net: AUPRC in epoch 1: 0.5657850319434321, AUROC: 0.9936799582145998, val-Loss:  0.065, train-loss: 0.075\n",
      "##############################################################################################\n",
      "  Epoch 2\n",
      "cnn-net: AUPRC in epoch 2: 0.4574085859014113,  AUROC: 0.9894999305780534, val-Loss:  0.115, train-loss: 0.096\n",
      "siamese-net: AUPRC in epoch 2: 0.6289473985541126,  AUROC: 0.9948072383949647, val-Loss:  0.053, train-loss: 0.033\n",
      "sequence_cnn_net: AUPRC in epoch 2: 0.6031456694564968, AUROC: 0.9945546747416513, val-Loss:  0.064, train-loss: 0.054\n",
      "##############################################################################################\n",
      "  Epoch 3\n",
      "cnn-net: AUPRC in epoch 3: 0.4517297939856196,  AUROC: 0.9892179452426132, val-Loss:  0.099, train-loss: 0.080\n",
      "siamese-net: AUPRC in epoch 3: 0.6424962044895317,  AUROC: 0.9952396379480195, val-Loss:  0.040, train-loss: 0.024\n",
      "sequence_cnn_net: AUPRC in epoch 3: 0.6068002673295194, AUROC: 0.9950497193369874, val-Loss:  0.055, train-loss: 0.040\n",
      "##############################################################################################\n",
      "END OF TRAINING\n",
      "#######################################################################\n",
      "TEST ON HEK CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 2: 0.4349750791469154 , Test-Loss:  0.085\n",
      "siamese-net: AUPRC in fold 2:  0.6167691550663695, Test-Loss:  0.033\n",
      "sequence_cnn_net: AUPRC in fold 2:  0.6479522469562596, Test-Loss:  0.047\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON K562 CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 2: 0.6807992266760121 , Test-Loss:  0.155\n",
      "siamese-net: AUPRC in fold 2:  0.6698356775460816, Test-Loss:  0.041\n",
      "sequence_cnn_net: AUPRC in fold 2:  0.716985464066828, Test-Loss:  0.051\n",
      "#######################################################################\n"
     ]
    }
   ],
   "source": [
    "for enum, indeces in enumerate(kfold.split(dataset.return_x(), dataset.return_class_y())):\n",
    "        print(f\"Processing fold {enum}\")\n",
    "\n",
    "        train_indices, test_val_ind = indeces[0], indeces[1]\n",
    "\n",
    "        split2 = int(np.floor(0.5 * len(test_val_ind)))\n",
    "        val_indices, test_indices = test_val_ind[split2:], test_val_ind[:split2]\n",
    "\n",
    "        train_oversample_mask = get_oversample_mask(train_indices, dataset)\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_oversample_mask)\n",
    "        valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "        test_indices_hek, test_indices_k562 = dataset.seperate_by_class(test_indices)\n",
    "\n",
    "        test_sampler_hek = SubsetRandomSampler(test_indices_hek)\n",
    "        test_sampler_k562 = SubsetRandomSampler(test_indices_k562)\n",
    "\n",
    "        train_loader = DataLoader(dataset, batch_size=30, sampler=train_sampler)\n",
    "        validation_loader = DataLoader(\n",
    "            dataset, batch_size=len(val_indices), sampler=valid_sampler\n",
    "        )\n",
    "\n",
    "        test_loader_hek = DataLoader(\n",
    "            dataset, batch_size=len(test_indices), sampler=test_sampler_hek\n",
    "        )\n",
    "        test_loader_k562 = DataLoader(\n",
    "            dataset, batch_size=len(test_indices), sampler=test_sampler_k562\n",
    "        )\n",
    "\n",
    "        # weights = compute_class_weight(\"balanced\", classes = [0,1], y = dataset.return_y(train_indices))\n",
    "\n",
    "        net = cnn_net()\n",
    "        saved_cnn_net = cnn_net()\n",
    "        optimizer1 = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "        criterion1 = nn.BCELoss()\n",
    "        siamese_net = siamese_cnn_net()\n",
    "        saved_siamese_net = siamese_cnn_net()\n",
    "        optimizer2 = torch.optim.Adam(siamese_net.parameters(), lr=0.0001)\n",
    "        criterion2 = nn.BCELoss()\n",
    "        sequence_net = sequence_cnn_net()\n",
    "        saved_sequence_cnn_net = sequence_cnn_net()\n",
    "        optimizer3 = torch.optim.Adam(sequence_net.parameters(), lr=0.0001)\n",
    "        criterion3 = nn.BCELoss()\n",
    "\n",
    "        y_value_list = []\n",
    "        predicted_list1_full = []\n",
    "        predicted_list2_full = []\n",
    "        predicted_list3_full = []\n",
    "\n",
    "        predicted_list1_auroc = []\n",
    "        predicted_list2_auroc = []\n",
    "        predicted_list3_auroc = []\n",
    "\n",
    "        best_validation_cnn_net = np.inf\n",
    "        best_validation_siamese_cnn_net = np.inf\n",
    "        best_validation_sequence_cnn_net = np.inf\n",
    "\n",
    "        for epoch in range(EPOCH_NUM):\n",
    "            print(f\"  Epoch {epoch}\")\n",
    "\n",
    "            # Train CNN\n",
    "            net, running_loss_train1 = train(\n",
    "                train_loader,\n",
    "                net=net,\n",
    "                criterion=criterion1,\n",
    "                optimizer=optimizer1,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "            y_values1, predicted_list1, running_loss1, eval_1, eval_auroc_1 = evaluate(\n",
    "                validation_loader, net, criterion=criterion1, epoch=epoch\n",
    "            )\n",
    "\n",
    "            # Train Siamese CNN\n",
    "            siamese_net, running_loss_train2 = train(\n",
    "                train_loader,\n",
    "                net=siamese_net,\n",
    "                criterion=criterion2,\n",
    "                optimizer=optimizer2,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "            y_values2, predicted_list2, running_loss2, eval_2, eval_auroc_2 = evaluate(\n",
    "                validation_loader, siamese_net, criterion=criterion2, epoch=epoch\n",
    "            )\n",
    "\n",
    "            # Train Sequence CNN\n",
    "            sequence_net, running_loss_train3 = train(\n",
    "                train_loader,\n",
    "                net=sequence_net,\n",
    "                criterion=criterion3,\n",
    "                optimizer=optimizer3,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "            y_values3, predicted_list3, running_loss3, eval_3, eval_auroc_3 = evaluate(\n",
    "                validation_loader, sequence_net, criterion=criterion3, epoch=epoch\n",
    "            )\n",
    "\n",
    "            y_value_list.extend(y_values1)\n",
    "            predicted_list1_full.append(eval_1)\n",
    "            predicted_list2_full.append(eval_2)\n",
    "            predicted_list3_full.append(eval_3)\n",
    "            predicted_list1_auroc.append(eval_auroc_1)\n",
    "            predicted_list2_auroc.append(eval_auroc_2)\n",
    "            predicted_list3_auroc.append(eval_auroc_3)\n",
    "\n",
    "            if running_loss1 <= best_validation_cnn_net:\n",
    "                best_validation_cnn_net = running_loss1\n",
    "                saved_cnn_net.load_state_dict(net.state_dict())\n",
    "\n",
    "            if running_loss2 <= best_validation_siamese_cnn_net:\n",
    "                best_validation_siamese_cnn_net = running_loss2\n",
    "                saved_siamese_net.load_state_dict(siamese_net.state_dict())\n",
    "\n",
    "            if running_loss3 <= best_validation_sequence_cnn_net:\n",
    "                best_validation_sequence_cnn_net = running_loss3\n",
    "                saved_sequence_cnn_net.load_state_dict(sequence_net.state_dict())\n",
    "\n",
    "            print(f\"cnn-net: AUPRC in epoch {epoch}: {np.mean(eval_1)},  AUROC: {np.mean(eval_auroc_1)}, val-Loss:  {running_loss1 / len(validation_loader):.3f}, train-loss: {running_loss_train1 / len(train_loader):.3f}\")\n",
    "            print(f\"siamese-net: AUPRC in epoch {epoch}: {np.mean(eval_2)},  AUROC: {np.mean(eval_auroc_2)}, val-Loss:  {running_loss2 / len(validation_loader):.3f}, train-loss: {running_loss_train2 / len(train_loader):.3f}\")\n",
    "            print(f\"sequence_cnn_net: AUPRC in epoch {epoch}: {np.mean(eval_3)}, AUROC: {np.mean(eval_auroc_3)}, val-Loss:  {running_loss3 / len(validation_loader):.3f}, train-loss: {running_loss_train3 / len(train_loader):.3f}\")\n",
    "            print(\"##############################################################################################\")\n",
    "\n",
    "        print(\"END OF TRAINING\")\n",
    "\n",
    "        # plot_metric(epoch, predicted_list1_full, predicted_list2_full, predicted_list3_full, [\"cnn-net\",\"siamese-net\",\"sequence_cnn_net\"])\n",
    "\n",
    "        print(\"#######################################################################\")\n",
    "        print(\"TEST ON HEK CELL LINE\")\n",
    "        print(\"#######################################################################\")\n",
    "\n",
    "        #### final test #####\n",
    "        y_values1, predicted_list1, running_loss1, eval_1, eval_auroc_1 = evaluate(\n",
    "            test_loader_hek, saved_cnn_net, criterion=criterion1, epoch=epoch\n",
    "        )\n",
    "        y_values2, predicted_list2, running_loss2, eval_2, eval_auroc_2 = evaluate(\n",
    "            test_loader_hek, saved_siamese_net, criterion=criterion2, epoch=epoch\n",
    "        )\n",
    "        y_values3, predicted_list3, running_loss3, eval_3, eval_auroc_3 = evaluate(\n",
    "            test_loader_hek, saved_sequence_cnn_net, criterion=criterion3, epoch=epoch\n",
    "        )\n",
    "\n",
    "        cv_list1_auprc_hek.append(eval_1)\n",
    "        cv_list2_auprc_hek.append(eval_2)\n",
    "        cv_list3_auprc_hek.append(eval_3)\n",
    "\n",
    "        print(f\"cnn-net: AUPRC in fold {enum}: {np.mean(eval_1)} , Test-Loss:  {running_loss1 / len(test_loader_hek):.3f}\")\n",
    "        print(f\"siamese-net: AUPRC in fold {enum}:  {np.mean(eval_2)}, Test-Loss:  {running_loss2 / len(test_loader_hek):.3f}\")\n",
    "        print(f\"sequence_cnn_net: AUPRC in fold {enum}:  {np.mean(eval_3)}, Test-Loss:  {running_loss3 / len(test_loader_hek):.3f}\")\n",
    "        print(\"#######################################################################\")\n",
    "\n",
    "        #### final test #####\n",
    "        y_values1, predicted_list1, running_loss1, eval_1, eval_auroc_1 = evaluate(\n",
    "            test_loader_k562, saved_cnn_net, criterion=criterion1, epoch=epoch\n",
    "        )\n",
    "        y_values2, predicted_list2, running_loss2, eval_2, eval_auroc_2 = evaluate(\n",
    "            test_loader_k562, saved_siamese_net, criterion=criterion2, epoch=epoch\n",
    "        )\n",
    "        y_values3, predicted_list3, running_loss3, eval_3, eval_auroc_3 = evaluate(\n",
    "            test_loader_k562, saved_sequence_cnn_net, criterion=criterion3, epoch=epoch\n",
    "        )\n",
    "\n",
    "        cv_list1_auprc_k562.append(eval_1)\n",
    "        cv_list2_auprc_k562.append(eval_2)\n",
    "        cv_list3_auprc_k562.append(eval_3)\n",
    "\n",
    "        print(\"#######################################################################\")\n",
    "        print(\"TEST ON K562 CELL LINE\")\n",
    "        print(\"#######################################################################\")\n",
    "        print(f\"cnn-net: AUPRC in fold {enum}: {np.mean(eval_1)} , Test-Loss:  {running_loss1 / len(test_loader_k562):.3f}\")\n",
    "        print(f\"siamese-net: AUPRC in fold {enum}:  {np.mean(eval_2)}, Test-Loss:  {running_loss2 / len(test_loader_k562):.3f}\")\n",
    "        print(f\"sequence_cnn_net: AUPRC in fold {enum}:  {np.mean(eval_3)}, Test-Loss:  {running_loss3 / len(test_loader_k562):.3f}\")\n",
    "        print(\"#######################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mFdKQaFrErk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################################\n",
      "TEST ON HEK CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 0: 0.451\n",
      "siamese-net: AUPRC in fold 0:  0.581\n",
      "sequence_cnn_net: AUPRC in fold 0:  0.633\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON K562 CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 0: 0.576\n",
      "siamese-net: AUPRC in fold 0:  0.609\n",
      "sequence_cnn_net: AUPRC in fold 0:  0.658\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON HEK CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 1: 0.535\n",
      "siamese-net: AUPRC in fold 1:  0.602\n",
      "sequence_cnn_net: AUPRC in fold 1:  0.590\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON K562 CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 1: 0.742\n",
      "siamese-net: AUPRC in fold 1:  0.708\n",
      "sequence_cnn_net: AUPRC in fold 1:  0.592\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON HEK CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 2: 0.435\n",
      "siamese-net: AUPRC in fold 2:  0.617\n",
      "sequence_cnn_net: AUPRC in fold 2:  0.648\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON K562 CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: AUPRC in fold 2: 0.681\n",
      "siamese-net: AUPRC in fold 2:  0.670\n",
      "sequence_cnn_net: AUPRC in fold 2:  0.717\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON HEK CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: Average AUPRC over all folds: 0.474\n",
      "siamese-net: Average AUPRC over all folds: 0.600\n",
      "sequence_cnn_net: Average AUPRC over all folds: 0.624\n",
      "#######################################################################\n",
      "#######################################################################\n",
      "TEST ON K562 CELL LINE\n",
      "#######################################################################\n",
      "cnn-net: Average AUPRC over all folds: 0.666\n",
      "siamese-net: Average AUPRC over all folds: 0.662\n",
      "sequence_cnn_net: Average AUPRC over all folds: 0.656\n",
      "#######################################################################\n"
     ]
    }
   ],
   "source": [
    "# Loop through each fold's results for HEK and K562\n",
    "for num, auprc in enumerate(cv_list1_auprc_hek):\n",
    "    # Report results for HEK Cell Line\n",
    "    print(\"#######################################################################\")\n",
    "    print(\"TEST ON HEK CELL LINE\")\n",
    "    print(\"#######################################################################\")\n",
    "    print(f\"cnn-net: AUPRC in fold {num}: {np.mean(auprc):.3f}\")\n",
    "    print(f\"siamese-net: AUPRC in fold {num}:  {np.mean(cv_list2_auprc_hek[num]):.3f}\")\n",
    "    print(f\"sequence_cnn_net: AUPRC in fold {num}:  {np.mean(cv_list3_auprc_hek[num]):.3f}\")\n",
    "    print(\"#######################################################################\")\n",
    "\n",
    "    # Report results for K562 Cell Line\n",
    "    print(\"#######################################################################\")\n",
    "    print(\"TEST ON K562 CELL LINE\")\n",
    "    print(\"#######################################################################\")\n",
    "    print(f\"cnn-net: AUPRC in fold {num}: {np.mean(cv_list1_auprc_k562[num]):.3f}\")\n",
    "    print(f\"siamese-net: AUPRC in fold {num}:  {np.mean(cv_list2_auprc_k562[num]):.3f}\")\n",
    "    print(f\"sequence_cnn_net: AUPRC in fold {num}:  {np.mean(cv_list3_auprc_k562[num]):.3f}\")\n",
    "    print(\"#######################################################################\")\n",
    "\n",
    "# Report average results across all folds for HEK Cell Line\n",
    "print(\"#######################################################################\")\n",
    "print(\"TEST ON HEK CELL LINE\")\n",
    "print(\"#######################################################################\")\n",
    "print(f\"cnn-net: Average AUPRC over all folds: {np.mean(cv_list1_auprc_hek):.3f}\")\n",
    "print(f\"siamese-net: Average AUPRC over all folds: {np.mean(cv_list2_auprc_hek):.3f}\")\n",
    "print(f\"sequence_cnn_net: Average AUPRC over all folds: {np.mean(cv_list3_auprc_hek):.3f}\")\n",
    "print(\"#######################################################################\")\n",
    "\n",
    "# Report average results across all folds for K562 Cell Line\n",
    "print(\"#######################################################################\")\n",
    "print(\"TEST ON K562 CELL LINE\")\n",
    "print(\"#######################################################################\")\n",
    "print(f\"cnn-net: Average AUPRC over all folds: {np.mean(cv_list1_auprc_k562):.3f}\")\n",
    "print(f\"siamese-net: Average AUPRC over all folds: {np.mean(cv_list2_auprc_k562):.3f}\")\n",
    "print(f\"sequence_cnn_net: Average AUPRC over all folds: {np.mean(cv_list3_auprc_k562):.3f}\")\n",
    "print(\"#######################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy67eoQhIqk6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
