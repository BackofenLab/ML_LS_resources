{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqFM2XwLkhGx"
   },
   "source": [
    "# Exercise 5 (Hands-on): Single-cell RNA Sequencing and Deep Clustering\n",
    "\n",
    "This notebook demonstrates the implementation of a deep learning-based clustering pipeline for single-cell RNA sequencing (scRNA-seq) data.\n",
    "The workflow includes:\n",
    "1. Data preprocessing for PBMC gene expression data.\n",
    "2. Dimensionality reduction using an autoencoder.\n",
    "3. Clustering cells with k-means.\n",
    "Evaluation is done using Rand Index or Adjusted Rand Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qu3hnoRkliV"
   },
   "source": [
    "## Install requirements\n",
    "This section installs all necessary Python libraries and dependencies for running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlUA0t1XkeZS",
    "outputId": "8ef8a5ac-3460-4bda-b512-0e56e2d73815"
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet scikit-learn==1.3.2 torch==2.5.1 matplotlib==3.9.2 umap-learn==0.5.7 requests seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzLVqVz0kq3O"
   },
   "source": [
    "## Import necessary libraries\n",
    "We import the libraries needed for data manipulation, model building, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pM6LK10pknoZ"
   },
   "outputs": [],
   "source": [
    "# For file handling, data fetching, and extraction.\n",
    "import os\n",
    "from requests import get\n",
    "import tarfile\n",
    "\n",
    "# For numerical operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# For building and training deep learning models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "# For clustering (KMeans), evaluation metrics (rand_score,\n",
    "# adjusted_rand_score), and data splitting utilities\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import rand_score, adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# For timing and visualise final results\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "# For extracting files\n",
    "import tarfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVAWj8ftniUL"
   },
   "source": [
    "## Configurable Constants\n",
    "Define key constants for the notebook, such as file paths, model parameters, or dataset configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3_hYJNkktya"
   },
   "outputs": [],
   "source": [
    "# DATASET_URL: URL to download the compressed gene expression dataset.\n",
    "DATASET_URL = \"https://github.com/BackofenLab/ML_LS_resources/raw/refs/heads/master/exercise_5_scrna_deep_clustering_hands_on/data/gene_expression.tar.xz\"\n",
    "\n",
    "# DATASET_DIR_PATH: Path to the directory where the gene expression dataset will be stored.\n",
    "BASE_DATA_DIR = Path(\"./data\")\n",
    "RAW_DATASET_DIR = Path(f\"{BASE_DATA_DIR}/gene_expression.tar.xz\")\n",
    "DATA_DIR = Path(f\"{BASE_DATA_DIR}/gene_expression\")\n",
    "\n",
    "# EPOCH_NUM: Number of epochs for training the model.\n",
    "EPOCH_NUM = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab50Q2tBnRFC"
   },
   "source": [
    "## Setup GPU\n",
    "Check for GPU availability and ensures that the computations are optimized for CUDA if a compatible GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuDog5b3nQsC"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6usosm4Nna1V"
   },
   "source": [
    "## Extract the data\n",
    "Extract the scRNA-seq dataset, which contains gene expression data\n",
    "for 32,738 genes from 2700 cells of peripheral blood mononuclear cells (PBMCs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the gene expression dataset if not already extracted.\n",
    "with tarfile.open(RAW_DATASET_DIR, mode=\"r:xz\") as tar:\n",
    "    tar.extractall(path=BASE_DATA_DIR)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v221nyoznoZf"
   },
   "source": [
    "## Data Preprocessing\n",
    "Load, preprocess, and prepare the scRNA-seq dataset for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjbbiRVzk0el"
   },
   "outputs": [],
   "source": [
    "# Define a custom Dataset class for loading and preprocessing the gene expression data.\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, dataset_path, label_path):\n",
    "        # Load the dataset and corresponding labels.\n",
    "        self.x_values, self.y_values = self.get_dataset(dataset_path, label_path)\n",
    "\n",
    "        # TODO: Set the input shape and number of distinct labels for further use.\n",
    "\n",
    "    def get_dataset(self, dataset_path, label_path):\n",
    "        # TODO: Load gene expression matrix from the dataset path using numpy.\n",
    "\n",
    "        # TODO: Load labels from the label file.\n",
    "\n",
    "        # TODO: Remove genes with zero expression across all cells.\n",
    "\n",
    "        return dataset, labels\n",
    "\n",
    "    def return_y(self):\n",
    "        # Return the labels.\n",
    "        return self.y_values\n",
    "\n",
    "    def return_x(self):\n",
    "        # Return the dataset.\n",
    "        return self.x_values\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples.\n",
    "        return len(self.y_values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a specific sample and its corresponding label.\n",
    "        return self.x_values[idx], self.y_values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and check the shape of the raw gene expression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNAEoV2MkvEE"
   },
   "outputs": [],
   "source": [
    "# TODO: Instantiate the RNADataset class\n",
    "\n",
    "print(\"Shape of the filtered dataset:\", dataset.return_x().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1Him566lFER"
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize lists to store loss metrics\n",
    "\n",
    "# TODO: Define a stratified split for cross-validation\n",
    "# StratifiedShuffleSplit ensures proportional representation of labels in the splits\n",
    "\n",
    "# TODO: Split the dataset into training and testing sets\n",
    "# Using train_test_split to split data indices into train and test sets with a fixed random state\n",
    "\n",
    "# TODO: Assign indices to training and test/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of cell types: {}\".format(np.unique(dataset.return_y())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPc41LfMoBvO"
   },
   "source": [
    "## Define the Autoencoder\n",
    "This section defines the autoencoder architecture, which will be used for dimensionality reduction of the scRNA-seq data.\n",
    "The compressed representation learned by the autoencoder will serve as input for the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MCUt__1lU_2"
   },
   "outputs": [],
   "source": [
    "# TODO: Define an Autoencoder (AE) class (hint: use nn.Module as in the previous exercises)\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        # TODO: define the encoder: Maps input features to a lower-dimensional representation (latent space)\n",
    "\n",
    "        # TODO: define the decoder: Reconstructs the input features from the latent space\n",
    "\n",
    "    def forward(self, features):\n",
    "        # TODO: Encoder forward pass\n",
    "\n",
    "        # TODO: Decoder forward pass\n",
    "\n",
    "        reconstructed = self.decoder_output_layer(activation)  # Reconstructed output\n",
    "\n",
    "        return reconstructed, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjX8U8kglKaw"
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize the Autoencoder\n",
    "\n",
    "# TODO: Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FZDcgG2lQmP"
   },
   "outputs": [],
   "source": [
    "# TODO: Split the test/validation indices into validation and test sets\n",
    "\n",
    "# TODO: Set a fixed random seed for reproducibility\n",
    "\n",
    "# TODO: Create samplers for different data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KMVRor5lqvI"
   },
   "outputs": [],
   "source": [
    "# TODO: Create DataLoader objects for efficient data batch processing (total, train, validation, test)\n",
    "\n",
    "# TODO: Initialize a variable to track the best validation loss (used for early stopping or model evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXtnmAddoGZE"
   },
   "source": [
    "## Build, train, and evaluate the model\n",
    "Define functions to build, train, and evaluate the autoencoder model.\n",
    "The training process minimizes reconstruction loss to ensure the autoencoder effectively captures latent representations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACZrZmeRl2N6"
   },
   "outputs": [],
   "source": [
    "def train_model(trainloader, net, criterion, optimizer, epoch):\n",
    "    # TODO: Enable anomaly detection for debugging potential issues during backpropagation\n",
    "\n",
    "    # TODO: initialize running loss and reset network gradients\n",
    "\n",
    "    for i, data in enumerate(trainloader):  # Loop through the training data in batches\n",
    "        input_data, target_data = data  # Extract inputs and targets\n",
    "        loss = 0\n",
    "\n",
    "        # TODO: Clear optimizer gradients, perform forward pass, compute loss, backpropagate, and update parameters\n",
    "\n",
    "    # Return the trained model and average loss over all batches\n",
    "    return net, running_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_2JFa7UmAnA"
   },
   "outputs": [],
   "source": [
    "def eval_model(dataloader, net, criterion, epoch):\n",
    "    # TODO: Define a function to evaluate the autoencoder model\n",
    "    running_loss = 0  # Initialize running loss\n",
    "    pearson_running_loss2 = 0  # Placeholder for additional loss metrics (if needed)\n",
    "\n",
    "    # Lists to store input, output, and target data for analysis\n",
    "    input_data_list = []\n",
    "    output_data_list = []\n",
    "    target_data_list = []\n",
    "\n",
    "    # Return total loss and collected data\n",
    "    return running_loss, output_data_list, input_data_list, target_data_list, pearson_running_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYMQhCh7lssF",
    "outputId": "823e6ae5-3b19-4e77-e44a-ebd2871f52a3"
   },
   "outputs": [],
   "source": [
    "# TODO: Train and validate the autoencoder model for the specified number of epochs\n",
    "print(\"Training the model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okTwQTtzmRsi"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate the final trained model on the test set\n",
    "# The `eval_model` function computes the test loss and collects output/input/target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2R4uhPVOmwdz"
   },
   "outputs": [],
   "source": [
    "# TODO: Define function to extract latent embeddings from the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQ2-XwOMm4Nq"
   },
   "outputs": [],
   "source": [
    "# TODO: Define a function to perform k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2fBgNGkmhhz"
   },
   "outputs": [],
   "source": [
    "# TODO: Generate embeddings from the autoencoder's latent space\n",
    "\n",
    "# TODO: Perform k-means clustering on the latent embeddings\n",
    "\n",
    "# TODO: Evaluate k-means clustering performance using Rand Index and Adjusted Rand Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4lbvI-spN1s"
   },
   "source": [
    "## Analyze the results\n",
    "Present and analyze the evaluation results, including:\n",
    "- Test loss from the autoencoder.\n",
    "- Clustering performance metrics (Rand Index and Adjusted Rand Index) for k-means and spectral clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THndkBj5mV8q",
    "outputId": "57a98061-3e2b-4cce-e7c7-d16d027e418d"
   },
   "outputs": [],
   "source": [
    "# TODO: Print evaluation results with improved formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NMI: Normalized Mutual Information (NMI) is a normalization of the Mutual Information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation).\n",
    "\n",
    "- ARI: Adjusted Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical\n",
    "\n",
    "- Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of. The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ensure running_losses and running_losses_val are detached tensors before plotting\n",
    "\n",
    "# TODO: Plot training vs. validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction of preprocessed single-cell data using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "SlWTd57AUZEJ",
    "outputId": "fe56992f-d082-402e-9e30-0130aa0b3c05"
   },
   "outputs": [],
   "source": [
    "# TODO: Perform UMAP on the original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct colors for labelling clusters\n",
    "color_list = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# TODO: Visualize UMAP embeddings with original cell types\n",
    "\n",
    "# TODO: Add cell type labels to the DataFrame for visualization\n",
    "\n",
    "# TODO: Visualize UMAP embeddings with original cell types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction of latent dimensions using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "SFl8gttNM5TJ",
    "outputId": "735cb0d5-b329-474d-b62c-cb53bd51d6c9"
   },
   "outputs": [],
   "source": [
    "# TODO: Perform UMAP dimensionality reduction of latent dimensions\n",
    "\n",
    "# TODO: Visualize UMAP embeddings with k-means cluster labels"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "exercise_venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
