{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08153f31",
   "metadata": {},
   "source": [
    "# Exercise 1: Introduction to Machine Learning and Life Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533d4e6",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "We start by installing the required packages into the same environment we use as the kernel for this Jupyter notebook using `pip`. If you are unfamiliar with Jupyter notebooks, please refer to: https://jupyter.org/ for setting up and using a Jupyter server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88af66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U scikit-learn==1.3.2 pandas==2.0.3 torch==2.4.1 matplotlib requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874da840",
   "metadata": {},
   "source": [
    "## Import libraries and download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "polished-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f8689",
   "metadata": {},
   "source": [
    "## Parse and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de416930",
   "metadata": {},
   "source": [
    "Download the ELAV1 dataset (`ELAV1_PARCLIP.txt`) from https://github.com/BackofenLab/ML_LS_resources/blob/master/exercise_1_introduction/ELAVL1_PARCLIP.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bd935",
   "metadata": {},
   "source": [
    "We will featurize the sequences by creating k-mers with k=3, which will result in 4^3 = 64 predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: List all possible three-mers\n",
    "\n",
    "\n",
    "# Read the data\n",
    "with open(\"ELAVL1_PARCLIP.txt\") as f:\n",
    "    data_text = f.read().split(\"\\n>\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# TODO: Parse all data elements to get target variable and counts for k-mers\n",
    "\n",
    "\n",
    "# convert to pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b9d93",
   "metadata": {},
   "source": [
    "## Construct Classifier Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84fe9a",
   "metadata": {},
   "source": [
    "We will now train a simple machine learning classifier on the data. We can select and implement a simple classifier easily from scikit-learn. We will first format our dataset from before by splitting the dataset into a train and test split using `train_test_split()` with the stratified option to ensure class balance is taken into account when splitting the dataset. \n",
    "\n",
    "For more information, refer to https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare training and test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d12822",
   "metadata": {},
   "source": [
    "Then select a classifier from scikit-learn. One possible choice is the Random Forest Classifier implementation at https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html. Construct this classifier and then call the `fit()` function on it and pass the training samples, `X_train` and the targets, `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6447ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a classifier and fit it to the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb7c02",
   "metadata": {},
   "source": [
    "Evaluate the trained classifer on our test data, obtained from the stratified train-test split before. To evaluate our model on the test data, we must first obtain its predictions using `predict()` and passing the test samples, `X_test`. We then compute the following evaluation metrics using these predictions and the test labels, `y_test`: AUCROC, accuracy, precision, recall, and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a793a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions and compute metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d071f",
   "metadata": {},
   "source": [
    "Now implement, train, and evaluate another classfier from scratch to compare their performance. This time, train the model using 5-fold cross-validation and then evaluate it on the average of the 5 runs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optional-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train another classifier and compute metrics with cross-validation to compare with the previous one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f134b-8866-4d19-9003-e0a386b1ef45",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913baf4e",
   "metadata": {},
   "source": [
    "Implement and train an MLP as a classifier and compare it to the previous two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "functioning-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define hyperparameters\n",
    "\n",
    "\n",
    "# TODO: Define the MLP model (use torch.nn.Module)\n",
    "\n",
    "\n",
    "# TODO : Prepare data for PyTorch (convert to tensors)\n",
    "\n",
    "\n",
    "# TODO: Create a DataLoader for batch processing\n",
    "\n",
    "\n",
    "# TODO: Initialize the MLP model\n",
    "\n",
    "\n",
    "# TODO: Define the loss function and optimizer\n",
    "\n",
    "\n",
    "# TODO: define function to evaluate the model on the test set\n",
    "\n",
    "\n",
    "# TODO: Train the MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca543f0d-fcc0-4154-9a0d-94a6805d6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the training and test losses (use matplotlib)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d57b4d",
   "metadata": {},
   "source": [
    "# Improve the performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b67f25",
   "metadata": {},
   "source": [
    "After comparing the performances of the previous three classifers, explore other techniques, architectures, and strategies to further improve the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e4a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve the performance then evaluate it using the same metrics and compare with previous methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
